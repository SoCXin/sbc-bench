sbc-bench v0.9.9 HUAQIN P6410 HQ3110BR49000 (Wed, 04 Jan 2023 02:11:10 +0000)

Distributor ID:	Ubuntu
Description:	Ubuntu 20.04.5 LTS
Release:	20.04
Codename:	focal

Device Info:
	Manufacturer: HUAQIN
	Product Name: P6410 HQ3110BR49000
	SKU Number: Unknown
	Family: Altra Max

BIOS/UEFI:
	Vendor: American Megatrends International LLC
	Version: 1.15.00
	Release Date: 11/25/2022
	BIOS Revision: 5.15
	Firmware Revision: 2.10

/usr/bin/gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0

Uptime: 02:11:11 up  1:07,  4 users,  load average: 0.78, 1.99, 28.99,  Â°C,  57484481

Linux 5.4.219-sg1-dk (p6410-server) 	01/04/23 	_aarch64_	(256 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          36.35    0.01    0.80    0.00    0.00   62.84

Device             tps    kB_read/s    kB_wrtn/s    kB_dscd/s    kB_read    kB_wrtn    kB_dscd
nvme0n1          21.37       910.29       144.33         0.00    3680793     583601          0

              total        used        free      shared  buff/cache   available
Mem:          375Gi       3.5Gi       371Gi       4.0Mi       576Mi       370Gi
Swap:         8.0Gi          0B       8.0Gi

Filename				Type		Size	Used	Priority
/swap.img                              	file    	8388604	0	-2

##########################################################################

Checking cpufreq OPP for cpu0 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2998 (2999.085/2998.946/2998.806)
Cpufreq OPP: 1000    Measured: 1030 (1030.567/1030.567/1030.466)     (+3.0%)

Checking cpufreq OPP for cpu1 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2998 (2999.155/2998.876/2998.806)
Cpufreq OPP: 1000    Measured: 1030 (1030.541/1030.516/1030.466)     (+3.0%)

Checking cpufreq OPP for cpu2 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2998 (2999.085/2998.876/2998.249)
Cpufreq OPP: 1000    Measured: 1030 (1030.491/1030.416/1030.416)     (+3.0%)

Checking cpufreq OPP for cpu3 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2999 (2999.085/2999.085/2998.946)
Cpufreq OPP: 1000    Measured: 1030 (1030.541/1030.541/1030.541)     (+3.0%)

Checking cpufreq OPP for cpu4 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2999 (2999.085/2999.085/2998.876)
Cpufreq OPP: 1000    Measured: 1030 (1030.541/1030.516/1030.491)     (+3.0%)

Checking cpufreq OPP for cpu5 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2999 (2999.155/2999.015/2998.946)
Cpufreq OPP: 1000    Measured: 1030 (1030.567/1030.466/1030.416)     (+3.0%)

Checking cpufreq OPP for cpu6 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2999 (2999.224/2999.085/2998.946)
Cpufreq OPP: 1000    Measured: 1030 (1030.516/1030.466/1030.441)     (+3.0%)

Checking cpufreq OPP for cpu7 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2999 (2999.085/2999.015/2998.946)
Cpufreq OPP: 1000    Measured: 1030 (1030.491/1030.466/1030.466)     (+3.0%)

Checking cpufreq OPP for cpu8 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2998 (2999.015/2998.946/2998.806)
Cpufreq OPP: 1000    Measured: 1030 (1030.642/1030.541/1030.466)     (+3.0%)

Checking cpufreq OPP for cpu9-cpu255 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2998 (2999.155/2998.946/2998.737)
Cpufreq OPP: 1000    Measured: 1030 (1030.491/1030.491/1030.416)     (+3.0%)

##########################################################################

Hardware sensors:

apm_xgene-isa-0000
SoC Temperature:  +44.0 C  
CPU power:        20.00 W  
IO power:         21.02 W  

apm_xgene-isa-0000
SoC Temperature:  +43.0 C  
CPU power:        20.80 W  
IO power:         18.02 W  

##########################################################################

Executing benchmark on cpu0 (Neoverse-N1):

tinymembench v0.4.9 (simple benchmark for memory throughput and latency)

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 3: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 4: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                     :  12409.7 MB/s
 C copy backwards (32 byte blocks)                    :  12335.0 MB/s
 C copy backwards (64 byte blocks)                    :  12342.2 MB/s
 C copy                                               :  12605.8 MB/s
 C copy prefetched (32 bytes step)                    :  13128.9 MB/s (0.5%)
 C copy prefetched (64 bytes step)                    :  13137.9 MB/s
 C 2-pass copy                                        :   5898.5 MB/s (0.2%)
 C 2-pass copy prefetched (32 bytes step)             :   7893.7 MB/s (0.1%)
 C 2-pass copy prefetched (64 bytes step)             :   7802.1 MB/s (0.2%)
 C fill                                               :  47039.0 MB/s
 C fill (shuffle within 16 byte blocks)               :  47031.0 MB/s
 C fill (shuffle within 32 byte blocks)               :  47024.3 MB/s
 C fill (shuffle within 64 byte blocks)               :  47035.3 MB/s
 ---
 standard memcpy                                      :  13254.1 MB/s
 standard memset                                      :  47953.1 MB/s
 ---
 NEON LDP/STP copy                                    :  13360.2 MB/s
 NEON LDP/STP copy pldl2strm (32 bytes step)          :  14780.8 MB/s (0.2%)
 NEON LDP/STP copy pldl2strm (64 bytes step)          :  14800.8 MB/s
 NEON LDP/STP copy pldl1keep (32 bytes step)          :  15035.3 MB/s (0.2%)
 NEON LDP/STP copy pldl1keep (64 bytes step)          :  15000.1 MB/s (0.1%)
 NEON LD1/ST1 copy                                    :  13391.6 MB/s
 NEON STP fill                                        :  47946.4 MB/s
 NEON STNP fill                                       :  47949.2 MB/s
 ARM LDP/STP copy                                     :  13434.9 MB/s
 ARM STP fill                                         :  47935.4 MB/s
 ARM STNP fill                                        :  47879.9 MB/s

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)                 :    111.9 MB/s (3.5%)
 NEON LDP/STP 2-pass copy (from framebuffer)          :    114.8 MB/s (1.3%)
 NEON LD1/ST1 copy (from framebuffer)                 :    115.2 MB/s (0.7%)
 NEON LD1/ST1 2-pass copy (from framebuffer)          :    106.9 MB/s (0.8%)
 ARM LDP/STP copy (from framebuffer)                  :    111.3 MB/s (2.9%)
 ARM LDP/STP 2-pass copy (from framebuffer)           :    114.6 MB/s (1.1%)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    2.2 ns          /     2.8 ns 
    524288 :    3.2 ns          /     3.7 ns 
   1048576 :    6.8 ns          /     9.4 ns 
   2097152 :   16.7 ns          /    23.4 ns 
   4194304 :   23.3 ns          /    28.9 ns 
   8388608 :   29.2 ns          /    33.0 ns 
  16777216 :   38.3 ns          /    47.9 ns 
  33554432 :   69.3 ns          /    91.0 ns 
  67108864 :   89.3 ns          /   109.6 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    1.8 ns          /     2.2 ns 
    524288 :    2.1 ns          /     2.4 ns 
   1048576 :    2.4 ns          /     2.5 ns 
   2097152 :   15.1 ns          /    21.9 ns 
   4194304 :   21.5 ns          /    27.2 ns 
   8388608 :   24.8 ns          /    28.8 ns 
  16777216 :   26.8 ns          /    29.9 ns 
  33554432 :   63.0 ns          /    83.7 ns 
  67108864 :   81.7 ns          /   100.8 ns 

Executing benchmark on cpu1 (Neoverse-N1):

tinymembench v0.4.9 (simple benchmark for memory throughput and latency)

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 3: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 4: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                     :  12447.9 MB/s
 C copy backwards (32 byte blocks)                    :  12364.8 MB/s
 C copy backwards (64 byte blocks)                    :  12369.5 MB/s
 C copy                                               :  12597.6 MB/s
 C copy prefetched (32 bytes step)                    :  13121.7 MB/s
 C copy prefetched (64 bytes step)                    :  13135.3 MB/s
 C 2-pass copy                                        :   5897.2 MB/s (0.3%)
 C 2-pass copy prefetched (32 bytes step)             :   7883.8 MB/s
 C 2-pass copy prefetched (64 bytes step)             :   7798.0 MB/s (0.1%)
 C fill                                               :  47031.6 MB/s
 C fill (shuffle within 16 byte blocks)               :  47047.6 MB/s
 C fill (shuffle within 32 byte blocks)               :  47046.7 MB/s
 C fill (shuffle within 64 byte blocks)               :  47016.1 MB/s
 ---
 standard memcpy                                      :  13247.3 MB/s
 standard memset                                      :  47962.6 MB/s
 ---
 NEON LDP/STP copy                                    :  13350.9 MB/s
 NEON LDP/STP copy pldl2strm (32 bytes step)          :  14802.2 MB/s (0.1%)
 NEON LDP/STP copy pldl2strm (64 bytes step)          :  14852.0 MB/s (0.1%)
 NEON LDP/STP copy pldl1keep (32 bytes step)          :  15047.1 MB/s (0.1%)
 NEON LDP/STP copy pldl1keep (64 bytes step)          :  15018.7 MB/s (0.2%)
 NEON LD1/ST1 copy                                    :  13396.2 MB/s
 NEON STP fill                                        :  47963.4 MB/s
 NEON STNP fill                                       :  47971.9 MB/s
 ARM LDP/STP copy                                     :  13439.5 MB/s
 ARM STP fill                                         :  47962.1 MB/s
 ARM STNP fill                                        :  47950.2 MB/s

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)                 :    110.9 MB/s
 NEON LDP/STP 2-pass copy (from framebuffer)          :    114.7 MB/s (1.1%)
 NEON LD1/ST1 copy (from framebuffer)                 :    114.6 MB/s (0.4%)
 NEON LD1/ST1 2-pass copy (from framebuffer)          :    114.7 MB/s (1.2%)
 ARM LDP/STP copy (from framebuffer)                  :    112.6 MB/s (1.0%)
 ARM LDP/STP 2-pass copy (from framebuffer)           :    114.2 MB/s (1.1%)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    2.2 ns          /     2.8 ns 
    524288 :    3.2 ns          /     3.6 ns 
   1048576 :    6.6 ns          /     9.2 ns 
   2097152 :   16.6 ns          /    23.4 ns 
   4194304 :   23.3 ns          /    28.9 ns 
   8388608 :   29.3 ns          /    33.1 ns 
  16777216 :   38.4 ns          /    47.9 ns 
  33554432 :   69.3 ns          /    91.2 ns 
  67108864 :   89.6 ns          /   110.0 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    1.8 ns          /     2.2 ns 
    524288 :    2.1 ns          /     2.4 ns 
   1048576 :    2.4 ns          /     2.5 ns 
   2097152 :   15.1 ns          /    21.9 ns 
   4194304 :   21.6 ns          /    27.2 ns 
   8388608 :   24.8 ns          /    28.8 ns 
  16777216 :   26.8 ns          /    30.0 ns 
  33554432 :   62.9 ns          /    83.7 ns 
  67108864 :   81.7 ns          /   100.8 ns 

Executing benchmark on cpu2 (Neoverse-N1):

tinymembench v0.4.9 (simple benchmark for memory throughput and latency)

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 3: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 4: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                     :  12432.2 MB/s
 C copy backwards (32 byte blocks)                    :  12347.0 MB/s
 C copy backwards (64 byte blocks)                    :  12353.7 MB/s
 C copy                                               :  12623.6 MB/s
 C copy prefetched (32 bytes step)                    :  13187.1 MB/s
 C copy prefetched (64 bytes step)                    :  13203.8 MB/s
 C 2-pass copy                                        :   5904.3 MB/s (0.5%)
 C 2-pass copy prefetched (32 bytes step)             :   7896.7 MB/s (0.1%)
 C 2-pass copy prefetched (64 bytes step)             :   7809.3 MB/s (0.1%)
 C fill                                               :  47050.7 MB/s
 C fill (shuffle within 16 byte blocks)               :  47049.8 MB/s
 C fill (shuffle within 32 byte blocks)               :  47046.3 MB/s
 C fill (shuffle within 64 byte blocks)               :  47038.1 MB/s
 ---
 standard memcpy                                      :  13311.4 MB/s
 standard memset                                      :  47961.7 MB/s
 ---
 NEON LDP/STP copy                                    :  13402.0 MB/s
 NEON LDP/STP copy pldl2strm (32 bytes step)          :  14803.3 MB/s (0.2%)
 NEON LDP/STP copy pldl2strm (64 bytes step)          :  14850.6 MB/s (0.2%)
 NEON LDP/STP copy pldl1keep (32 bytes step)          :  15062.0 MB/s (0.1%)
 NEON LDP/STP copy pldl1keep (64 bytes step)          :  15023.8 MB/s (0.1%)
 NEON LD1/ST1 copy                                    :  13439.0 MB/s
 NEON STP fill                                        :  47943.9 MB/s
 NEON STNP fill                                       :  47966.5 MB/s
 ARM LDP/STP copy                                     :  13465.3 MB/s
 ARM STP fill                                         :  47951.9 MB/s
 ARM STNP fill                                        :  47931.2 MB/s

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)                 :    112.1 MB/s (0.7%)
 NEON LDP/STP 2-pass copy (from framebuffer)          :    114.4 MB/s (1.1%)
 NEON LD1/ST1 copy (from framebuffer)                 :    116.2 MB/s (1.0%)
 NEON LD1/ST1 2-pass copy (from framebuffer)          :    114.0 MB/s (0.9%)
 ARM LDP/STP copy (from framebuffer)                  :    112.8 MB/s (0.9%)
 ARM LDP/STP 2-pass copy (from framebuffer)           :    114.7 MB/s (1.0%)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    2.2 ns          /     2.8 ns 
    524288 :    3.2 ns          /     3.7 ns 
   1048576 :    6.9 ns          /     9.7 ns 
   2097152 :   16.5 ns          /    23.3 ns 
   4194304 :   23.2 ns          /    28.7 ns 
   8388608 :   28.9 ns          /    33.0 ns 
  16777216 :   37.9 ns          /    47.1 ns 
  33554432 :   69.0 ns          /    90.9 ns 
  67108864 :   89.2 ns          /   109.6 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    1.8 ns          /     2.2 ns 
    524288 :    2.1 ns          /     2.4 ns 
   1048576 :    2.4 ns          /     2.5 ns 
   2097152 :   15.1 ns          /    21.8 ns 
   4194304 :   21.5 ns          /    27.1 ns 
   8388608 :   24.7 ns          /    28.7 ns 
  16777216 :   26.7 ns          /    29.8 ns 
  33554432 :   62.8 ns          /    83.5 ns 
  67108864 :   81.7 ns          /   100.7 ns 

Executing benchmark on cpu3 (Neoverse-N1):

tinymembench v0.4.9 (simple benchmark for memory throughput and latency)

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 3: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 4: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                     :  12461.3 MB/s
 C copy backwards (32 byte blocks)                    :  12387.2 MB/s
 C copy backwards (64 byte blocks)                    :  12390.2 MB/s
 C copy                                               :  12616.9 MB/s
 C copy prefetched (32 bytes step)                    :  13182.6 MB/s
 C copy prefetched (64 bytes step)                    :  13198.6 MB/s
 C 2-pass copy                                        :   5752.9 MB/s
 C 2-pass copy prefetched (32 bytes step)             :   7591.9 MB/s
 C 2-pass copy prefetched (64 bytes step)             :   7149.3 MB/s (0.1%)
 C fill                                               :  47024.7 MB/s
 C fill (shuffle within 16 byte blocks)               :  47044.4 MB/s
 C fill (shuffle within 32 byte blocks)               :  47049.1 MB/s
 C fill (shuffle within 64 byte blocks)               :  47038.3 MB/s
 ---
 standard memcpy                                      :  13304.7 MB/s
 standard memset                                      :  47945.4 MB/s
 ---
 NEON LDP/STP copy                                    :  13397.6 MB/s
 NEON LDP/STP copy pldl2strm (32 bytes step)          :  14817.2 MB/s (0.1%)
 NEON LDP/STP copy pldl2strm (64 bytes step)          :  14873.2 MB/s
 NEON LDP/STP copy pldl1keep (32 bytes step)          :  15065.3 MB/s (0.1%)
 NEON LDP/STP copy pldl1keep (64 bytes step)          :  15037.5 MB/s (0.1%)
 NEON LD1/ST1 copy                                    :  13421.8 MB/s
 NEON STP fill                                        :  47939.8 MB/s
 NEON STNP fill                                       :  47965.5 MB/s
 ARM LDP/STP copy                                     :  13450.8 MB/s
 ARM STP fill                                         :  47949.7 MB/s
 ARM STNP fill                                        :  47951.1 MB/s

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)                 :    113.5 MB/s (1.0%)
 NEON LDP/STP 2-pass copy (from framebuffer)          :    113.8 MB/s (0.8%)
 NEON LD1/ST1 copy (from framebuffer)                 :    116.0 MB/s (0.9%)
 NEON LD1/ST1 2-pass copy (from framebuffer)          :    114.3 MB/s (0.9%)
 ARM LDP/STP copy (from framebuffer)                  :    112.3 MB/s (1.0%)
 ARM LDP/STP 2-pass copy (from framebuffer)           :    115.1 MB/s (1.1%)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    2.2 ns          /     2.8 ns 
    524288 :    3.2 ns          /     3.6 ns 
   1048576 :    6.6 ns          /     9.4 ns 
   2097152 :   16.6 ns          /    23.3 ns 
   4194304 :   23.2 ns          /    28.7 ns 
   8388608 :   28.9 ns          /    33.0 ns 
  16777216 :   37.9 ns          /    47.2 ns 
  33554432 :   69.1 ns          /    90.9 ns 
  67108864 :   89.2 ns          /   109.6 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    1.8 ns          /     2.2 ns 
    524288 :    2.1 ns          /     2.4 ns 
   1048576 :    2.4 ns          /     2.5 ns 
   2097152 :   15.1 ns          /    21.8 ns 
   4194304 :   21.5 ns          /    27.1 ns 
   8388608 :   24.7 ns          /    28.7 ns 
  16777216 :   26.7 ns          /    29.8 ns 
  33554432 :   62.8 ns          /    83.6 ns 
  67108864 :   81.6 ns          /   100.7 ns 

Executing benchmark on cpu4 (Neoverse-N1):

tinymembench v0.4.9 (simple benchmark for memory throughput and latency)

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 3: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 4: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                     :  12387.3 MB/s
 C copy backwards (32 byte blocks)                    :  12302.1 MB/s
 C copy backwards (64 byte blocks)                    :  12311.0 MB/s
 C copy                                               :  12547.0 MB/s
 C copy prefetched (32 bytes step)                    :  13095.8 MB/s
 C copy prefetched (64 bytes step)                    :  13119.3 MB/s
 C 2-pass copy                                        :   5893.0 MB/s (0.7%)
 C 2-pass copy prefetched (32 bytes step)             :   7895.5 MB/s (0.1%)
 C 2-pass copy prefetched (64 bytes step)             :   7790.0 MB/s (0.1%)
 C fill                                               :  47031.7 MB/s
 C fill (shuffle within 16 byte blocks)               :  47050.2 MB/s
 C fill (shuffle within 32 byte blocks)               :  47045.4 MB/s
 C fill (shuffle within 64 byte blocks)               :  47029.0 MB/s
 ---
 standard memcpy                                      :  13233.3 MB/s
 standard memset                                      :  47936.2 MB/s
 ---
 NEON LDP/STP copy                                    :  13322.1 MB/s
 NEON LDP/STP copy pldl2strm (32 bytes step)          :  14744.7 MB/s
 NEON LDP/STP copy pldl2strm (64 bytes step)          :  14811.0 MB/s (0.1%)
 NEON LDP/STP copy pldl1keep (32 bytes step)          :  15004.4 MB/s
 NEON LDP/STP copy pldl1keep (64 bytes step)          :  14983.7 MB/s (0.1%)
 NEON LD1/ST1 copy                                    :  13358.4 MB/s
 NEON STP fill                                        :  47972.6 MB/s
 NEON STNP fill                                       :  47972.2 MB/s
 ARM LDP/STP copy                                     :  13385.3 MB/s
 ARM STP fill                                         :  47947.1 MB/s
 ARM STNP fill                                        :  47947.8 MB/s

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)                 :    111.6 MB/s (0.4%)
 NEON LDP/STP 2-pass copy (from framebuffer)          :    114.8 MB/s (1.1%)
 NEON LD1/ST1 copy (from framebuffer)                 :    115.5 MB/s (0.8%)
 NEON LD1/ST1 2-pass copy (from framebuffer)          :    115.0 MB/s (1.1%)
 ARM LDP/STP copy (from framebuffer)                  :    113.0 MB/s (1.1%)
 ARM LDP/STP 2-pass copy (from framebuffer)           :    114.1 MB/s (0.9%)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    2.2 ns          /     2.8 ns 
    524288 :    3.2 ns          /     3.6 ns 
   1048576 :    6.9 ns          /     9.9 ns 
   2097152 :   16.8 ns          /    23.4 ns 
   4194304 :   23.3 ns          /    28.9 ns 
   8388608 :   29.2 ns          /    33.1 ns 
  16777216 :   38.0 ns          /    47.3 ns 
  33554432 :   69.6 ns          /    91.5 ns 
  67108864 :   89.8 ns          /   110.2 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    1.8 ns          /     2.2 ns 
    524288 :    2.1 ns          /     2.4 ns 
   1048576 :    2.4 ns          /     2.5 ns 
   2097152 :   15.1 ns          /    21.9 ns 
   4194304 :   21.6 ns          /    27.2 ns 
   8388608 :   24.8 ns          /    28.8 ns 
  16777216 :   26.7 ns          /    29.9 ns 
  33554432 :   63.1 ns          /    84.1 ns 
  67108864 :   82.0 ns          /   101.2 ns 

Executing benchmark on cpu5 (Neoverse-N1):

tinymembench v0.4.9 (simple benchmark for memory throughput and latency)

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 3: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 4: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                     :  12405.7 MB/s
 C copy backwards (32 byte blocks)                    :  12314.2 MB/s
 C copy backwards (64 byte blocks)                    :  12321.0 MB/s
 C copy                                               :  12578.8 MB/s
 C copy prefetched (32 bytes step)                    :  13130.8 MB/s
 C copy prefetched (64 bytes step)                    :  13144.2 MB/s
 C 2-pass copy                                        :   5724.0 MB/s
 C 2-pass copy prefetched (32 bytes step)             :   7572.9 MB/s
 C 2-pass copy prefetched (64 bytes step)             :   7133.1 MB/s (0.1%)
 C fill                                               :  47023.6 MB/s
 C fill (shuffle within 16 byte blocks)               :  47039.1 MB/s
 C fill (shuffle within 32 byte blocks)               :  47034.0 MB/s
 C fill (shuffle within 64 byte blocks)               :  47039.2 MB/s
 ---
 standard memcpy                                      :  13261.0 MB/s
 standard memset                                      :  47962.4 MB/s
 ---
 NEON LDP/STP copy                                    :  13333.5 MB/s
 NEON LDP/STP copy pldl2strm (32 bytes step)          :  14744.9 MB/s
 NEON LDP/STP copy pldl2strm (64 bytes step)          :  14843.7 MB/s (0.1%)
 NEON LDP/STP copy pldl1keep (32 bytes step)          :  15031.1 MB/s (0.2%)
 NEON LDP/STP copy pldl1keep (64 bytes step)          :  14984.2 MB/s (0.3%)
 NEON LD1/ST1 copy                                    :  13367.1 MB/s
 NEON STP fill                                        :  47965.8 MB/s
 NEON STNP fill                                       :  47962.8 MB/s
 ARM LDP/STP copy                                     :  13400.6 MB/s
 ARM STP fill                                         :  47958.9 MB/s
 ARM STNP fill                                        :  47918.3 MB/s

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)                 :    112.3 MB/s (0.8%)
 NEON LDP/STP 2-pass copy (from framebuffer)          :    114.1 MB/s (1.2%)
 NEON LD1/ST1 copy (from framebuffer)                 :    114.8 MB/s (0.6%)
 NEON LD1/ST1 2-pass copy (from framebuffer)          :    115.0 MB/s (1.0%)
 ARM LDP/STP copy (from framebuffer)                  :    112.0 MB/s (0.8%)
 ARM LDP/STP 2-pass copy (from framebuffer)           :    113.9 MB/s (1.0%)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    2.2 ns          /     2.8 ns 
    524288 :    3.2 ns          /     3.7 ns 
   1048576 :    5.9 ns          /     8.2 ns 
   2097152 :   16.7 ns          /    23.4 ns 
   4194304 :   23.3 ns          /    28.9 ns 
   8388608 :   29.2 ns          /    33.1 ns 
  16777216 :   38.6 ns          /    46.8 ns 
  33554432 :   69.7 ns          /    91.5 ns 
  67108864 :   89.8 ns          /   110.3 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    1.8 ns          /     2.2 ns 
    524288 :    2.1 ns          /     2.4 ns 
   1048576 :    2.4 ns          /     2.5 ns 
   2097152 :   15.1 ns          /    21.9 ns 
   4194304 :   21.6 ns          /    27.2 ns 
   8388608 :   24.8 ns          /    28.8 ns 
  16777216 :   26.8 ns          /    29.9 ns 
  33554432 :   63.1 ns          /    84.0 ns 
  67108864 :   82.0 ns          /   101.2 ns 

Executing benchmark on cpu6 (Neoverse-N1):

tinymembench v0.4.9 (simple benchmark for memory throughput and latency)

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 3: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 4: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                     :  12406.4 MB/s
 C copy backwards (32 byte blocks)                    :  12325.1 MB/s
 C copy backwards (64 byte blocks)                    :  12325.8 MB/s
 C copy                                               :  12567.5 MB/s
 C copy prefetched (32 bytes step)                    :  13141.9 MB/s
 C copy prefetched (64 bytes step)                    :  13163.2 MB/s
 C 2-pass copy                                        :   5892.8 MB/s (0.5%)
 C 2-pass copy prefetched (32 bytes step)             :   7905.1 MB/s (0.1%)
 C 2-pass copy prefetched (64 bytes step)             :   7791.5 MB/s
 C fill                                               :  47045.4 MB/s
 C fill (shuffle within 16 byte blocks)               :  47040.5 MB/s
 C fill (shuffle within 32 byte blocks)               :  47043.4 MB/s
 C fill (shuffle within 64 byte blocks)               :  47016.8 MB/s
 ---
 standard memcpy                                      :  13260.7 MB/s
 standard memset                                      :  47962.0 MB/s
 ---
 NEON LDP/STP copy                                    :  13346.7 MB/s
 NEON LDP/STP copy pldl2strm (32 bytes step)          :  14770.4 MB/s (0.1%)
 NEON LDP/STP copy pldl2strm (64 bytes step)          :  14790.0 MB/s
 NEON LDP/STP copy pldl1keep (32 bytes step)          :  15006.7 MB/s (0.1%)
 NEON LDP/STP copy pldl1keep (64 bytes step)          :  14960.9 MB/s
 NEON LD1/ST1 copy                                    :  13369.7 MB/s
 NEON STP fill                                        :  47943.4 MB/s
 NEON STNP fill                                       :  47967.6 MB/s
 ARM LDP/STP copy                                     :  13393.0 MB/s
 ARM STP fill                                         :  47954.4 MB/s
 ARM STNP fill                                        :  47930.8 MB/s

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)                 :    113.3 MB/s (1.0%)
 NEON LDP/STP 2-pass copy (from framebuffer)          :    114.3 MB/s (1.1%)
 NEON LD1/ST1 copy (from framebuffer)                 :    115.5 MB/s (0.7%)
 NEON LD1/ST1 2-pass copy (from framebuffer)          :    114.1 MB/s (1.0%)
 ARM LDP/STP copy (from framebuffer)                  :    112.9 MB/s (1.0%)
 ARM LDP/STP 2-pass copy (from framebuffer)           :    114.7 MB/s (1.1%)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    2.2 ns          /     2.8 ns 
    524288 :    3.2 ns          /     3.6 ns 
   1048576 :    6.6 ns          /     9.4 ns 
   2097152 :   16.6 ns          /    23.4 ns 
   4194304 :   23.3 ns          /    28.9 ns 
   8388608 :   29.1 ns          /    33.1 ns 
  16777216 :   38.5 ns          /    48.2 ns 
  33554432 :   69.6 ns          /    91.5 ns 
  67108864 :   90.0 ns          /   110.6 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    1.8 ns          /     2.2 ns 
    524288 :    2.1 ns          /     2.4 ns 
   1048576 :    2.4 ns          /     2.5 ns 
   2097152 :   15.1 ns          /    21.9 ns 
   4194304 :   21.6 ns          /    27.2 ns 
   8388608 :   24.8 ns          /    28.8 ns 
  16777216 :   26.8 ns          /    29.9 ns 
  33554432 :   63.1 ns          /    84.0 ns 
  67108864 :   82.1 ns          /   101.3 ns 

Executing benchmark on cpu7 (Neoverse-N1):

tinymembench v0.4.9 (simple benchmark for memory throughput and latency)

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 3: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 4: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                     :  12402.6 MB/s
 C copy backwards (32 byte blocks)                    :  12319.4 MB/s
 C copy backwards (64 byte blocks)                    :  12324.9 MB/s
 C copy                                               :  12564.0 MB/s
 C copy prefetched (32 bytes step)                    :  13130.0 MB/s (0.2%)
 C copy prefetched (64 bytes step)                    :  13143.1 MB/s
 C 2-pass copy                                        :   5896.5 MB/s
 C 2-pass copy prefetched (32 bytes step)             :   7901.9 MB/s
 C 2-pass copy prefetched (64 bytes step)             :   7798.9 MB/s
 C fill                                               :  47024.9 MB/s
 C fill (shuffle within 16 byte blocks)               :  47044.1 MB/s
 C fill (shuffle within 32 byte blocks)               :  47038.3 MB/s
 C fill (shuffle within 64 byte blocks)               :  47034.9 MB/s
 ---
 standard memcpy                                      :  13278.2 MB/s
 standard memset                                      :  47970.2 MB/s
 ---
 NEON LDP/STP copy                                    :  13352.7 MB/s
 NEON LDP/STP copy pldl2strm (32 bytes step)          :  14769.8 MB/s (0.2%)
 NEON LDP/STP copy pldl2strm (64 bytes step)          :  14808.3 MB/s
 NEON LDP/STP copy pldl1keep (32 bytes step)          :  15014.8 MB/s (0.1%)
 NEON LDP/STP copy pldl1keep (64 bytes step)          :  14981.4 MB/s (0.1%)
 NEON LD1/ST1 copy                                    :  13379.2 MB/s
 NEON STP fill                                        :  47971.4 MB/s
 NEON STNP fill                                       :  47971.5 MB/s
 ARM LDP/STP copy                                     :  13407.1 MB/s
 ARM STP fill                                         :  47927.7 MB/s
 ARM STNP fill                                        :  47925.4 MB/s

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)                 :    113.1 MB/s (2.6%)
 NEON LDP/STP 2-pass copy (from framebuffer)          :    114.3 MB/s (1.1%)
 NEON LD1/ST1 copy (from framebuffer)                 :    115.6 MB/s (3.0%)
 NEON LD1/ST1 2-pass copy (from framebuffer)          :    114.8 MB/s (3.3%)
 ARM LDP/STP copy (from framebuffer)                  :    112.4 MB/s (1.0%)
 ARM LDP/STP 2-pass copy (from framebuffer)           :    114.9 MB/s (1.1%)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    2.2 ns          /     2.7 ns 
    524288 :    3.2 ns          /     3.6 ns 
   1048576 :    6.7 ns          /     9.4 ns 
   2097152 :   16.7 ns          /    23.4 ns 
   4194304 :   23.4 ns          /    28.9 ns 
   8388608 :   29.2 ns          /    33.1 ns 
  16777216 :   38.5 ns          /    47.7 ns 
  33554432 :   69.5 ns          /    91.4 ns 
  67108864 :   89.7 ns          /   110.3 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    1.8 ns          /     2.2 ns 
    524288 :    2.1 ns          /     2.4 ns 
   1048576 :    2.4 ns          /     2.5 ns 
   2097152 :   15.1 ns          /    21.9 ns 
   4194304 :   21.6 ns          /    27.2 ns 
   8388608 :   24.8 ns          /    28.8 ns 
  16777216 :   26.8 ns          /    30.0 ns 
  33554432 :   63.1 ns          /    84.1 ns 
  67108864 :   82.1 ns          /   101.3 ns 

Executing benchmark on cpu8 (Neoverse-N1):

tinymembench v0.4.9 (simple benchmark for memory throughput and latency)

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 3: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 4: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                     :  12394.4 MB/s
 C copy backwards (32 byte blocks)                    :  12293.4 MB/s
 C copy backwards (64 byte blocks)                    :  12300.8 MB/s
 C copy                                               :  12546.5 MB/s
 C copy prefetched (32 bytes step)                    :  13040.9 MB/s
 C copy prefetched (64 bytes step)                    :  13053.0 MB/s
 C 2-pass copy                                        :   5876.1 MB/s (0.6%)
 C 2-pass copy prefetched (32 bytes step)             :   7918.8 MB/s (0.1%)
 C 2-pass copy prefetched (64 bytes step)             :   7780.1 MB/s (0.1%)
 C fill                                               :  47049.1 MB/s
 C fill (shuffle within 16 byte blocks)               :  47034.9 MB/s
 C fill (shuffle within 32 byte blocks)               :  47049.2 MB/s
 C fill (shuffle within 64 byte blocks)               :  47016.9 MB/s
 ---
 standard memcpy                                      :  13175.5 MB/s
 standard memset                                      :  47934.0 MB/s
 ---
 NEON LDP/STP copy                                    :  13277.4 MB/s
 NEON LDP/STP copy pldl2strm (32 bytes step)          :  14694.7 MB/s (0.1%)
 NEON LDP/STP copy pldl2strm (64 bytes step)          :  14727.1 MB/s
 NEON LDP/STP copy pldl1keep (32 bytes step)          :  14914.8 MB/s (0.1%)
 NEON LDP/STP copy pldl1keep (64 bytes step)          :  14873.4 MB/s
 NEON LD1/ST1 copy                                    :  13307.7 MB/s
 NEON STP fill                                        :  47934.1 MB/s
 NEON STNP fill                                       :  47963.5 MB/s
 ARM LDP/STP copy                                     :  13349.0 MB/s
 ARM STP fill                                         :  47949.0 MB/s
 ARM STNP fill                                        :  47958.5 MB/s

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)                 :    112.7 MB/s (1.1%)
 NEON LDP/STP 2-pass copy (from framebuffer)          :    114.6 MB/s (1.0%)
 NEON LD1/ST1 copy (from framebuffer)                 :    113.3 MB/s (3.8%)
 NEON LD1/ST1 2-pass copy (from framebuffer)          :    115.0 MB/s (3.4%)
 ARM LDP/STP copy (from framebuffer)                  :    112.6 MB/s (1.2%)
 ARM LDP/STP 2-pass copy (from framebuffer)           :    114.8 MB/s (1.2%)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    2.2 ns          /     2.8 ns 
    524288 :    3.2 ns          /     3.6 ns 
   1048576 :    6.9 ns          /    10.0 ns 
   2097152 :   17.2 ns          /    24.5 ns 
   4194304 :   23.9 ns          /    30.4 ns 
   8388608 :   29.3 ns          /    34.8 ns 
  16777216 :   39.3 ns          /    49.5 ns 
  33554432 :   69.9 ns          /    92.1 ns 
  67108864 :   90.3 ns          /   111.0 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    1.8 ns          /     2.2 ns 
    524288 :    2.1 ns          /     2.4 ns 
   1048576 :    2.4 ns          /     2.5 ns 
   2097152 :   15.6 ns          /    22.9 ns 
   4194304 :   22.3 ns          /    28.8 ns 
   8388608 :   25.6 ns          /    30.6 ns 
  16777216 :   27.7 ns          /    31.8 ns 
  33554432 :   63.5 ns          /    84.6 ns 
  67108864 :   82.2 ns          /   101.6 ns 

Executing benchmark on cpu9 (Neoverse-N1):

tinymembench v0.4.9 (simple benchmark for memory throughput and latency)

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 3: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 4: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                     :  12392.8 MB/s
 C copy backwards (32 byte blocks)                    :  12297.1 MB/s
 C copy backwards (64 byte blocks)                    :  12300.4 MB/s
 C copy                                               :  12536.0 MB/s
 C copy prefetched (32 bytes step)                    :  13037.1 MB/s
 C copy prefetched (64 bytes step)                    :  13049.1 MB/s
 C 2-pass copy                                        :   5731.3 MB/s
 C 2-pass copy prefetched (32 bytes step)             :   7548.0 MB/s
 C 2-pass copy prefetched (64 bytes step)             :   7162.4 MB/s (0.2%)
 C fill                                               :  47041.0 MB/s
 C fill (shuffle within 16 byte blocks)               :  47045.6 MB/s
 C fill (shuffle within 32 byte blocks)               :  47050.0 MB/s
 C fill (shuffle within 64 byte blocks)               :  47028.0 MB/s
 ---
 standard memcpy                                      :  13165.0 MB/s
 standard memset                                      :  47966.5 MB/s
 ---
 NEON LDP/STP copy                                    :  13264.5 MB/s
 NEON LDP/STP copy pldl2strm (32 bytes step)          :  14662.6 MB/s (0.2%)
 NEON LDP/STP copy pldl2strm (64 bytes step)          :  14711.7 MB/s (0.1%)
 NEON LDP/STP copy pldl1keep (32 bytes step)          :  14901.6 MB/s (0.1%)
 NEON LDP/STP copy pldl1keep (64 bytes step)          :  14862.5 MB/s (0.1%)
 NEON LD1/ST1 copy                                    :  13301.1 MB/s
 NEON STP fill                                        :  47968.3 MB/s
 NEON STNP fill                                       :  47942.7 MB/s
 ARM LDP/STP copy                                     :  13342.6 MB/s
 ARM STP fill                                         :  47926.4 MB/s
 ARM STNP fill                                        :  47935.2 MB/s

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)                 :    112.4 MB/s (2.2%)
 NEON LDP/STP 2-pass copy (from framebuffer)          :    111.0 MB/s
 NEON LD1/ST1 copy (from framebuffer)                 :    115.9 MB/s (0.9%)
 NEON LD1/ST1 2-pass copy (from framebuffer)          :    114.7 MB/s (3.4%)
 ARM LDP/STP copy (from framebuffer)                  :    112.0 MB/s (3.2%)
 ARM LDP/STP 2-pass copy (from framebuffer)           :    113.9 MB/s (0.8%)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    2.2 ns          /     2.7 ns 
    524288 :    3.2 ns          /     3.6 ns 
   1048576 :    6.9 ns          /     9.9 ns 
   2097152 :   17.1 ns          /    24.5 ns 
   4194304 :   24.0 ns          /    30.4 ns 
   8388608 :   30.0 ns          /    34.8 ns 
  16777216 :   39.3 ns          /    49.0 ns 
  33554432 :   70.0 ns          /    92.0 ns 
  67108864 :   90.0 ns          /   110.6 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.7 ns 
    262144 :    1.8 ns          /     2.2 ns 
    524288 :    2.1 ns          /     2.4 ns 
   1048576 :    2.3 ns          /     2.5 ns 
   2097152 :   15.8 ns          /    22.9 ns 
   4194304 :   22.8 ns          /    28.8 ns 
   8388608 :   26.0 ns          /    30.6 ns 
  16777216 :   27.7 ns          /    31.8 ns 
  33554432 :   63.6 ns          /    84.7 ns 
  67108864 :   82.2 ns          /   101.6 ns 

##########################################################################

Executing ramlat on cpu0 (Neoverse-N1), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 1.334 1.334 1.334 1.334 1.334 1.335 1.334 2.528 
         8k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.600 
        16k: 1.335 1.334 1.334 1.334 1.335 1.334 1.334 2.599 
        32k: 1.334 1.334 1.335 1.334 1.334 1.335 1.334 2.603 
        64k: 1.335 1.335 1.335 1.335 1.335 1.335 1.335 2.603 
       128k: 4.670 4.670 4.668 4.669 4.669 5.164 6.162 10.11 
       256k: 5.337 5.486 5.330 5.483 5.336 5.354 6.504 10.11 
       512k: 6.822 6.492 6.634 6.497 6.648 6.592 7.542 11.26 
      1024k: 17.31 12.73 17.40 12.70 17.19 13.96 14.34 18.72 
      2048k: 31.37 28.32 31.20 28.32 31.22 28.59 27.63 31.42 
      4096k: 32.30 31.15 31.88 31.12 31.88 31.24 31.29 32.33 
      8192k: 36.88 34.26 36.26 34.27 36.66 34.74 35.58 36.73 
     16384k: 56.41 42.08 50.43 41.44 51.13 44.19 42.91 50.89 

Executing ramlat on cpu1 (Neoverse-N1), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.527 
         8k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.599 
        16k: 1.334 1.334 1.334 1.334 1.334 1.334 1.335 2.600 
        32k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.602 
        64k: 1.335 1.335 1.335 1.334 1.335 1.334 1.335 2.602 
       128k: 4.689 4.690 4.690 4.689 4.688 5.240 6.374 10.11 
       256k: 5.336 5.479 5.335 5.482 5.336 5.355 6.501 10.11 
       512k: 6.270 6.242 6.176 6.242 6.183 6.268 7.260 11.00 
      1024k: 17.44 12.99 17.89 12.99 17.65 14.37 14.83 19.33 
      2048k: 30.48 28.32 30.41 28.34 30.40 28.54 27.64 30.85 
      4096k: 32.26 31.17 31.86 31.15 31.78 31.27 31.25 32.47 
      8192k: 36.49 34.04 36.10 34.10 36.15 34.48 35.44 36.69 
     16384k: 55.80 41.91 51.14 41.52 50.67 44.21 43.09 51.21 

Executing ramlat on cpu2 (Neoverse-N1), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.526 
         8k: 1.334 1.334 1.334 1.334 1.337 1.334 1.335 2.599 
        16k: 1.334 1.335 1.334 1.334 1.334 1.334 1.334 2.599 
        32k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.603 
        64k: 1.335 1.334 1.334 1.334 1.335 1.335 1.334 2.602 
       128k: 4.689 4.691 4.689 4.689 4.689 5.238 6.373 10.11 
       256k: 5.357 5.350 5.357 5.352 5.355 5.362 6.512 10.12 
       512k: 5.347 5.497 5.336 5.496 5.336 5.364 6.509 10.11 
      1024k: 13.64 10.95 13.71 10.95 13.56 11.84 12.39 16.18 
      2048k: 31.03 28.74 30.87 28.76 30.88 28.86 27.77 31.43 
      4096k: 32.04 30.92 31.69 30.93 31.71 31.18 31.28 32.40 
      8192k: 36.97 34.06 36.04 34.02 36.18 34.42 35.35 36.67 
     16384k: 56.80 42.37 50.85 41.46 50.75 44.15 43.03 51.33 

Executing ramlat on cpu3 (Neoverse-N1), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.526 
         8k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.599 
        16k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.599 
        32k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.602 
        64k: 1.335 1.334 1.334 1.334 1.335 1.334 1.335 2.602 
       128k: 4.710 4.711 4.709 4.711 4.711 5.229 6.390 10.12 
       256k: 5.347 5.363 5.344 5.369 5.346 5.354 6.506 10.11 
       512k: 7.247 6.947 7.247 6.951 7.243 7.121 7.870 11.45 
      1024k: 16.91 13.95 17.27 13.93 17.06 14.87 15.16 18.98 
      2048k: 30.59 27.90 30.44 27.92 30.48 28.28 27.37 30.82 
      4096k: 32.19 30.93 31.66 30.92 31.69 31.10 31.19 32.31 
      8192k: 36.54 34.09 36.14 34.02 36.24 34.51 35.39 36.58 
     16384k: 55.68 41.91 50.61 41.39 50.54 44.33 42.98 51.30 

Executing ramlat on cpu4 (Neoverse-N1), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.526 
         8k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.599 
        16k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.600 
        32k: 1.334 1.334 1.335 1.334 1.334 1.334 1.334 2.603 
        64k: 1.335 1.334 1.335 1.334 1.335 1.335 1.334 2.602 
       128k: 4.668 4.669 4.668 4.669 4.668 5.217 6.180 10.11 
       256k: 5.347 5.367 5.346 5.366 5.346 5.362 6.508 10.11 
       512k: 5.920 5.708 5.850 5.701 5.858 5.703 6.828 10.66 
      1024k: 16.67 13.54 17.06 13.54 16.98 14.36 14.78 18.59 
      2048k: 30.22 27.75 30.01 27.76 30.07 27.99 27.02 30.41 
      4096k: 31.95 31.08 31.92 31.16 31.91 31.26 31.28 32.46 
      8192k: 36.45 34.08 36.23 34.15 36.42 34.61 35.43 36.71 
     16384k: 56.20 41.90 51.11 41.52 50.91 44.29 43.17 51.64 

Executing ramlat on cpu5 (Neoverse-N1), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.525 
         8k: 1.334 1.334 1.334 1.334 1.334 1.334 1.335 2.599 
        16k: 1.334 1.334 1.334 1.335 1.334 1.334 1.336 2.602 
        32k: 1.334 1.334 1.334 1.335 1.334 1.334 1.334 2.603 
        64k: 1.335 1.334 1.334 1.334 1.335 1.334 1.334 2.602 
       128k: 4.689 4.690 4.688 4.689 4.690 5.238 6.355 10.11 
       256k: 5.432 5.360 5.368 5.372 5.397 5.377 6.524 10.11 
       512k: 5.721 5.524 5.577 5.459 5.507 5.455 6.545 10.13 
      1024k: 15.02 11.42 15.06 11.42 14.88 12.44 13.03 17.28 
      2048k: 31.04 28.12 30.86 28.16 30.86 28.51 27.55 31.24 
      4096k: 32.35 31.10 31.82 31.10 31.79 31.18 31.23 32.41 
      8192k: 36.49 34.11 36.30 34.16 36.23 34.65 35.40 36.72 
     16384k: 55.98 41.87 50.43 41.56 50.58 43.98 42.94 51.18 

Executing ramlat on cpu6 (Neoverse-N1), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.526 
         8k: 1.334 1.334 1.335 1.334 1.334 1.334 1.334 2.600 
        16k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.599 
        32k: 1.334 1.334 1.334 1.335 1.334 1.335 1.334 2.602 
        64k: 1.335 1.334 1.334 1.335 1.336 1.334 1.334 2.602 
       128k: 4.668 4.671 4.668 4.668 4.668 5.203 6.182 10.11 
       256k: 5.337 5.482 5.332 5.487 5.336 5.354 6.502 10.11 
       512k: 5.345 5.493 5.338 5.494 5.338 5.366 6.509 10.11 
      1024k: 17.67 13.79 17.68 13.84 17.55 14.78 15.05 19.20 
      2048k: 30.02 27.98 29.94 28.01 29.93 28.09 27.31 30.44 
      4096k: 32.04 31.07 31.92 31.13 31.94 31.26 31.24 32.46 
      8192k: 36.58 34.11 36.37 34.04 36.30 34.63 35.45 36.75 
     16384k: 55.37 41.72 50.28 41.04 50.16 43.76 42.59 50.52 

Executing ramlat on cpu7 (Neoverse-N1), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.529 
         8k: 1.334 1.335 1.334 1.334 1.334 1.334 1.334 2.600 
        16k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.599 
        32k: 1.334 1.334 1.334 1.335 1.334 1.334 1.334 2.602 
        64k: 1.336 1.334 1.334 1.335 1.335 1.336 1.334 2.603 
       128k: 4.669 4.674 4.668 4.668 4.668 5.177 6.205 10.12 
       256k: 5.345 5.396 5.347 5.382 5.342 5.356 6.504 10.12 
       512k: 6.159 5.754 5.919 5.751 5.915 5.739 6.768 10.50 
      1024k: 19.24 15.46 19.46 15.46 19.32 16.61 16.66 20.82 
      2048k: 30.49 27.75 30.28 27.80 30.22 28.06 27.17 30.60 
      4096k: 31.99 31.03 31.88 31.09 31.88 31.19 31.23 32.40 
      8192k: 36.54 34.16 36.17 34.04 36.47 34.63 35.39 36.72 
     16384k: 54.56 41.29 49.94 40.89 49.80 43.61 42.48 50.01 

Executing ramlat on cpu8 (Neoverse-N1), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.526 
         8k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.599 
        16k: 1.334 1.335 1.334 1.334 1.334 1.334 1.334 2.600 
        32k: 1.334 1.334 1.334 1.335 1.334 1.334 1.334 2.602 
        64k: 1.335 1.334 1.335 1.335 1.335 1.335 1.334 2.602 
       128k: 4.691 4.690 4.689 4.690 4.689 5.228 6.375 10.12 
       256k: 5.344 5.376 5.341 5.379 5.342 5.352 6.506 10.11 
       512k: 5.364 5.400 5.360 5.400 5.355 5.370 6.526 10.11 
      1024k: 15.91 13.49 16.26 13.49 15.85 13.60 14.11 18.08 
      2048k: 30.99 29.35 30.78 29.39 30.83 27.49 27.30 31.20 
      4096k: 33.38 32.43 32.82 32.42 32.81 30.76 30.96 33.26 
      8192k: 38.03 35.63 37.30 35.63 37.16 33.93 34.66 37.13 
     16384k: 55.96 43.02 51.08 42.34 50.71 43.10 42.35 50.75 

Executing ramlat on cpu9 (Neoverse-N1), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.529 
         8k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.601 
        16k: 1.334 1.334 1.334 1.334 1.334 1.334 1.335 2.599 
        32k: 1.334 1.334 1.334 1.334 1.334 1.334 1.334 2.603 
        64k: 1.335 1.334 1.334 1.335 1.335 1.335 1.334 2.602 
       128k: 4.668 4.671 4.668 4.668 4.669 5.236 6.202 10.11 
       256k: 5.347 5.374 5.346 5.372 5.343 5.364 6.507 10.11 
       512k: 6.215 6.251 6.214 6.251 6.208 6.293 7.176 10.78 
      1024k: 14.90 12.73 14.89 12.66 14.78 13.05 13.62 17.36 
      2048k: 31.77 30.21 31.65 30.23 31.67 27.83 27.57 32.03 
      4096k: 33.20 32.45 32.98 32.47 32.97 30.63 30.91 33.29 
      8192k: 37.64 35.68 37.31 35.67 37.29 34.08 34.72 37.17 
     16384k: 56.26 43.00 51.37 42.58 53.11 43.92 42.73 51.27 

##########################################################################

Executing benchmark on each cluster individually

OpenSSL 1.1.1f, built on 31 Mar 2020
type             16 bytes     64 bytes    256 bytes   1024 bytes   8192 bytes  16384 bytes
aes-128-cbc     903132.04k  1699211.22k  2174485.16k  2326034.77k  2384390.83k  2391490.56k (Neoverse-N1)
aes-128-cbc     910001.69k  1698287.51k  2177111.21k  2327231.83k  2384524.63k  2391834.62k (Neoverse-N1)
aes-128-cbc     899391.71k  1695608.92k  2174506.15k  2326828.71k  2384663.89k  2391676.25k (Neoverse-N1)
aes-128-cbc     895686.89k  1703127.94k  2175211.01k  2325484.54k  2384516.44k  2391709.01k (Neoverse-N1)
aes-128-cbc     901739.31k  1698358.04k  2174618.37k  2327239.68k  2384609.28k  2391714.47k (Neoverse-N1)
aes-128-cbc     896267.15k  1698891.56k  2174970.88k  2326662.49k  2384426.33k  2391780.01k (Neoverse-N1)
aes-128-cbc     894706.44k  1698973.65k  2177735.25k  2324986.88k  2384259.75k  2391506.94k (Neoverse-N1)
aes-128-cbc     900374.13k  1698985.17k  2174944.26k  2326294.87k  2384560.13k  2391736.32k (Neoverse-N1)
aes-128-cbc     901383.83k  1701603.33k  2174877.10k  2325841.58k  2384674.82k  2391627.09k (Neoverse-N1)
aes-128-cbc     894276.03k  1698589.21k  2174857.22k  2327145.81k  2384609.28k  2391747.24k (Neoverse-N1)
aes-192-cbc     844082.43k  1487199.25k  1840803.50k  1924354.39k  1988954.79k  1994216.79k (Neoverse-N1)
aes-192-cbc     844570.77k  1487691.75k  1840942.76k  1926155.61k  1989356.20k  1994298.71k (Neoverse-N1)
aes-192-cbc     841086.55k  1487759.68k  1841608.87k  1924597.76k  1989473.62k  1994293.25k (Neoverse-N1)
aes-192-cbc     839067.31k  1487585.69k  1840493.14k  1924501.50k  1989522.77k  1994145.79k (Neoverse-N1)
aes-192-cbc     845264.57k  1487658.84k  1840874.15k  1924489.90k  1989498.20k  1994309.63k (Neoverse-N1)
aes-192-cbc     833173.27k  1487720.83k  1840911.10k  1924444.50k  1989429.93k  1994304.17k (Neoverse-N1)
aes-192-cbc     844864.95k  1487726.68k  1840895.83k  1924304.90k  1988793.69k  1993998.34k (Neoverse-N1)
aes-192-cbc     845472.79k  1487570.09k  1840743.34k  1924552.02k  1989416.28k  1994255.02k (Neoverse-N1)
aes-192-cbc     844466.55k  1487809.02k  1840849.92k  1924586.50k  1989457.24k  1994265.94k (Neoverse-N1)
aes-192-cbc     833947.27k  1487695.34k  1840856.83k  1924503.55k  1989541.89k  1994298.71k (Neoverse-N1)
aes-256-cbc     786555.52k  1322120.81k  1595435.01k  1674133.50k  1706232.49k  1709921.62k (Neoverse-N1)
aes-256-cbc     785554.77k  1322455.13k  1595839.91k  1674584.06k  1706366.29k  1710014.46k (Neoverse-N1)
aes-256-cbc     786402.28k  1323176.90k  1595378.77k  1674676.91k  1706401.79k  1709959.85k (Neoverse-N1)
aes-256-cbc     788933.39k  1322035.63k  1595141.80k  1673540.61k  1706308.95k  1709801.47k (Neoverse-N1)
aes-256-cbc     785927.58k  1323082.77k  1595714.73k  1674326.36k  1706363.56k  1709932.54k (Neoverse-N1)
aes-256-cbc     789402.69k  1321556.22k  1595574.44k  1674485.76k  1706147.84k  1709998.08k (Neoverse-N1)
aes-256-cbc     786293.97k  1322679.25k  1595891.80k  1674537.98k  1706016.77k  1709839.70k (Neoverse-N1)
aes-256-cbc     785481.55k  1322593.17k  1595176.36k  1674328.41k  1706120.53k  1709752.32k (Neoverse-N1)
aes-256-cbc     785479.79k  1323109.53k  1595478.44k  1674234.20k  1706390.87k  1709899.78k (Neoverse-N1)
aes-256-cbc     785611.37k  1322668.63k  1595441.15k  1674339.67k  1706431.83k  1709681.32k (Neoverse-N1)

##########################################################################

Executing benchmark single-threaded on cpu0 (Neoverse-N1)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,256 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:  384732 MB,  # CPU hardware threads: 256
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       4657   100   4533   4531  |      49399   100   4219   4218
23:       4107   100   4185   4185  |      48880   100   4232   4231
24:       3760   100   4044   4043  |      48217   100   4234   4233
25:       3513   100   4012   4011  |      47265   100   4208   4207
----------------------------------  | ------------------------------
Avr:             100   4194   4192  |              100   4223   4222
Tot:             100   4209   4207

Executing benchmark single-threaded on cpu1 (Neoverse-N1)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,256 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:  384732 MB,  # CPU hardware threads: 256
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       4668   100   4542   4542  |      49412   100   4219   4219
23:       4109   100   4188   4187  |      48883   100   4232   4231
24:       3760   100   4044   4044  |      48246   100   4236   4236
25:       3514   100   4014   4013  |      47273   100   4209   4208
----------------------------------  | ------------------------------
Avr:             100   4197   4196  |              100   4224   4223
Tot:             100   4211   4210

Executing benchmark single-threaded on cpu2 (Neoverse-N1)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,256 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:  384732 MB,  # CPU hardware threads: 256
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       4658   100   4532   4532  |      49392   100   4218   4217
23:       4101   100   4179   4179  |      48914   100   4234   4234
24:       3757   100   4041   4040  |      48262   100   4237   4237
25:       3512   100   4010   4010  |      47283   100   4209   4208
----------------------------------  | ------------------------------
Avr:             100   4191   4190  |              100   4225   4224
Tot:             100   4208   4207

Executing benchmark single-threaded on cpu3 (Neoverse-N1)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,256 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:  384732 MB,  # CPU hardware threads: 256
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       4672   100   4546   4546  |      49411   100   4219   4219
23:       4110   100   4188   4188  |      48865   100   4230   4230
24:       3765   100   4049   4048  |      48210   100   4234   4232
25:       3521   100   4021   4020  |      47240   100   4205   4205
----------------------------------  | ------------------------------
Avr:             100   4201   4201  |              100   4222   4221
Tot:             100   4212   4211

Executing benchmark single-threaded on cpu4 (Neoverse-N1)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,256 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:  384732 MB,  # CPU hardware threads: 256
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       4666   100   4540   4539  |      49416   100   4220   4219
23:       4101   100   4180   4179  |      48902   100   4234   4233
24:       3752   100   4036   4034  |      48237   100   4236   4235
25:       3508   100   4007   4005  |      47243   100   4206   4205
----------------------------------  | ------------------------------
Avr:             100   4191   4190  |              100   4224   4223
Tot:             100   4207   4206

Executing benchmark single-threaded on cpu5 (Neoverse-N1)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,256 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:  384732 MB,  # CPU hardware threads: 256
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       4667   100   4540   4540  |      49414   100   4219   4219
23:       4105   100   4183   4183  |      48896   100   4234   4232
24:       3757   100   4041   4040  |      48239   100   4235   4235
25:       3504   100   4002   4001  |      47278   100   4209   4208
----------------------------------  | ------------------------------
Avr:             100   4192   4191  |              100   4224   4224
Tot:             100   4208   4207

Executing benchmark single-threaded on cpu6 (Neoverse-N1)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,256 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:  384732 MB,  # CPU hardware threads: 256
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       4660   100   4534   4534  |      49389   100   4217   4217
23:       4094   100   4172   4171  |      48929   100   4236   4235
24:       3748   100   4030   4030  |      48202   100   4233   4232
25:       3509   100   4007   4007  |      47286   100   4209   4209
----------------------------------  | ------------------------------
Avr:             100   4186   4186  |              100   4224   4223
Tot:             100   4205   4204

Executing benchmark single-threaded on cpu7 (Neoverse-N1)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,256 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:  384732 MB,  # CPU hardware threads: 256
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       4668   100   4542   4542  |      49400   100   4219   4218
23:       4091   100   4169   4169  |      48908   100   4234   4234
24:       3747   100   4030   4029  |      48241   100   4235   4235
25:       3497   100   3994   3994  |      47260   100   4207   4206
----------------------------------  | ------------------------------
Avr:             100   4184   4183  |              100   4224   4223
Tot:             100   4204   4203

Executing benchmark single-threaded on cpu8 (Neoverse-N1)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,256 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:  384732 MB,  # CPU hardware threads: 256
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       4641   100   4516   4515  |      49391   100   4217   4217
23:       4081   100   4159   4158  |      48874   100   4232   4231
24:       3738   100   4020   4020  |      48189   100   4231   4231
25:       3489   100   3985   3985  |      47248   100   4206   4205
----------------------------------  | ------------------------------
Avr:             100   4170   4170  |              100   4222   4221
Tot:             100   4196   4195

Executing benchmark single-threaded on cpu9 (Neoverse-N1)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,256 CPUs LE)

LE
CPU Freq: - - - 64000000 - - - - -

RAM size:  384732 MB,  # CPU hardware threads: 256
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       4632   100   4507   4506  |      49400   100   4219   4218
23:       4076   100   4154   4153  |      48821   100   4226   4226
24:       3735   100   4017   4016  |      48184   100   4230   4230
25:       3487   100   3982   3982  |      47255   100   4206   4206
----------------------------------  | ------------------------------
Avr:             100   4165   4164  |              100   4220   4220
Tot:             100   4193   4192

##########################################################################

Executing benchmark 3 times multi-threaded on CPUs 0-255

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,256 CPUs LE)

LE
CPU Freq: - - 64000000 - - 256000000 512000000 - -

RAM size:  384732 MB,  # CPU hardware threads: 256
RAM usage:  56480 MB,  # Benchmark threads:    256

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:     212318 20858    991 206544  |    8365681 18887   3779 713339
23:     170719 17128   1016 173942  |    7779059 18106   3719 673000
24:     173606 18559   1006 186661  |    6816094 16372   3656 598187
25:     224513 17681   1451 256341  |    7909276 20375   3456 703815
----------------------------------  | ------------------------------
Avr:           18556   1116 205872  |            18435   3653 672085
Tot:           18496   2384 438979

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,256 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:  384732 MB,  # CPU hardware threads: 256
RAM usage:  56480 MB,  # Benchmark threads:    256

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:     203356 20307    975 197825  |    7659380 17203   3799 653113
23:     180441 17328   1062 183849  |    8250667 19081   3743 713801
24:     182084 19229   1019 195777  |    7593455 18730   3560 666410
25:     236594 16971   1593 270134  |    7464592 20494   3243 664244
----------------------------------  | ------------------------------
Avr:           18459   1162 211896  |            18877   3586 674392
Tot:           18668   2374 443144

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,256 CPUs LE)

LE
CPU Freq: 64000000 64000000 - - - - - - 2048000000

RAM size:  384732 MB,  # CPU hardware threads: 256
RAM usage:  56480 MB,  # Benchmark threads:    256

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:     215962 19507   1078 210089  |    7831087 18015   3709 667754
23:     167879 16834   1017 171049  |    7921706 19788   3465 685341
24:     164775 17576   1009 177166  |    7316646 17836   3602 642116
25:     168489 19746    975 192375  |    6043710 16361   3289 537806
----------------------------------  | ------------------------------
Avr:           18416   1019 187670  |            18000   3516 633254
Tot:           18208   2268 410462

Compression: 205872,211896,187670
Decompression: 672085,674392,633254
Total: 438979,443144,410462

##########################################################################

** cpuminer-multi 1.3.7 by tpruvot@github **
BTC donation address: 1FhDPLPpw18X4srecguG3MxJYe4a1JsZnd (tpruvot)

[2023-01-04 03:16:47] 256 miner threads started, using 'scrypt' algorithm.

Total Scores: 

##########################################################################

Testing maximum cpufreq again, still under full load. System health now:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
03:21:35: 3000/3000MHz 255.15  99%   0%  99%   0%   0%   0%      Â°C

Checking cpufreq OPP for cpu0 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2999 (2999.294/2999.224/2998.946)

Checking cpufreq OPP for cpu1 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2999 (2999.224/2999.085/2999.015)

Checking cpufreq OPP for cpu2 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2999 (2999.224/2999.085/2998.876)

Checking cpufreq OPP for cpu3 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2999 (2999.224/2999.155/2999.155)

Checking cpufreq OPP for cpu4 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2999 (2999.224/2999.224/2999.155)

Checking cpufreq OPP for cpu5 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2999 (2999.224/2999.155/2999.085)

Checking cpufreq OPP for cpu6 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2999 (2999.224/2999.224/2999.015)

Checking cpufreq OPP for cpu7 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2999 (2999.155/2999.155/2999.155)

Checking cpufreq OPP for cpu8 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2999 (2999.224/2999.155/2998.946)

Checking cpufreq OPP for cpu9-cpu255 (Neoverse-N1):

Cpufreq OPP: 3000    Measured: 2999 (2999.224/2999.155/2999.085)

##########################################################################

Hardware sensors:

apm_xgene-isa-0000
SoC Temperature:  +50.0 C  
CPU power:        22.96 W  
IO power:         22.02 W  

apm_xgene-isa-0000
SoC Temperature:  +50.0 C  
CPU power:        22.40 W  
IO power:         18.02 W  

##########################################################################

System health while running tinymembench:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
02:12:19: 3000/3000MHz  0.97  36%   0%  35%   0%   0%   0%      Â°C
02:18:59: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:25:39: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:32:19: 3000/3000MHz  1.02   0%   0%   0%   0%   0%   0%      Â°C
02:38:59: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:45:39: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:52:19: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C

System health while running ramlat:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
02:52:33: 3000/3000MHz  1.00  23%   0%  22%   0%   0%   0%      Â°C
02:53:03: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:53:33: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:54:03: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:54:34: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:55:04: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:55:34: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:56:04: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C

System health while running OpenSSL benchmark:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
02:56:18: 3000/3000MHz  1.00  22%   0%  21%   0%   0%   0%      Â°C
02:56:34: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:56:50: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:57:06: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:57:22: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:57:38: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:57:54: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:58:10: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:58:26: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:58:42: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:58:58: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:59:15: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:59:31: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
02:59:47: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:00:03: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:00:19: 3000/3000MHz  1.06   0%   0%   0%   0%   0%   0%      Â°C
03:00:35: 3000/3000MHz  1.05   0%   0%   0%   0%   0%   0%      Â°C
03:00:51: 3000/3000MHz  1.04   0%   0%   0%   0%   0%   0%      Â°C
03:01:07: 3000/3000MHz  1.03   0%   0%   0%   0%   0%   0%      Â°C
03:01:23: 3000/3000MHz  1.02   0%   0%   0%   0%   0%   0%      Â°C
03:01:39: 3000/3000MHz  1.01   0%   0%   0%   0%   0%   0%      Â°C
03:01:55: 3000/3000MHz  1.01   0%   0%   0%   0%   0%   0%      Â°C
03:02:11: 3000/3000MHz  1.01   0%   0%   0%   0%   0%   0%      Â°C
03:02:27: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:02:43: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:02:59: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:03:15: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:03:31: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:03:47: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:04:03: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:04:19: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:04:35: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:04:51: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:05:07: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C

System health while running 7-zip single core benchmark:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
03:05:19: 3000/3000MHz  1.00  20%   0%  20%   0%   0%   0%      Â°C
03:05:34: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:05:49: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:06:04: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:06:19: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:06:34: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:06:49: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:07:04: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:07:19: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:07:34: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:07:49: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:08:04: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:08:19: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:08:34: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:08:49: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:09:04: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:09:19: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:09:34: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:09:49: 3000/3000MHz  1.00   0%   0%   0%   0%   0%   0%      Â°C
03:10:04: 3000/3000MHz  1.08   0%   0%   0%   0%   0%   0%      Â°C
03:10:19: 3000/3000MHz  1.06   0%   0%   0%   0%   0%   0%      Â°C
03:10:34: 3000/3000MHz  1.12   0%   0%   0%   0%   0%   0%      Â°C
03:10:50: 3000/3000MHz  1.09   0%   0%   0%   0%   0%   0%      Â°C

System health while running 7-zip multi core benchmark:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
03:11:00: 3000/3000MHz  1.08  19%   0%  19%   0%   0%   0%      Â°C
03:11:10: 3000/3000MHz 41.37  61%   1%  59%   0%   0%   0%      Â°C
03:11:20: 3000/3000MHz 53.95  45%   0%  44%   0%   0%   0%      Â°C
03:11:30: 3000/3000MHz 89.07  55%   4%  51%   0%   0%   0%      Â°C
03:11:40: 3000/3000MHz 85.18  31%   0%  31%   0%   0%   0%      Â°C
03:11:52: 3000/3000MHz 97.54  41%   4%  37%   0%   0%   0%      Â°C
03:12:03: 3000/3000MHz 117.10  70%   1%  69%   0%   0%   0%      Â°C
03:12:13: 3000/3000MHz 104.40   4%   0%   3%   0%   0%   0%      Â°C
03:12:23: 3000/3000MHz 88.51   0%   0%   0%   0%   0%   0%      Â°C
03:12:33: 3000/3000MHz 85.30  42%   4%  38%   0%   0%   0%      Â°C
03:12:43: 3000/3000MHz 112.24  80%   6%  74%   0%   0%   0%      Â°C
03:12:53: 3000/3000MHz 113.93  45%   3%  41%   0%   0%   0%      Â°C
03:13:03: 3000/3000MHz 117.71  62%   1%  60%   0%   0%   0%      Â°C
03:13:14: 3000/3000MHz 112.44  44%   0%  43%   0%   0%   0%      Â°C
03:13:24: 3000/3000MHz 138.08  53%   4%  49%   0%   0%   0%      Â°C
03:13:34: 3000/3000MHz 117.00  30%   0%  30%   0%   0%   0%      Â°C
03:13:45: 3000/3000MHz 123.83  46%   4%  41%   0%   0%   0%      Â°C
03:13:56: 3000/3000MHz 141.79  74%   2%  71%   0%   0%   0%      Â°C
03:14:06: 3000/3000MHz 120.15   1%   0%   1%   0%   0%   0%      Â°C
03:14:16: 3000/3000MHz 101.83   0%   0%   0%   0%   0%   0%      Â°C
03:14:27: 3000/3000MHz 109.71  47%   9%  37%   0%   0%   0%      Â°C
03:14:37: 3000/3000MHz 127.86  67%  10%  57%   0%   0%   0%      Â°C
03:14:47: 3000/3000MHz 123.81  46%   6%  39%   0%   0%   0%      Â°C
03:14:57: 3000/3000MHz 122.23  60%   5%  55%   0%   0%   0%      Â°C
03:15:07: 3000/3000MHz 122.37  40%   0%  40%   0%   0%   0%      Â°C
03:15:19: 3000/3000MHz 125.05  65%   3%  61%   0%   0%   0%      Â°C
03:15:29: 3000/3000MHz 114.92  21%   0%  21%   0%   0%   0%      Â°C
03:15:39: 3000/3000MHz 142.35  60%   5%  55%   0%   0%   0%      Â°C
03:15:50: 3000/3000MHz 144.20  62%   3%  58%   0%   0%   0%      Â°C
03:16:00: 3000/3000MHz 122.19   6%   0%   6%   0%   0%   0%      Â°C
03:16:10: 3000/3000MHz 103.55   0%   0%   0%   0%   0%   0%      Â°C
03:16:20: 3000/3000MHz 111.89  47%   7%  39%   0%   0%   0%      Â°C
03:16:31: 3000/3000MHz 139.10  98%   4%  93%   0%   0%   0%      Â°C
03:16:41: 3000/3000MHz 133.32  50%   6%  43%   0%   0%   0%      Â°C

System health while running cpuminer:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
03:16:47: 3000/3000MHz 143.14  20%   0%  20%   0%   0%   0%      Â°C
03:17:28: 3000/3000MHz 198.12  96%   0%  96%   0%   0%   0%      Â°C
03:18:09: 3000/3000MHz 226.32  98%   0%  98%   0%   0%   0%      Â°C
03:18:51: 3000/3000MHz 242.04  99%   0%  99%   0%   0%   0%      Â°C
03:19:32: 3000/3000MHz 248.84  99%   0%  99%   0%   0%   0%      Â°C
03:20:13: 3000/3000MHz 252.33  99%   0%  99%   0%   0%   0%      Â°C
03:20:54: 3000/3000MHz 254.12  99%   0%  99%   0%   0%   0%      Â°C
03:21:35: 3000/3000MHz 255.15  99%   0%  99%   0%   0%   0%      Â°C

##########################################################################

Linux 5.4.219-sg1-dk (p6410-server) 	01/04/23 	_aarch64_	(256 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          23.13    0.00    0.52    0.00    0.00   76.35

Device             tps    kB_read/s    kB_wrtn/s    kB_dscd/s    kB_read    kB_wrtn    kB_dscd
nvme0n1          10.61       443.48        71.44         0.00    3687409     594009          0

              total        used        free      shared  buff/cache   available
Mem:          375Gi       3.6Gi       371Gi       4.0Mi       593Mi       370Gi
Swap:         8.0Gi          0B       8.0Gi

Filename				Type		Size	Used	Priority
/swap.img                              	file    	8388604	0	-2

CPU sysfs topology (clusters, cpufreq members, clockspeeds)
                 cpufreq   min    max
 CPU    cluster  policy   speed  speed   core type
  0       56        0     1000    3000   Neoverse-N1 / r3p1
  1       56        1     1000    3000   Neoverse-N1 / r3p1
  2       56        2     1000    3000   Neoverse-N1 / r3p1
  3       56        3     1000    3000   Neoverse-N1 / r3p1
  4       56        4     1000    3000   Neoverse-N1 / r3p1
  5       56        5     1000    3000   Neoverse-N1 / r3p1
  6       56        6     1000    3000   Neoverse-N1 / r3p1
  7       56        7     1000    3000   Neoverse-N1 / r3p1
  8       56        8     1000    3000   Neoverse-N1 / r3p1
  9       56        9     1000    3000   Neoverse-N1 / r3p1
 10       56       10     1000    3000   Neoverse-N1 / r3p1
 11       56       11     1000    3000   Neoverse-N1 / r3p1
 12       56       12     1000    3000   Neoverse-N1 / r3p1
 13       56       13     1000    3000   Neoverse-N1 / r3p1
 14       56       14     1000    3000   Neoverse-N1 / r3p1
 15       56       15     1000    3000   Neoverse-N1 / r3p1
 16       56       16     1000    3000   Neoverse-N1 / r3p1
 17       56       17     1000    3000   Neoverse-N1 / r3p1
 18       56       18     1000    3000   Neoverse-N1 / r3p1
 19       56       19     1000    3000   Neoverse-N1 / r3p1
 20       56       20     1000    3000   Neoverse-N1 / r3p1
 21       56       21     1000    3000   Neoverse-N1 / r3p1
 22       56       22     1000    3000   Neoverse-N1 / r3p1
 23       56       23     1000    3000   Neoverse-N1 / r3p1
 24       56       24     1000    3000   Neoverse-N1 / r3p1
 25       56       25     1000    3000   Neoverse-N1 / r3p1
 26       56       26     1000    3000   Neoverse-N1 / r3p1
 27       56       27     1000    3000   Neoverse-N1 / r3p1
 28       56       28     1000    3000   Neoverse-N1 / r3p1
 29       56       29     1000    3000   Neoverse-N1 / r3p1
 30       56       30     1000    3000   Neoverse-N1 / r3p1
 31       56       31     1000    3000   Neoverse-N1 / r3p1
 32       56       32     1000    3000   Neoverse-N1 / r3p1
 33       56       33     1000    3000   Neoverse-N1 / r3p1
 34       56       34     1000    3000   Neoverse-N1 / r3p1
 35       56       35     1000    3000   Neoverse-N1 / r3p1
 36       56       36     1000    3000   Neoverse-N1 / r3p1
 37       56       37     1000    3000   Neoverse-N1 / r3p1
 38       56       38     1000    3000   Neoverse-N1 / r3p1
 39       56       39     1000    3000   Neoverse-N1 / r3p1
 40       56       40     1000    3000   Neoverse-N1 / r3p1
 41       56       41     1000    3000   Neoverse-N1 / r3p1
 42       56       42     1000    3000   Neoverse-N1 / r3p1
 43       56       43     1000    3000   Neoverse-N1 / r3p1
 44       56       44     1000    3000   Neoverse-N1 / r3p1
 45       56       45     1000    3000   Neoverse-N1 / r3p1
 46       56       46     1000    3000   Neoverse-N1 / r3p1
 47       56       47     1000    3000   Neoverse-N1 / r3p1
 48       56       48     1000    3000   Neoverse-N1 / r3p1
 49       56       49     1000    3000   Neoverse-N1 / r3p1
 50       56       50     1000    3000   Neoverse-N1 / r3p1
 51       56       51     1000    3000   Neoverse-N1 / r3p1
 52       56       52     1000    3000   Neoverse-N1 / r3p1
 53       56       53     1000    3000   Neoverse-N1 / r3p1
 54       56       54     1000    3000   Neoverse-N1 / r3p1
 55       56       55     1000    3000   Neoverse-N1 / r3p1
 56       56       56     1000    3000   Neoverse-N1 / r3p1
 57       56       57     1000    3000   Neoverse-N1 / r3p1
 58       56       58     1000    3000   Neoverse-N1 / r3p1
 59       56       59     1000    3000   Neoverse-N1 / r3p1
 60       56       60     1000    3000   Neoverse-N1 / r3p1
 61       56       61     1000    3000   Neoverse-N1 / r3p1
 62       56       62     1000    3000   Neoverse-N1 / r3p1
 63       56       63     1000    3000   Neoverse-N1 / r3p1
 64       56       64     1000    3000   Neoverse-N1 / r3p1
 65       56       65     1000    3000   Neoverse-N1 / r3p1
 66       56       66     1000    3000   Neoverse-N1 / r3p1
 67       56       67     1000    3000   Neoverse-N1 / r3p1
 68       56       68     1000    3000   Neoverse-N1 / r3p1
 69       56       69     1000    3000   Neoverse-N1 / r3p1
 70       56       70     1000    3000   Neoverse-N1 / r3p1
 71       56       71     1000    3000   Neoverse-N1 / r3p1
 72       56       72     1000    3000   Neoverse-N1 / r3p1
 73       56       73     1000    3000   Neoverse-N1 / r3p1
 74       56       74     1000    3000   Neoverse-N1 / r3p1
 75       56       75     1000    3000   Neoverse-N1 / r3p1
 76       56       76     1000    3000   Neoverse-N1 / r3p1
 77       56       77     1000    3000   Neoverse-N1 / r3p1
 78       56       78     1000    3000   Neoverse-N1 / r3p1
 79       56       79     1000    3000   Neoverse-N1 / r3p1
 80       56       80     1000    3000   Neoverse-N1 / r3p1
 81       56       81     1000    3000   Neoverse-N1 / r3p1
 82       56       82     1000    3000   Neoverse-N1 / r3p1
 83       56       83     1000    3000   Neoverse-N1 / r3p1
 84       56       84     1000    3000   Neoverse-N1 / r3p1
 85       56       85     1000    3000   Neoverse-N1 / r3p1
 86       56       86     1000    3000   Neoverse-N1 / r3p1
 87       56       87     1000    3000   Neoverse-N1 / r3p1
 88       56       88     1000    3000   Neoverse-N1 / r3p1
 89       56       89     1000    3000   Neoverse-N1 / r3p1
 90       56       90     1000    3000   Neoverse-N1 / r3p1
 91       56       91     1000    3000   Neoverse-N1 / r3p1
 92       56       92     1000    3000   Neoverse-N1 / r3p1
 93       56       93     1000    3000   Neoverse-N1 / r3p1
 94       56       94     1000    3000   Neoverse-N1 / r3p1
 95       56       95     1000    3000   Neoverse-N1 / r3p1
 96       56       96     1000    3000   Neoverse-N1 / r3p1
 97       56       97     1000    3000   Neoverse-N1 / r3p1
 98       56       98     1000    3000   Neoverse-N1 / r3p1
 99       56       99     1000    3000   Neoverse-N1 / r3p1
100       56      100     1000    3000   Neoverse-N1 / r3p1
101       56      101     1000    3000   Neoverse-N1 / r3p1
102       56      102     1000    3000   Neoverse-N1 / r3p1
103       56      103     1000    3000   Neoverse-N1 / r3p1
104       56      104     1000    3000   Neoverse-N1 / r3p1
105       56      105     1000    3000   Neoverse-N1 / r3p1
106       56      106     1000    3000   Neoverse-N1 / r3p1
107       56      107     1000    3000   Neoverse-N1 / r3p1
108       56      108     1000    3000   Neoverse-N1 / r3p1
109       56      109     1000    3000   Neoverse-N1 / r3p1
110       56      110     1000    3000   Neoverse-N1 / r3p1
111       56      111     1000    3000   Neoverse-N1 / r3p1
112       56      112     1000    3000   Neoverse-N1 / r3p1
113       56      113     1000    3000   Neoverse-N1 / r3p1
114       56      114     1000    3000   Neoverse-N1 / r3p1
115       56      115     1000    3000   Neoverse-N1 / r3p1
116       56      116     1000    3000   Neoverse-N1 / r3p1
117       56      117     1000    3000   Neoverse-N1 / r3p1
118       56      118     1000    3000   Neoverse-N1 / r3p1
119       56      119     1000    3000   Neoverse-N1 / r3p1
120       56      120     1000    3000   Neoverse-N1 / r3p1
121       56      121     1000    3000   Neoverse-N1 / r3p1
122       56      122     1000    3000   Neoverse-N1 / r3p1
123       56      123     1000    3000   Neoverse-N1 / r3p1
124       56      124     1000    3000   Neoverse-N1 / r3p1
125       56      125     1000    3000   Neoverse-N1 / r3p1
126       56      126     1000    3000   Neoverse-N1 / r3p1
127       56      127     1000    3000   Neoverse-N1 / r3p1
128       76      128     1000    3000   Neoverse-N1 / r3p1
129       76      129     1000    3000   Neoverse-N1 / r3p1
130       76      130     1000    3000   Neoverse-N1 / r3p1
131       76      131     1000    3000   Neoverse-N1 / r3p1
132       76      132     1000    3000   Neoverse-N1 / r3p1
133       76      133     1000    3000   Neoverse-N1 / r3p1
134       76      134     1000    3000   Neoverse-N1 / r3p1
135       76      135     1000    3000   Neoverse-N1 / r3p1
136       76      136     1000    3000   Neoverse-N1 / r3p1
137       76      137     1000    3000   Neoverse-N1 / r3p1
138       76      138     1000    3000   Neoverse-N1 / r3p1
139       76      139     1000    3000   Neoverse-N1 / r3p1
140       76      140     1000    3000   Neoverse-N1 / r3p1
141       76      141     1000    3000   Neoverse-N1 / r3p1
142       76      142     1000    3000   Neoverse-N1 / r3p1
143       76      143     1000    3000   Neoverse-N1 / r3p1
144       76      144     1000    3000   Neoverse-N1 / r3p1
145       76      145     1000    3000   Neoverse-N1 / r3p1
146       76      146     1000    3000   Neoverse-N1 / r3p1
147       76      147     1000    3000   Neoverse-N1 / r3p1
148       76      148     1000    3000   Neoverse-N1 / r3p1
149       76      149     1000    3000   Neoverse-N1 / r3p1
150       76      150     1000    3000   Neoverse-N1 / r3p1
151       76      151     1000    3000   Neoverse-N1 / r3p1
152       76      152     1000    3000   Neoverse-N1 / r3p1
153       76      153     1000    3000   Neoverse-N1 / r3p1
154       76      154     1000    3000   Neoverse-N1 / r3p1
155       76      155     1000    3000   Neoverse-N1 / r3p1
156       76      156     1000    3000   Neoverse-N1 / r3p1
157       76      157     1000    3000   Neoverse-N1 / r3p1
158       76      158     1000    3000   Neoverse-N1 / r3p1
159       76      159     1000    3000   Neoverse-N1 / r3p1
160       76      160     1000    3000   Neoverse-N1 / r3p1
161       76      161     1000    3000   Neoverse-N1 / r3p1
162       76      162     1000    3000   Neoverse-N1 / r3p1
163       76      163     1000    3000   Neoverse-N1 / r3p1
164       76      164     1000    3000   Neoverse-N1 / r3p1
165       76      165     1000    3000   Neoverse-N1 / r3p1
166       76      166     1000    3000   Neoverse-N1 / r3p1
167       76      167     1000    3000   Neoverse-N1 / r3p1
168       76      168     1000    3000   Neoverse-N1 / r3p1
169       76      169     1000    3000   Neoverse-N1 / r3p1
170       76      170     1000    3000   Neoverse-N1 / r3p1
171       76      171     1000    3000   Neoverse-N1 / r3p1
172       76      172     1000    3000   Neoverse-N1 / r3p1
173       76      173     1000    3000   Neoverse-N1 / r3p1
174       76      174     1000    3000   Neoverse-N1 / r3p1
175       76      175     1000    3000   Neoverse-N1 / r3p1
176       76      176     1000    3000   Neoverse-N1 / r3p1
177       76      177     1000    3000   Neoverse-N1 / r3p1
178       76      178     1000    3000   Neoverse-N1 / r3p1
179       76      179     1000    3000   Neoverse-N1 / r3p1
180       76      180     1000    3000   Neoverse-N1 / r3p1
181       76      181     1000    3000   Neoverse-N1 / r3p1
182       76      182     1000    3000   Neoverse-N1 / r3p1
183       76      183     1000    3000   Neoverse-N1 / r3p1
184       76      184     1000    3000   Neoverse-N1 / r3p1
185       76      185     1000    3000   Neoverse-N1 / r3p1
186       76      186     1000    3000   Neoverse-N1 / r3p1
187       76      187     1000    3000   Neoverse-N1 / r3p1
188       76      188     1000    3000   Neoverse-N1 / r3p1
189       76      189     1000    3000   Neoverse-N1 / r3p1
190       76      190     1000    3000   Neoverse-N1 / r3p1
191       76      191     1000    3000   Neoverse-N1 / r3p1
192       76      192     1000    3000   Neoverse-N1 / r3p1
193       76      193     1000    3000   Neoverse-N1 / r3p1
194       76      194     1000    3000   Neoverse-N1 / r3p1
195       76      195     1000    3000   Neoverse-N1 / r3p1
196       76      196     1000    3000   Neoverse-N1 / r3p1
197       76      197     1000    3000   Neoverse-N1 / r3p1
198       76      198     1000    3000   Neoverse-N1 / r3p1
199       76      199     1000    3000   Neoverse-N1 / r3p1
200       76      200     1000    3000   Neoverse-N1 / r3p1
201       76      201     1000    3000   Neoverse-N1 / r3p1
202       76      202     1000    3000   Neoverse-N1 / r3p1
203       76      203     1000    3000   Neoverse-N1 / r3p1
204       76      204     1000    3000   Neoverse-N1 / r3p1
205       76      205     1000    3000   Neoverse-N1 / r3p1
206       76      206     1000    3000   Neoverse-N1 / r3p1
207       76      207     1000    3000   Neoverse-N1 / r3p1
208       76      208     1000    3000   Neoverse-N1 / r3p1
209       76      209     1000    3000   Neoverse-N1 / r3p1
210       76      210     1000    3000   Neoverse-N1 / r3p1
211       76      211     1000    3000   Neoverse-N1 / r3p1
212       76      212     1000    3000   Neoverse-N1 / r3p1
213       76      213     1000    3000   Neoverse-N1 / r3p1
214       76      214     1000    3000   Neoverse-N1 / r3p1
215       76      215     1000    3000   Neoverse-N1 / r3p1
216       76      216     1000    3000   Neoverse-N1 / r3p1
217       76      217     1000    3000   Neoverse-N1 / r3p1
218       76      218     1000    3000   Neoverse-N1 / r3p1
219       76      219     1000    3000   Neoverse-N1 / r3p1
220       76      220     1000    3000   Neoverse-N1 / r3p1
221       76      221     1000    3000   Neoverse-N1 / r3p1
222       76      222     1000    3000   Neoverse-N1 / r3p1
223       76      223     1000    3000   Neoverse-N1 / r3p1
224       76      224     1000    3000   Neoverse-N1 / r3p1
225       76      225     1000    3000   Neoverse-N1 / r3p1
226       76      226     1000    3000   Neoverse-N1 / r3p1
227       76      227     1000    3000   Neoverse-N1 / r3p1
228       76      228     1000    3000   Neoverse-N1 / r3p1
229       76      229     1000    3000   Neoverse-N1 / r3p1
230       76      230     1000    3000   Neoverse-N1 / r3p1
231       76      231     1000    3000   Neoverse-N1 / r3p1
232       76      232     1000    3000   Neoverse-N1 / r3p1
233       76      233     1000    3000   Neoverse-N1 / r3p1
234       76      234     1000    3000   Neoverse-N1 / r3p1
235       76      235     1000    3000   Neoverse-N1 / r3p1
236       76      236     1000    3000   Neoverse-N1 / r3p1
237       76      237     1000    3000   Neoverse-N1 / r3p1
238       76      238     1000    3000   Neoverse-N1 / r3p1
239       76      239     1000    3000   Neoverse-N1 / r3p1
240       76      240     1000    3000   Neoverse-N1 / r3p1
241       76      241     1000    3000   Neoverse-N1 / r3p1
242       76      242     1000    3000   Neoverse-N1 / r3p1
243       76      243     1000    3000   Neoverse-N1 / r3p1
244       76      244     1000    3000   Neoverse-N1 / r3p1
245       76      245     1000    3000   Neoverse-N1 / r3p1
246       76      246     1000    3000   Neoverse-N1 / r3p1
247       76      247     1000    3000   Neoverse-N1 / r3p1
248       76      248     1000    3000   Neoverse-N1 / r3p1
249       76      249     1000    3000   Neoverse-N1 / r3p1
250       76      250     1000    3000   Neoverse-N1 / r3p1
251       76      251     1000    3000   Neoverse-N1 / r3p1
252       76      252     1000    3000   Neoverse-N1 / r3p1
253       76      253     1000    3000   Neoverse-N1 / r3p1
254       76      254     1000    3000   Neoverse-N1 / r3p1
255       76      255     1000    3000   Neoverse-N1 / r3p1

Architecture:                    aarch64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
CPU(s):                          256
On-line CPU(s) list:             0-255
Thread(s) per core:              1
Core(s) per socket:              128
Socket(s):                       2
NUMA node(s):                    2
Vendor ID:                       ARM
Model:                           1
Model name:                      Neoverse-N1
Stepping:                        r3p1
CPU max MHz:                     3000.0000
CPU min MHz:                     1000.0000
BogoMIPS:                        50.00
L1d cache:                       16 MiB
L1i cache:                       16 MiB
L2 cache:                        256 MiB
NUMA node0 CPU(s):               0-127
NUMA node1 CPU(s):               128-255
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Not affected
Vulnerability Retbleed:          Not affected
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1:        Mitigation; __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; CSV2, but not BHB
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm lrcpc dcpop asimddp ssbs

Processor Information
	Socket Designation: CPU 0
	Type: Central Processor
	Family: ARMv8
	Manufacturer: Ampere(R)
	ID: 02 00 16 0A A1 00 00 00
	Signature: Implementor 0x0a, Variant 0x1, Architecture 6, Part 0x000, Revision 2
	Version: Ampere(R) Altra(R) Max Processor
	Voltage: 1.0 V
	External Clock: 2000 MHz
	Max Speed: 3000 MHz
	Current Speed: 3000 MHz
	Status: Populated, Enabled
	Asset Tag: 00000001
	Part Number: M128-30
	Core Count: 128
	Core Enabled: 128
	Thread Count: 128
	Characteristics:
		64-bit capable
		Multi-Core
		Power/Performance Control

Processor Information
	Socket Designation: CPU 1
	Type: Central Processor
	Family: ARMv8
	Manufacturer: Ampere(R)
	ID: 02 00 16 0A A1 00 00 00
	Signature: Implementor 0x0a, Variant 0x1, Architecture 6, Part 0x000, Revision 2
	Version: Ampere(R) Altra(R) Max Processor
	Voltage: 1.0 V
	External Clock: 2000 MHz
	Max Speed: 3000 MHz
	Current Speed: 3000 MHz
	Status: Populated, Enabled
	Asset Tag: 00000001
	Part Number: M128-30
	Core Count: 128
	Core Enabled: 128
	Thread Count: 128
	Characteristics:
		64-bit capable
		Multi-Core
		Power/Performance Control

Signature: 560NeoverseN1r3p1561NeoverseN1r3p1562NeoverseN1r3p1563NeoverseN1r3p1564NeoverseN1r3p1565NeoverseN1r3p1566NeoverseN1r3p1567NeoverseN1r3p1568NeoverseN1r3p1569NeoverseN1r3p15610NeoverseN1r3p15611NeoverseN1r3p15612NeoverseN1r3p15613NeoverseN1r3p15614NeoverseN1r3p15615NeoverseN1r3p15616NeoverseN1r3p15617NeoverseN1r3p15618NeoverseN1r3p15619NeoverseN1r3p15620NeoverseN1r3p15621NeoverseN1r3p15622NeoverseN1r3p15623NeoverseN1r3p15624NeoverseN1r3p15625NeoverseN1r3p15626NeoverseN1r3p15627NeoverseN1r3p15628NeoverseN1r3p15629NeoverseN1r3p15630NeoverseN1r3p15631NeoverseN1r3p15632NeoverseN1r3p15633NeoverseN1r3p15634NeoverseN1r3p15635NeoverseN1r3p15636NeoverseN1r3p15637NeoverseN1r3p15638NeoverseN1r3p15639NeoverseN1r3p15640NeoverseN1r3p15641NeoverseN1r3p15642NeoverseN1r3p15643NeoverseN1r3p15644NeoverseN1r3p15645NeoverseN1r3p15646NeoverseN1r3p15647NeoverseN1r3p15648NeoverseN1r3p15649NeoverseN1r3p15650NeoverseN1r3p15651NeoverseN1r3p15652NeoverseN1r3p15653NeoverseN1r3p15654NeoverseN1r3p15655NeoverseN1r3p15656NeoverseN1r3p15657NeoverseN1r3p15658NeoverseN1r3p15659NeoverseN1r3p15660NeoverseN1r3p15661NeoverseN1r3p15662NeoverseN1r3p15663NeoverseN1r3p15664NeoverseN1r3p15665NeoverseN1r3p15666NeoverseN1r3p15667NeoverseN1r3p15668NeoverseN1r3p15669NeoverseN1r3p15670NeoverseN1r3p15671NeoverseN1r3p15672NeoverseN1r3p15673NeoverseN1r3p15674NeoverseN1r3p15675NeoverseN1r3p15676NeoverseN1r3p15677NeoverseN1r3p15678NeoverseN1r3p15679NeoverseN1r3p15680NeoverseN1r3p15681NeoverseN1r3p15682NeoverseN1r3p15683NeoverseN1r3p15684NeoverseN1r3p15685NeoverseN1r3p15686NeoverseN1r3p15687NeoverseN1r3p15688NeoverseN1r3p15689NeoverseN1r3p15690NeoverseN1r3p15691NeoverseN1r3p15692NeoverseN1r3p15693NeoverseN1r3p15694NeoverseN1r3p15695NeoverseN1r3p15696NeoverseN1r3p15697NeoverseN1r3p15698NeoverseN1r3p15699NeoverseN1r3p156100NeoverseN1r3p156101NeoverseN1r3p156102NeoverseN1r3p156103NeoverseN1r3p156104NeoverseN1r3p156105NeoverseN1r3p156106NeoverseN1r3p156107NeoverseN1r3p156108NeoverseN1r3p156109NeoverseN1r3p156110NeoverseN1r3p156111NeoverseN1r3p156112NeoverseN1r3p156113NeoverseN1r3p156114NeoverseN1r3p156115NeoverseN1r3p156116NeoverseN1r3p156117NeoverseN1r3p156118NeoverseN1r3p156119NeoverseN1r3p156120NeoverseN1r3p156121NeoverseN1r3p156122NeoverseN1r3p156123NeoverseN1r3p156124NeoverseN1r3p156125NeoverseN1r3p156126NeoverseN1r3p156127NeoverseN1r3p176128NeoverseN1r3p176129NeoverseN1r3p176130NeoverseN1r3p176131NeoverseN1r3p176132NeoverseN1r3p176133NeoverseN1r3p176134NeoverseN1r3p176135NeoverseN1r3p176136NeoverseN1r3p176137NeoverseN1r3p176138NeoverseN1r3p176139NeoverseN1r3p176140NeoverseN1r3p176141NeoverseN1r3p176142NeoverseN1r3p176143NeoverseN1r3p176144NeoverseN1r3p176145NeoverseN1r3p176146NeoverseN1r3p176147NeoverseN1r3p176148NeoverseN1r3p176149NeoverseN1r3p176150NeoverseN1r3p176151NeoverseN1r3p176152NeoverseN1r3p176153NeoverseN1r3p176154NeoverseN1r3p176155NeoverseN1r3p176156NeoverseN1r3p176157NeoverseN1r3p176158NeoverseN1r3p176159NeoverseN1r3p176160NeoverseN1r3p176161NeoverseN1r3p176162NeoverseN1r3p176163NeoverseN1r3p176164NeoverseN1r3p176165NeoverseN1r3p176166NeoverseN1r3p176167NeoverseN1r3p176168NeoverseN1r3p176169NeoverseN1r3p176170NeoverseN1r3p176171NeoverseN1r3p176172NeoverseN1r3p176173NeoverseN1r3p176174NeoverseN1r3p176175NeoverseN1r3p176176NeoverseN1r3p176177NeoverseN1r3p176178NeoverseN1r3p176179NeoverseN1r3p176180NeoverseN1r3p176181NeoverseN1r3p176182NeoverseN1r3p176183NeoverseN1r3p176184NeoverseN1r3p176185NeoverseN1r3p176186NeoverseN1r3p176187NeoverseN1r3p176188NeoverseN1r3p176189NeoverseN1r3p176190NeoverseN1r3p176191NeoverseN1r3p176192NeoverseN1r3p176193NeoverseN1r3p176194NeoverseN1r3p176195NeoverseN1r3p176196NeoverseN1r3p176197NeoverseN1r3p176198NeoverseN1r3p176199NeoverseN1r3p176200NeoverseN1r3p176201NeoverseN1r3p176202NeoverseN1r3p176203NeoverseN1r3p176204NeoverseN1r3p176205NeoverseN1r3p176206NeoverseN1r3p176207NeoverseN1r3p176208NeoverseN1r3p176209NeoverseN1r3p176210NeoverseN1r3p176211NeoverseN1r3p176212NeoverseN1r3p176213NeoverseN1r3p176214NeoverseN1r3p176215NeoverseN1r3p176216NeoverseN1r3p176217NeoverseN1r3p176218NeoverseN1r3p176219NeoverseN1r3p176220NeoverseN1r3p176221NeoverseN1r3p176222NeoverseN1r3p176223NeoverseN1r3p176224NeoverseN1r3p176225NeoverseN1r3p176226NeoverseN1r3p176227NeoverseN1r3p176228NeoverseN1r3p176229NeoverseN1r3p176230NeoverseN1r3p176231NeoverseN1r3p176232NeoverseN1r3p176233NeoverseN1r3p176234NeoverseN1r3p176235NeoverseN1r3p176236NeoverseN1r3p176237NeoverseN1r3p176238NeoverseN1r3p176239NeoverseN1r3p176240NeoverseN1r3p176241NeoverseN1r3p176242NeoverseN1r3p176243NeoverseN1r3p176244NeoverseN1r3p176245NeoverseN1r3p176246NeoverseN1r3p176247NeoverseN1r3p176248NeoverseN1r3p176249NeoverseN1r3p176250NeoverseN1r3p176251NeoverseN1r3p176252NeoverseN1r3p176253NeoverseN1r3p176254NeoverseN1r3p176255NeoverseN1r3p1
 Compiler: /usr/bin/gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0 / aarch64-linux-gnu
 Userland: arm64
   Kernel: 5.4.219-sg1-dk/aarch64
           CONFIG_HZ=250
           CONFIG_HZ_250=y
           CONFIG_PREEMPT_NOTIFIERS=y
           CONFIG_PREEMPT_VOLUNTARY=y
           raid6: neonx8   gen() 13155 MB/s
           raid6: neonx8   xor() 11185 MB/s
           raid6: neonx4   gen() 12796 MB/s
           raid6: neonx4   xor() 11255 MB/s
           raid6: neonx2   gen() 11901 MB/s
           raid6: neonx2   xor() 10137 MB/s
           raid6: neonx1   gen()  8062 MB/s
           raid6: neonx1   xor()  7609 MB/s
           raid6: int64x8  gen()  6952 MB/s
           raid6: int64x8  xor()  3879 MB/s
           raid6: int64x4  gen()  7009 MB/s
           raid6: int64x4  xor()  3822 MB/s
           raid6: int64x2  gen()  5673 MB/s
           raid6: int64x2  xor()  3075 MB/s
           raid6: int64x1  gen()  3866 MB/s
           raid6: int64x1  xor()  2072 MB/s
           raid6: using algorithm neonx8 gen() 13155 MB/s
           raid6: .... xor() 11185 MB/s, rmw enabled
           raid6: using neon recovery algorithm
           xor: measuring software checksum speed
           xor: using function: arm64_neon (32559.000 MB/sec)

DIMM configuration:
     *-bank:0
          description: DIMM DDR4 3200 MHz (0.3 ns)
          product: M393A4K40EB3-CWE
          vendor: Samsung
          physical id: 0
          configured speed: 3200MT/s
          size: 32GiB
          width: 64 bits
          clock: 3200MHz (0.3ns)
     *-bank:1
          description: DIMM [empty]
          product: Array 1 Part Number 2
          vendor: Array 1 Manufacturer 2
          physical id: 1
          slot: DIMM001
     *-bank:2
          description: DIMM DDR4 3200 MHz (0.3 ns)
          product: M393A4K40EB3-CWE
          vendor: Samsung
          physical id: 2
          configured speed: 3200MT/s
          size: 32GiB
          width: 64 bits
          clock: 3200MHz (0.3ns)
     *-bank:3
          description: DIMM [empty]
          product: Array 1 Part Number 4
          vendor: Array 1 Manufacturer 4
          physical id: 3
          slot: DIMM011
     *-bank:4
          description: DIMM DDR4 3200 MHz (0.3 ns)
          product: M393A4K40EB3-CWE
          vendor: Samsung
          physical id: 4
          configured speed: 3200MT/s
          size: 32GiB
          width: 64 bits
          clock: 3200MHz (0.3ns)
     *-bank:5
          description: DIMM [empty]
          product: Array 1 Part Number 6
          vendor: Array 1 Manufacturer 6
          physical id: 5
          slot: DIMM021
     *-bank:6
          description: DIMM [empty]
          product: Array 1 Part Number 7
          vendor: Array 1 Manufacturer 7
          physical id: 6
          slot: DIMM030
     *-bank:7
          description: DIMM [empty]
          product: Array 1 Part Number 8
          vendor: Array 1 Manufacturer 8
          physical id: 7
          slot: DIMM031
     *-bank:8
          description: DIMM DDR4 3200 MHz (0.3 ns)
          product: M393A4K40EB3-CWE
          vendor: Samsung
          physical id: 8
          configured speed: 3200MT/s
          size: 32GiB
          width: 64 bits
          clock: 3200MHz (0.3ns)
     *-bank:9
          description: DIMM [empty]
          product: Array 1 Part Number 10
          vendor: Array 1 Manufacturer 10
          physical id: 9
          slot: DIMM041
     *-bank:10
          description: DIMM DDR4 3200 MHz (0.3 ns)
          product: M393A4K40EB3-CWE
          vendor: Samsung
          physical id: a
          configured speed: 3200MT/s
          size: 32GiB
          width: 64 bits
          clock: 3200MHz (0.3ns)
     *-bank:11
          description: DIMM [empty]
          product: Array 1 Part Number 12
          vendor: Array 1 Manufacturer 12
          physical id: b
          slot: DIMM051
     *-bank:12
          description: DIMM DDR4 3200 MHz (0.3 ns)
          product: M393A4K40EB3-CWE
          vendor: Samsung
          physical id: c
          configured speed: 3200MT/s
          size: 32GiB
          width: 64 bits
          clock: 3200MHz (0.3ns)
     *-bank:13
          description: DIMM [empty]
          product: Array 1 Part Number 14
          vendor: Array 1 Manufacturer 14
          physical id: d
          slot: DIMM061
     *-bank:14
          description: DIMM [empty]
          product: Array 1 Part Number 15
          vendor: Array 1 Manufacturer 15
          physical id: e
          slot: DIMM070
     *-bank:15
          description: DIMM [empty]
          product: Array 1 Part Number 16
          vendor: Array 1 Manufacturer 16
          physical id: f
          slot: DIMM071
     *-bank:16
          description: DIMM DDR4 3200 MHz (0.3 ns)
          product: M393A4K40EB3-CWE
          vendor: Samsung
          physical id: 10
          configured speed: 3200MT/s
          size: 32GiB
          width: 64 bits
          clock: 3200MHz (0.3ns)
     *-bank:17
          description: DIMM [empty]
          product: Array 1 Part Number 2
          vendor: Array 1 Manufacturer 2
          physical id: 11
          slot: DIMM101
     *-bank:18
          description: DIMM DDR4 3200 MHz (0.3 ns)
          product: M393A4K40EB3-CWE
          vendor: Samsung
          physical id: 12
          configured speed: 3200MT/s
          size: 32GiB
          width: 64 bits
          clock: 3200MHz (0.3ns)
     *-bank:19
          description: DIMM [empty]
          product: Array 1 Part Number 4
          vendor: Array 1 Manufacturer 4
          physical id: 13
          slot: DIMM111
     *-bank:20
          description: DIMM DDR4 3200 MHz (0.3 ns)
          product: M393A4K40EB3-CWE
          vendor: Samsung
          physical id: 14
          configured speed: 3200MT/s
          size: 32GiB
          width: 64 bits
          clock: 3200MHz (0.3ns)
     *-bank:21
          description: DIMM [empty]
          product: Array 1 Part Number 6
          vendor: Array 1 Manufacturer 6
          physical id: 15
          slot: DIMM121
     *-bank:22
          description: DIMM [empty]
          product: Array 1 Part Number 7
          vendor: Array 1 Manufacturer 7
          physical id: 16
          slot: DIMM130
     *-bank:23
          description: DIMM [empty]
          product: Array 1 Part Number 8
          vendor: Array 1 Manufacturer 8
          physical id: 17
          slot: DIMM131
     *-bank:24
          description: DIMM DDR4 3200 MHz (0.3 ns)
          product: M393A4K40EB3-CWE
          vendor: Samsung
          physical id: 18
          configured speed: 3200MT/s
          size: 32GiB
          width: 64 bits
          clock: 3200MHz (0.3ns)
     *-bank:25
          description: DIMM [empty]
          product: Array 1 Part Number 10
          vendor: Array 1 Manufacturer 10
          physical id: 19
          slot: DIMM141
     *-bank:26
          description: DIMM DDR4 3200 MHz (0.3 ns)
          product: M393A4K40EB3-CWE
          vendor: Samsung
          physical id: 1a
          configured speed: 3200MT/s
          size: 32GiB
          width: 64 bits
          clock: 3200MHz (0.3ns)
     *-bank:27
          description: DIMM [empty]
          product: Array 1 Part Number 12
          vendor: Array 1 Manufacturer 12
          physical id: 1b
          slot: DIMM151
     *-bank:28
          description: DIMM DDR4 3200 MHz (0.3 ns)
          product: M393A4K40EB3-CWE
          vendor: Samsung
          physical id: 1c
          configured speed: 3200MT/s
          size: 32GiB
          width: 64 bits
          clock: 3200MHz (0.3ns)
     *-bank:29
          description: DIMM [empty]
          product: Array 1 Part Number 14
          vendor: Array 1 Manufacturer 14
          physical id: 1d
          slot: DIMM161
     *-bank:30
          description: DIMM [empty]
          product: Array 1 Part Number 15
          vendor: Array 1 Manufacturer 15
          physical id: 1e
          slot: DIMM170
     *-bank:31
          description: DIMM [empty]
          product: Array 1 Part Number 16
          vendor: Array 1 Manufacturer 16
          physical id: 1f
          slot: DIMM171

cpu0/index0: 64K, level: 1, type: Data
cpu0/index1: 64K, level: 1, type: Instruction
cpu0/index2: 1024K, level: 2, type: Unified
cpu1/index0: 64K, level: 1, type: Data
cpu1/index1: 64K, level: 1, type: Instruction
cpu1/index2: 1024K, level: 2, type: Unified
cpu2/index0: 64K, level: 1, type: Data
cpu2/index1: 64K, level: 1, type: Instruction
cpu2/index2: 1024K, level: 2, type: Unified
cpu3/index0: 64K, level: 1, type: Data
cpu3/index1: 64K, level: 1, type: Instruction
cpu3/index2: 1024K, level: 2, type: Unified
cpu4/index0: 64K, level: 1, type: Data
cpu4/index1: 64K, level: 1, type: Instruction
cpu4/index2: 1024K, level: 2, type: Unified
cpu5/index0: 64K, level: 1, type: Data
cpu5/index1: 64K, level: 1, type: Instruction
cpu5/index2: 1024K, level: 2, type: Unified
cpu6/index0: 64K, level: 1, type: Data
cpu6/index1: 64K, level: 1, type: Instruction
cpu6/index2: 1024K, level: 2, type: Unified
cpu7/index0: 64K, level: 1, type: Data
cpu7/index1: 64K, level: 1, type: Instruction
cpu7/index2: 1024K, level: 2, type: Unified
cpu8/index0: 64K, level: 1, type: Data
cpu8/index1: 64K, level: 1, type: Instruction
cpu8/index2: 1024K, level: 2, type: Unified
cpu9/index0: 64K, level: 1, type: Data
cpu9/index1: 64K, level: 1, type: Instruction
cpu9/index2: 1024K, level: 2, type: Unified
cpu10/index0: 64K, level: 1, type: Data
cpu10/index1: 64K, level: 1, type: Instruction
cpu10/index2: 1024K, level: 2, type: Unified
cpu11/index0: 64K, level: 1, type: Data
cpu11/index1: 64K, level: 1, type: Instruction
cpu11/index2: 1024K, level: 2, type: Unified
cpu12/index0: 64K, level: 1, type: Data
cpu12/index1: 64K, level: 1, type: Instruction
cpu12/index2: 1024K, level: 2, type: Unified
cpu13/index0: 64K, level: 1, type: Data
cpu13/index1: 64K, level: 1, type: Instruction
cpu13/index2: 1024K, level: 2, type: Unified
cpu14/index0: 64K, level: 1, type: Data
cpu14/index1: 64K, level: 1, type: Instruction
cpu14/index2: 1024K, level: 2, type: Unified
cpu15/index0: 64K, level: 1, type: Data
cpu15/index1: 64K, level: 1, type: Instruction
cpu15/index2: 1024K, level: 2, type: Unified
cpu16/index0: 64K, level: 1, type: Data
cpu16/index1: 64K, level: 1, type: Instruction
cpu16/index2: 1024K, level: 2, type: Unified
cpu17/index0: 64K, level: 1, type: Data
cpu17/index1: 64K, level: 1, type: Instruction
cpu17/index2: 1024K, level: 2, type: Unified
cpu18/index0: 64K, level: 1, type: Data
cpu18/index1: 64K, level: 1, type: Instruction
cpu18/index2: 1024K, level: 2, type: Unified
cpu19/index0: 64K, level: 1, type: Data
cpu19/index1: 64K, level: 1, type: Instruction
cpu19/index2: 1024K, level: 2, type: Unified
cpu20/index0: 64K, level: 1, type: Data
cpu20/index1: 64K, level: 1, type: Instruction
cpu20/index2: 1024K, level: 2, type: Unified
cpu21/index0: 64K, level: 1, type: Data
cpu21/index1: 64K, level: 1, type: Instruction
cpu21/index2: 1024K, level: 2, type: Unified
cpu22/index0: 64K, level: 1, type: Data
cpu22/index1: 64K, level: 1, type: Instruction
cpu22/index2: 1024K, level: 2, type: Unified
cpu23/index0: 64K, level: 1, type: Data
cpu23/index1: 64K, level: 1, type: Instruction
cpu23/index2: 1024K, level: 2, type: Unified
cpu24/index0: 64K, level: 1, type: Data
cpu24/index1: 64K, level: 1, type: Instruction
cpu24/index2: 1024K, level: 2, type: Unified
cpu25/index0: 64K, level: 1, type: Data
cpu25/index1: 64K, level: 1, type: Instruction
cpu25/index2: 1024K, level: 2, type: Unified
cpu26/index0: 64K, level: 1, type: Data
cpu26/index1: 64K, level: 1, type: Instruction
cpu26/index2: 1024K, level: 2, type: Unified
cpu27/index0: 64K, level: 1, type: Data
cpu27/index1: 64K, level: 1, type: Instruction
cpu27/index2: 1024K, level: 2, type: Unified
cpu28/index0: 64K, level: 1, type: Data
cpu28/index1: 64K, level: 1, type: Instruction
cpu28/index2: 1024K, level: 2, type: Unified
cpu29/index0: 64K, level: 1, type: Data
cpu29/index1: 64K, level: 1, type: Instruction
cpu29/index2: 1024K, level: 2, type: Unified
cpu30/index0: 64K, level: 1, type: Data
cpu30/index1: 64K, level: 1, type: Instruction
cpu30/index2: 1024K, level: 2, type: Unified
cpu31/index0: 64K, level: 1, type: Data
cpu31/index1: 64K, level: 1, type: Instruction
cpu31/index2: 1024K, level: 2, type: Unified
cpu32/index0: 64K, level: 1, type: Data
cpu32/index1: 64K, level: 1, type: Instruction
cpu32/index2: 1024K, level: 2, type: Unified
cpu33/index0: 64K, level: 1, type: Data
cpu33/index1: 64K, level: 1, type: Instruction
cpu33/index2: 1024K, level: 2, type: Unified
cpu34/index0: 64K, level: 1, type: Data
cpu34/index1: 64K, level: 1, type: Instruction
cpu34/index2: 1024K, level: 2, type: Unified
cpu35/index0: 64K, level: 1, type: Data
cpu35/index1: 64K, level: 1, type: Instruction
cpu35/index2: 1024K, level: 2, type: Unified
cpu36/index0: 64K, level: 1, type: Data
cpu36/index1: 64K, level: 1, type: Instruction
cpu36/index2: 1024K, level: 2, type: Unified
cpu37/index0: 64K, level: 1, type: Data
cpu37/index1: 64K, level: 1, type: Instruction
cpu37/index2: 1024K, level: 2, type: Unified
cpu38/index0: 64K, level: 1, type: Data
cpu38/index1: 64K, level: 1, type: Instruction
cpu38/index2: 1024K, level: 2, type: Unified
cpu39/index0: 64K, level: 1, type: Data
cpu39/index1: 64K, level: 1, type: Instruction
cpu39/index2: 1024K, level: 2, type: Unified
cpu40/index0: 64K, level: 1, type: Data
cpu40/index1: 64K, level: 1, type: Instruction
cpu40/index2: 1024K, level: 2, type: Unified
cpu41/index0: 64K, level: 1, type: Data
cpu41/index1: 64K, level: 1, type: Instruction
cpu41/index2: 1024K, level: 2, type: Unified
cpu42/index0: 64K, level: 1, type: Data
cpu42/index1: 64K, level: 1, type: Instruction
cpu42/index2: 1024K, level: 2, type: Unified
cpu43/index0: 64K, level: 1, type: Data
cpu43/index1: 64K, level: 1, type: Instruction
cpu43/index2: 1024K, level: 2, type: Unified
cpu44/index0: 64K, level: 1, type: Data
cpu44/index1: 64K, level: 1, type: Instruction
cpu44/index2: 1024K, level: 2, type: Unified
cpu45/index0: 64K, level: 1, type: Data
cpu45/index1: 64K, level: 1, type: Instruction
cpu45/index2: 1024K, level: 2, type: Unified
cpu46/index0: 64K, level: 1, type: Data
cpu46/index1: 64K, level: 1, type: Instruction
cpu46/index2: 1024K, level: 2, type: Unified
cpu47/index0: 64K, level: 1, type: Data
cpu47/index1: 64K, level: 1, type: Instruction
cpu47/index2: 1024K, level: 2, type: Unified
cpu48/index0: 64K, level: 1, type: Data
cpu48/index1: 64K, level: 1, type: Instruction
cpu48/index2: 1024K, level: 2, type: Unified
cpu49/index0: 64K, level: 1, type: Data
cpu49/index1: 64K, level: 1, type: Instruction
cpu49/index2: 1024K, level: 2, type: Unified
cpu50/index0: 64K, level: 1, type: Data
cpu50/index1: 64K, level: 1, type: Instruction
cpu50/index2: 1024K, level: 2, type: Unified
cpu51/index0: 64K, level: 1, type: Data
cpu51/index1: 64K, level: 1, type: Instruction
cpu51/index2: 1024K, level: 2, type: Unified
cpu52/index0: 64K, level: 1, type: Data
cpu52/index1: 64K, level: 1, type: Instruction
cpu52/index2: 1024K, level: 2, type: Unified
cpu53/index0: 64K, level: 1, type: Data
cpu53/index1: 64K, level: 1, type: Instruction
cpu53/index2: 1024K, level: 2, type: Unified
cpu54/index0: 64K, level: 1, type: Data
cpu54/index1: 64K, level: 1, type: Instruction
cpu54/index2: 1024K, level: 2, type: Unified
cpu55/index0: 64K, level: 1, type: Data
cpu55/index1: 64K, level: 1, type: Instruction
cpu55/index2: 1024K, level: 2, type: Unified
cpu56/index0: 64K, level: 1, type: Data
cpu56/index1: 64K, level: 1, type: Instruction
cpu56/index2: 1024K, level: 2, type: Unified
cpu57/index0: 64K, level: 1, type: Data
cpu57/index1: 64K, level: 1, type: Instruction
cpu57/index2: 1024K, level: 2, type: Unified
cpu58/index0: 64K, level: 1, type: Data
cpu58/index1: 64K, level: 1, type: Instruction
cpu58/index2: 1024K, level: 2, type: Unified
cpu59/index0: 64K, level: 1, type: Data
cpu59/index1: 64K, level: 1, type: Instruction
cpu59/index2: 1024K, level: 2, type: Unified
cpu60/index0: 64K, level: 1, type: Data
cpu60/index1: 64K, level: 1, type: Instruction
cpu60/index2: 1024K, level: 2, type: Unified
cpu61/index0: 64K, level: 1, type: Data
cpu61/index1: 64K, level: 1, type: Instruction
cpu61/index2: 1024K, level: 2, type: Unified
cpu62/index0: 64K, level: 1, type: Data
cpu62/index1: 64K, level: 1, type: Instruction
cpu62/index2: 1024K, level: 2, type: Unified
cpu63/index0: 64K, level: 1, type: Data
cpu63/index1: 64K, level: 1, type: Instruction
cpu63/index2: 1024K, level: 2, type: Unified
cpu64/index0: 64K, level: 1, type: Data
cpu64/index1: 64K, level: 1, type: Instruction
cpu64/index2: 1024K, level: 2, type: Unified
cpu65/index0: 64K, level: 1, type: Data
cpu65/index1: 64K, level: 1, type: Instruction
cpu65/index2: 1024K, level: 2, type: Unified
cpu66/index0: 64K, level: 1, type: Data
cpu66/index1: 64K, level: 1, type: Instruction
cpu66/index2: 1024K, level: 2, type: Unified
cpu67/index0: 64K, level: 1, type: Data
cpu67/index1: 64K, level: 1, type: Instruction
cpu67/index2: 1024K, level: 2, type: Unified
cpu68/index0: 64K, level: 1, type: Data
cpu68/index1: 64K, level: 1, type: Instruction
cpu68/index2: 1024K, level: 2, type: Unified
cpu69/index0: 64K, level: 1, type: Data
cpu69/index1: 64K, level: 1, type: Instruction
cpu69/index2: 1024K, level: 2, type: Unified
cpu70/index0: 64K, level: 1, type: Data
cpu70/index1: 64K, level: 1, type: Instruction
cpu70/index2: 1024K, level: 2, type: Unified
cpu71/index0: 64K, level: 1, type: Data
cpu71/index1: 64K, level: 1, type: Instruction
cpu71/index2: 1024K, level: 2, type: Unified
cpu72/index0: 64K, level: 1, type: Data
cpu72/index1: 64K, level: 1, type: Instruction
cpu72/index2: 1024K, level: 2, type: Unified
cpu73/index0: 64K, level: 1, type: Data
cpu73/index1: 64K, level: 1, type: Instruction
cpu73/index2: 1024K, level: 2, type: Unified
cpu74/index0: 64K, level: 1, type: Data
cpu74/index1: 64K, level: 1, type: Instruction
cpu74/index2: 1024K, level: 2, type: Unified
cpu75/index0: 64K, level: 1, type: Data
cpu75/index1: 64K, level: 1, type: Instruction
cpu75/index2: 1024K, level: 2, type: Unified
cpu76/index0: 64K, level: 1, type: Data
cpu76/index1: 64K, level: 1, type: Instruction
cpu76/index2: 1024K, level: 2, type: Unified
cpu77/index0: 64K, level: 1, type: Data
cpu77/index1: 64K, level: 1, type: Instruction
cpu77/index2: 1024K, level: 2, type: Unified
cpu78/index0: 64K, level: 1, type: Data
cpu78/index1: 64K, level: 1, type: Instruction
cpu78/index2: 1024K, level: 2, type: Unified
cpu79/index0: 64K, level: 1, type: Data
cpu79/index1: 64K, level: 1, type: Instruction
cpu79/index2: 1024K, level: 2, type: Unified
cpu80/index0: 64K, level: 1, type: Data
cpu80/index1: 64K, level: 1, type: Instruction
cpu80/index2: 1024K, level: 2, type: Unified
cpu81/index0: 64K, level: 1, type: Data
cpu81/index1: 64K, level: 1, type: Instruction
cpu81/index2: 1024K, level: 2, type: Unified
cpu82/index0: 64K, level: 1, type: Data
cpu82/index1: 64K, level: 1, type: Instruction
cpu82/index2: 1024K, level: 2, type: Unified
cpu83/index0: 64K, level: 1, type: Data
cpu83/index1: 64K, level: 1, type: Instruction
cpu83/index2: 1024K, level: 2, type: Unified
cpu84/index0: 64K, level: 1, type: Data
cpu84/index1: 64K, level: 1, type: Instruction
cpu84/index2: 1024K, level: 2, type: Unified
cpu85/index0: 64K, level: 1, type: Data
cpu85/index1: 64K, level: 1, type: Instruction
cpu85/index2: 1024K, level: 2, type: Unified
cpu86/index0: 64K, level: 1, type: Data
cpu86/index1: 64K, level: 1, type: Instruction
cpu86/index2: 1024K, level: 2, type: Unified
cpu87/index0: 64K, level: 1, type: Data
cpu87/index1: 64K, level: 1, type: Instruction
cpu87/index2: 1024K, level: 2, type: Unified
cpu88/index0: 64K, level: 1, type: Data
cpu88/index1: 64K, level: 1, type: Instruction
cpu88/index2: 1024K, level: 2, type: Unified
cpu89/index0: 64K, level: 1, type: Data
cpu89/index1: 64K, level: 1, type: Instruction
cpu89/index2: 1024K, level: 2, type: Unified
cpu90/index0: 64K, level: 1, type: Data
cpu90/index1: 64K, level: 1, type: Instruction
cpu90/index2: 1024K, level: 2, type: Unified
cpu91/index0: 64K, level: 1, type: Data
cpu91/index1: 64K, level: 1, type: Instruction
cpu91/index2: 1024K, level: 2, type: Unified
cpu92/index0: 64K, level: 1, type: Data
cpu92/index1: 64K, level: 1, type: Instruction
cpu92/index2: 1024K, level: 2, type: Unified
cpu93/index0: 64K, level: 1, type: Data
cpu93/index1: 64K, level: 1, type: Instruction
cpu93/index2: 1024K, level: 2, type: Unified
cpu94/index0: 64K, level: 1, type: Data
cpu94/index1: 64K, level: 1, type: Instruction
cpu94/index2: 1024K, level: 2, type: Unified
cpu95/index0: 64K, level: 1, type: Data
cpu95/index1: 64K, level: 1, type: Instruction
cpu95/index2: 1024K, level: 2, type: Unified
cpu96/index0: 64K, level: 1, type: Data
cpu96/index1: 64K, level: 1, type: Instruction
cpu96/index2: 1024K, level: 2, type: Unified
cpu97/index0: 64K, level: 1, type: Data
cpu97/index1: 64K, level: 1, type: Instruction
cpu97/index2: 1024K, level: 2, type: Unified
cpu98/index0: 64K, level: 1, type: Data
cpu98/index1: 64K, level: 1, type: Instruction
cpu98/index2: 1024K, level: 2, type: Unified
cpu99/index0: 64K, level: 1, type: Data
cpu99/index1: 64K, level: 1, type: Instruction
cpu99/index2: 1024K, level: 2, type: Unified
cpu100/index0: 64K, level: 1, type: Data
cpu100/index1: 64K, level: 1, type: Instruction
cpu100/index2: 1024K, level: 2, type: Unified
cpu101/index0: 64K, level: 1, type: Data
cpu101/index1: 64K, level: 1, type: Instruction
cpu101/index2: 1024K, level: 2, type: Unified
cpu102/index0: 64K, level: 1, type: Data
cpu102/index1: 64K, level: 1, type: Instruction
cpu102/index2: 1024K, level: 2, type: Unified
cpu103/index0: 64K, level: 1, type: Data
cpu103/index1: 64K, level: 1, type: Instruction
cpu103/index2: 1024K, level: 2, type: Unified
cpu104/index0: 64K, level: 1, type: Data
cpu104/index1: 64K, level: 1, type: Instruction
cpu104/index2: 1024K, level: 2, type: Unified
cpu105/index0: 64K, level: 1, type: Data
cpu105/index1: 64K, level: 1, type: Instruction
cpu105/index2: 1024K, level: 2, type: Unified
cpu106/index0: 64K, level: 1, type: Data
cpu106/index1: 64K, level: 1, type: Instruction
cpu106/index2: 1024K, level: 2, type: Unified
cpu107/index0: 64K, level: 1, type: Data
cpu107/index1: 64K, level: 1, type: Instruction
cpu107/index2: 1024K, level: 2, type: Unified
cpu108/index0: 64K, level: 1, type: Data
cpu108/index1: 64K, level: 1, type: Instruction
cpu108/index2: 1024K, level: 2, type: Unified
cpu109/index0: 64K, level: 1, type: Data
cpu109/index1: 64K, level: 1, type: Instruction
cpu109/index2: 1024K, level: 2, type: Unified
cpu110/index0: 64K, level: 1, type: Data
cpu110/index1: 64K, level: 1, type: Instruction
cpu110/index2: 1024K, level: 2, type: Unified
cpu111/index0: 64K, level: 1, type: Data
cpu111/index1: 64K, level: 1, type: Instruction
cpu111/index2: 1024K, level: 2, type: Unified
cpu112/index0: 64K, level: 1, type: Data
cpu112/index1: 64K, level: 1, type: Instruction
cpu112/index2: 1024K, level: 2, type: Unified
cpu113/index0: 64K, level: 1, type: Data
cpu113/index1: 64K, level: 1, type: Instruction
cpu113/index2: 1024K, level: 2, type: Unified
cpu114/index0: 64K, level: 1, type: Data
cpu114/index1: 64K, level: 1, type: Instruction
cpu114/index2: 1024K, level: 2, type: Unified
cpu115/index0: 64K, level: 1, type: Data
cpu115/index1: 64K, level: 1, type: Instruction
cpu115/index2: 1024K, level: 2, type: Unified
cpu116/index0: 64K, level: 1, type: Data
cpu116/index1: 64K, level: 1, type: Instruction
cpu116/index2: 1024K, level: 2, type: Unified
cpu117/index0: 64K, level: 1, type: Data
cpu117/index1: 64K, level: 1, type: Instruction
cpu117/index2: 1024K, level: 2, type: Unified
cpu118/index0: 64K, level: 1, type: Data
cpu118/index1: 64K, level: 1, type: Instruction
cpu118/index2: 1024K, level: 2, type: Unified
cpu119/index0: 64K, level: 1, type: Data
cpu119/index1: 64K, level: 1, type: Instruction
cpu119/index2: 1024K, level: 2, type: Unified
cpu120/index0: 64K, level: 1, type: Data
cpu120/index1: 64K, level: 1, type: Instruction
cpu120/index2: 1024K, level: 2, type: Unified
cpu121/index0: 64K, level: 1, type: Data
cpu121/index1: 64K, level: 1, type: Instruction
cpu121/index2: 1024K, level: 2, type: Unified
cpu122/index0: 64K, level: 1, type: Data
cpu122/index1: 64K, level: 1, type: Instruction
cpu122/index2: 1024K, level: 2, type: Unified
cpu123/index0: 64K, level: 1, type: Data
cpu123/index1: 64K, level: 1, type: Instruction
cpu123/index2: 1024K, level: 2, type: Unified
cpu124/index0: 64K, level: 1, type: Data
cpu124/index1: 64K, level: 1, type: Instruction
cpu124/index2: 1024K, level: 2, type: Unified
cpu125/index0: 64K, level: 1, type: Data
cpu125/index1: 64K, level: 1, type: Instruction
cpu125/index2: 1024K, level: 2, type: Unified
cpu126/index0: 64K, level: 1, type: Data
cpu126/index1: 64K, level: 1, type: Instruction
cpu126/index2: 1024K, level: 2, type: Unified
cpu127/index0: 64K, level: 1, type: Data
cpu127/index1: 64K, level: 1, type: Instruction
cpu127/index2: 1024K, level: 2, type: Unified
cpu128/index0: 64K, level: 1, type: Data
cpu128/index1: 64K, level: 1, type: Instruction
cpu128/index2: 1024K, level: 2, type: Unified
cpu129/index0: 64K, level: 1, type: Data
cpu129/index1: 64K, level: 1, type: Instruction
cpu129/index2: 1024K, level: 2, type: Unified
cpu130/index0: 64K, level: 1, type: Data
cpu130/index1: 64K, level: 1, type: Instruction
cpu130/index2: 1024K, level: 2, type: Unified
cpu131/index0: 64K, level: 1, type: Data
cpu131/index1: 64K, level: 1, type: Instruction
cpu131/index2: 1024K, level: 2, type: Unified
cpu132/index0: 64K, level: 1, type: Data
cpu132/index1: 64K, level: 1, type: Instruction
cpu132/index2: 1024K, level: 2, type: Unified
cpu133/index0: 64K, level: 1, type: Data
cpu133/index1: 64K, level: 1, type: Instruction
cpu133/index2: 1024K, level: 2, type: Unified
cpu134/index0: 64K, level: 1, type: Data
cpu134/index1: 64K, level: 1, type: Instruction
cpu134/index2: 1024K, level: 2, type: Unified
cpu135/index0: 64K, level: 1, type: Data
cpu135/index1: 64K, level: 1, type: Instruction
cpu135/index2: 1024K, level: 2, type: Unified
cpu136/index0: 64K, level: 1, type: Data
cpu136/index1: 64K, level: 1, type: Instruction
cpu136/index2: 1024K, level: 2, type: Unified
cpu137/index0: 64K, level: 1, type: Data
cpu137/index1: 64K, level: 1, type: Instruction
cpu137/index2: 1024K, level: 2, type: Unified
cpu138/index0: 64K, level: 1, type: Data
cpu138/index1: 64K, level: 1, type: Instruction
cpu138/index2: 1024K, level: 2, type: Unified
cpu139/index0: 64K, level: 1, type: Data
cpu139/index1: 64K, level: 1, type: Instruction
cpu139/index2: 1024K, level: 2, type: Unified
cpu140/index0: 64K, level: 1, type: Data
cpu140/index1: 64K, level: 1, type: Instruction
cpu140/index2: 1024K, level: 2, type: Unified
cpu141/index0: 64K, level: 1, type: Data
cpu141/index1: 64K, level: 1, type: Instruction
cpu141/index2: 1024K, level: 2, type: Unified
cpu142/index0: 64K, level: 1, type: Data
cpu142/index1: 64K, level: 1, type: Instruction
cpu142/index2: 1024K, level: 2, type: Unified
cpu143/index0: 64K, level: 1, type: Data
cpu143/index1: 64K, level: 1, type: Instruction
cpu143/index2: 1024K, level: 2, type: Unified
cpu144/index0: 64K, level: 1, type: Data
cpu144/index1: 64K, level: 1, type: Instruction
cpu144/index2: 1024K, level: 2, type: Unified
cpu145/index0: 64K, level: 1, type: Data
cpu145/index1: 64K, level: 1, type: Instruction
cpu145/index2: 1024K, level: 2, type: Unified
cpu146/index0: 64K, level: 1, type: Data
cpu146/index1: 64K, level: 1, type: Instruction
cpu146/index2: 1024K, level: 2, type: Unified
cpu147/index0: 64K, level: 1, type: Data
cpu147/index1: 64K, level: 1, type: Instruction
cpu147/index2: 1024K, level: 2, type: Unified
cpu148/index0: 64K, level: 1, type: Data
cpu148/index1: 64K, level: 1, type: Instruction
cpu148/index2: 1024K, level: 2, type: Unified
cpu149/index0: 64K, level: 1, type: Data
cpu149/index1: 64K, level: 1, type: Instruction
cpu149/index2: 1024K, level: 2, type: Unified
cpu150/index0: 64K, level: 1, type: Data
cpu150/index1: 64K, level: 1, type: Instruction
cpu150/index2: 1024K, level: 2, type: Unified
cpu151/index0: 64K, level: 1, type: Data
cpu151/index1: 64K, level: 1, type: Instruction
cpu151/index2: 1024K, level: 2, type: Unified
cpu152/index0: 64K, level: 1, type: Data
cpu152/index1: 64K, level: 1, type: Instruction
cpu152/index2: 1024K, level: 2, type: Unified
cpu153/index0: 64K, level: 1, type: Data
cpu153/index1: 64K, level: 1, type: Instruction
cpu153/index2: 1024K, level: 2, type: Unified
cpu154/index0: 64K, level: 1, type: Data
cpu154/index1: 64K, level: 1, type: Instruction
cpu154/index2: 1024K, level: 2, type: Unified
cpu155/index0: 64K, level: 1, type: Data
cpu155/index1: 64K, level: 1, type: Instruction
cpu155/index2: 1024K, level: 2, type: Unified
cpu156/index0: 64K, level: 1, type: Data
cpu156/index1: 64K, level: 1, type: Instruction
cpu156/index2: 1024K, level: 2, type: Unified
cpu157/index0: 64K, level: 1, type: Data
cpu157/index1: 64K, level: 1, type: Instruction
cpu157/index2: 1024K, level: 2, type: Unified
cpu158/index0: 64K, level: 1, type: Data
cpu158/index1: 64K, level: 1, type: Instruction
cpu158/index2: 1024K, level: 2, type: Unified
cpu159/index0: 64K, level: 1, type: Data
cpu159/index1: 64K, level: 1, type: Instruction
cpu159/index2: 1024K, level: 2, type: Unified
cpu160/index0: 64K, level: 1, type: Data
cpu160/index1: 64K, level: 1, type: Instruction
cpu160/index2: 1024K, level: 2, type: Unified
cpu161/index0: 64K, level: 1, type: Data
cpu161/index1: 64K, level: 1, type: Instruction
cpu161/index2: 1024K, level: 2, type: Unified
cpu162/index0: 64K, level: 1, type: Data
cpu162/index1: 64K, level: 1, type: Instruction
cpu162/index2: 1024K, level: 2, type: Unified
cpu163/index0: 64K, level: 1, type: Data
cpu163/index1: 64K, level: 1, type: Instruction
cpu163/index2: 1024K, level: 2, type: Unified
cpu164/index0: 64K, level: 1, type: Data
cpu164/index1: 64K, level: 1, type: Instruction
cpu164/index2: 1024K, level: 2, type: Unified
cpu165/index0: 64K, level: 1, type: Data
cpu165/index1: 64K, level: 1, type: Instruction
cpu165/index2: 1024K, level: 2, type: Unified
cpu166/index0: 64K, level: 1, type: Data
cpu166/index1: 64K, level: 1, type: Instruction
cpu166/index2: 1024K, level: 2, type: Unified
cpu167/index0: 64K, level: 1, type: Data
cpu167/index1: 64K, level: 1, type: Instruction
cpu167/index2: 1024K, level: 2, type: Unified
cpu168/index0: 64K, level: 1, type: Data
cpu168/index1: 64K, level: 1, type: Instruction
cpu168/index2: 1024K, level: 2, type: Unified
cpu169/index0: 64K, level: 1, type: Data
cpu169/index1: 64K, level: 1, type: Instruction
cpu169/index2: 1024K, level: 2, type: Unified
cpu170/index0: 64K, level: 1, type: Data
cpu170/index1: 64K, level: 1, type: Instruction
cpu170/index2: 1024K, level: 2, type: Unified
cpu171/index0: 64K, level: 1, type: Data
cpu171/index1: 64K, level: 1, type: Instruction
cpu171/index2: 1024K, level: 2, type: Unified
cpu172/index0: 64K, level: 1, type: Data
cpu172/index1: 64K, level: 1, type: Instruction
cpu172/index2: 1024K, level: 2, type: Unified
cpu173/index0: 64K, level: 1, type: Data
cpu173/index1: 64K, level: 1, type: Instruction
cpu173/index2: 1024K, level: 2, type: Unified
cpu174/index0: 64K, level: 1, type: Data
cpu174/index1: 64K, level: 1, type: Instruction
cpu174/index2: 1024K, level: 2, type: Unified
cpu175/index0: 64K, level: 1, type: Data
cpu175/index1: 64K, level: 1, type: Instruction
cpu175/index2: 1024K, level: 2, type: Unified
cpu176/index0: 64K, level: 1, type: Data
cpu176/index1: 64K, level: 1, type: Instruction
cpu176/index2: 1024K, level: 2, type: Unified
cpu177/index0: 64K, level: 1, type: Data
cpu177/index1: 64K, level: 1, type: Instruction
cpu177/index2: 1024K, level: 2, type: Unified
cpu178/index0: 64K, level: 1, type: Data
cpu178/index1: 64K, level: 1, type: Instruction
cpu178/index2: 1024K, level: 2, type: Unified
cpu179/index0: 64K, level: 1, type: Data
cpu179/index1: 64K, level: 1, type: Instruction
cpu179/index2: 1024K, level: 2, type: Unified
cpu180/index0: 64K, level: 1, type: Data
cpu180/index1: 64K, level: 1, type: Instruction
cpu180/index2: 1024K, level: 2, type: Unified
cpu181/index0: 64K, level: 1, type: Data
cpu181/index1: 64K, level: 1, type: Instruction
cpu181/index2: 1024K, level: 2, type: Unified
cpu182/index0: 64K, level: 1, type: Data
cpu182/index1: 64K, level: 1, type: Instruction
cpu182/index2: 1024K, level: 2, type: Unified
cpu183/index0: 64K, level: 1, type: Data
cpu183/index1: 64K, level: 1, type: Instruction
cpu183/index2: 1024K, level: 2, type: Unified
cpu184/index0: 64K, level: 1, type: Data
cpu184/index1: 64K, level: 1, type: Instruction
cpu184/index2: 1024K, level: 2, type: Unified
cpu185/index0: 64K, level: 1, type: Data
cpu185/index1: 64K, level: 1, type: Instruction
cpu185/index2: 1024K, level: 2, type: Unified
cpu186/index0: 64K, level: 1, type: Data
cpu186/index1: 64K, level: 1, type: Instruction
cpu186/index2: 1024K, level: 2, type: Unified
cpu187/index0: 64K, level: 1, type: Data
cpu187/index1: 64K, level: 1, type: Instruction
cpu187/index2: 1024K, level: 2, type: Unified
cpu188/index0: 64K, level: 1, type: Data
cpu188/index1: 64K, level: 1, type: Instruction
cpu188/index2: 1024K, level: 2, type: Unified
cpu189/index0: 64K, level: 1, type: Data
cpu189/index1: 64K, level: 1, type: Instruction
cpu189/index2: 1024K, level: 2, type: Unified
cpu190/index0: 64K, level: 1, type: Data
cpu190/index1: 64K, level: 1, type: Instruction
cpu190/index2: 1024K, level: 2, type: Unified
cpu191/index0: 64K, level: 1, type: Data
cpu191/index1: 64K, level: 1, type: Instruction
cpu191/index2: 1024K, level: 2, type: Unified
cpu192/index0: 64K, level: 1, type: Data
cpu192/index1: 64K, level: 1, type: Instruction
cpu192/index2: 1024K, level: 2, type: Unified
cpu193/index0: 64K, level: 1, type: Data
cpu193/index1: 64K, level: 1, type: Instruction
cpu193/index2: 1024K, level: 2, type: Unified
cpu194/index0: 64K, level: 1, type: Data
cpu194/index1: 64K, level: 1, type: Instruction
cpu194/index2: 1024K, level: 2, type: Unified
cpu195/index0: 64K, level: 1, type: Data
cpu195/index1: 64K, level: 1, type: Instruction
cpu195/index2: 1024K, level: 2, type: Unified
cpu196/index0: 64K, level: 1, type: Data
cpu196/index1: 64K, level: 1, type: Instruction
cpu196/index2: 1024K, level: 2, type: Unified
cpu197/index0: 64K, level: 1, type: Data
cpu197/index1: 64K, level: 1, type: Instruction
cpu197/index2: 1024K, level: 2, type: Unified
cpu198/index0: 64K, level: 1, type: Data
cpu198/index1: 64K, level: 1, type: Instruction
cpu198/index2: 1024K, level: 2, type: Unified
cpu199/index0: 64K, level: 1, type: Data
cpu199/index1: 64K, level: 1, type: Instruction
cpu199/index2: 1024K, level: 2, type: Unified
cpu200/index0: 64K, level: 1, type: Data
cpu200/index1: 64K, level: 1, type: Instruction
cpu200/index2: 1024K, level: 2, type: Unified
cpu201/index0: 64K, level: 1, type: Data
cpu201/index1: 64K, level: 1, type: Instruction
cpu201/index2: 1024K, level: 2, type: Unified
cpu202/index0: 64K, level: 1, type: Data
cpu202/index1: 64K, level: 1, type: Instruction
cpu202/index2: 1024K, level: 2, type: Unified
cpu203/index0: 64K, level: 1, type: Data
cpu203/index1: 64K, level: 1, type: Instruction
cpu203/index2: 1024K, level: 2, type: Unified
cpu204/index0: 64K, level: 1, type: Data
cpu204/index1: 64K, level: 1, type: Instruction
cpu204/index2: 1024K, level: 2, type: Unified
cpu205/index0: 64K, level: 1, type: Data
cpu205/index1: 64K, level: 1, type: Instruction
cpu205/index2: 1024K, level: 2, type: Unified
cpu206/index0: 64K, level: 1, type: Data
cpu206/index1: 64K, level: 1, type: Instruction
cpu206/index2: 1024K, level: 2, type: Unified
cpu207/index0: 64K, level: 1, type: Data
cpu207/index1: 64K, level: 1, type: Instruction
cpu207/index2: 1024K, level: 2, type: Unified
cpu208/index0: 64K, level: 1, type: Data
cpu208/index1: 64K, level: 1, type: Instruction
cpu208/index2: 1024K, level: 2, type: Unified
cpu209/index0: 64K, level: 1, type: Data
cpu209/index1: 64K, level: 1, type: Instruction
cpu209/index2: 1024K, level: 2, type: Unified
cpu210/index0: 64K, level: 1, type: Data
cpu210/index1: 64K, level: 1, type: Instruction
cpu210/index2: 1024K, level: 2, type: Unified
cpu211/index0: 64K, level: 1, type: Data
cpu211/index1: 64K, level: 1, type: Instruction
cpu211/index2: 1024K, level: 2, type: Unified
cpu212/index0: 64K, level: 1, type: Data
cpu212/index1: 64K, level: 1, type: Instruction
cpu212/index2: 1024K, level: 2, type: Unified
cpu213/index0: 64K, level: 1, type: Data
cpu213/index1: 64K, level: 1, type: Instruction
cpu213/index2: 1024K, level: 2, type: Unified
cpu214/index0: 64K, level: 1, type: Data
cpu214/index1: 64K, level: 1, type: Instruction
cpu214/index2: 1024K, level: 2, type: Unified
cpu215/index0: 64K, level: 1, type: Data
cpu215/index1: 64K, level: 1, type: Instruction
cpu215/index2: 1024K, level: 2, type: Unified
cpu216/index0: 64K, level: 1, type: Data
cpu216/index1: 64K, level: 1, type: Instruction
cpu216/index2: 1024K, level: 2, type: Unified
cpu217/index0: 64K, level: 1, type: Data
cpu217/index1: 64K, level: 1, type: Instruction
cpu217/index2: 1024K, level: 2, type: Unified
cpu218/index0: 64K, level: 1, type: Data
cpu218/index1: 64K, level: 1, type: Instruction
cpu218/index2: 1024K, level: 2, type: Unified
cpu219/index0: 64K, level: 1, type: Data
cpu219/index1: 64K, level: 1, type: Instruction
cpu219/index2: 1024K, level: 2, type: Unified
cpu220/index0: 64K, level: 1, type: Data
cpu220/index1: 64K, level: 1, type: Instruction
cpu220/index2: 1024K, level: 2, type: Unified
cpu221/index0: 64K, level: 1, type: Data
cpu221/index1: 64K, level: 1, type: Instruction
cpu221/index2: 1024K, level: 2, type: Unified
cpu222/index0: 64K, level: 1, type: Data
cpu222/index1: 64K, level: 1, type: Instruction
cpu222/index2: 1024K, level: 2, type: Unified
cpu223/index0: 64K, level: 1, type: Data
cpu223/index1: 64K, level: 1, type: Instruction
cpu223/index2: 1024K, level: 2, type: Unified
cpu224/index0: 64K, level: 1, type: Data
cpu224/index1: 64K, level: 1, type: Instruction
cpu224/index2: 1024K, level: 2, type: Unified
cpu225/index0: 64K, level: 1, type: Data
cpu225/index1: 64K, level: 1, type: Instruction
cpu225/index2: 1024K, level: 2, type: Unified
cpu226/index0: 64K, level: 1, type: Data
cpu226/index1: 64K, level: 1, type: Instruction
cpu226/index2: 1024K, level: 2, type: Unified
cpu227/index0: 64K, level: 1, type: Data
cpu227/index1: 64K, level: 1, type: Instruction
cpu227/index2: 1024K, level: 2, type: Unified
cpu228/index0: 64K, level: 1, type: Data
cpu228/index1: 64K, level: 1, type: Instruction
cpu228/index2: 1024K, level: 2, type: Unified
cpu229/index0: 64K, level: 1, type: Data
cpu229/index1: 64K, level: 1, type: Instruction
cpu229/index2: 1024K, level: 2, type: Unified
cpu230/index0: 64K, level: 1, type: Data
cpu230/index1: 64K, level: 1, type: Instruction
cpu230/index2: 1024K, level: 2, type: Unified
cpu231/index0: 64K, level: 1, type: Data
cpu231/index1: 64K, level: 1, type: Instruction
cpu231/index2: 1024K, level: 2, type: Unified
cpu232/index0: 64K, level: 1, type: Data
cpu232/index1: 64K, level: 1, type: Instruction
cpu232/index2: 1024K, level: 2, type: Unified
cpu233/index0: 64K, level: 1, type: Data
cpu233/index1: 64K, level: 1, type: Instruction
cpu233/index2: 1024K, level: 2, type: Unified
cpu234/index0: 64K, level: 1, type: Data
cpu234/index1: 64K, level: 1, type: Instruction
cpu234/index2: 1024K, level: 2, type: Unified
cpu235/index0: 64K, level: 1, type: Data
cpu235/index1: 64K, level: 1, type: Instruction
cpu235/index2: 1024K, level: 2, type: Unified
cpu236/index0: 64K, level: 1, type: Data
cpu236/index1: 64K, level: 1, type: Instruction
cpu236/index2: 1024K, level: 2, type: Unified
cpu237/index0: 64K, level: 1, type: Data
cpu237/index1: 64K, level: 1, type: Instruction
cpu237/index2: 1024K, level: 2, type: Unified
cpu238/index0: 64K, level: 1, type: Data
cpu238/index1: 64K, level: 1, type: Instruction
cpu238/index2: 1024K, level: 2, type: Unified
cpu239/index0: 64K, level: 1, type: Data
cpu239/index1: 64K, level: 1, type: Instruction
cpu239/index2: 1024K, level: 2, type: Unified
cpu240/index0: 64K, level: 1, type: Data
cpu240/index1: 64K, level: 1, type: Instruction
cpu240/index2: 1024K, level: 2, type: Unified
cpu241/index0: 64K, level: 1, type: Data
cpu241/index1: 64K, level: 1, type: Instruction
cpu241/index2: 1024K, level: 2, type: Unified
cpu242/index0: 64K, level: 1, type: Data
cpu242/index1: 64K, level: 1, type: Instruction
cpu242/index2: 1024K, level: 2, type: Unified
cpu243/index0: 64K, level: 1, type: Data
cpu243/index1: 64K, level: 1, type: Instruction
cpu243/index2: 1024K, level: 2, type: Unified
cpu244/index0: 64K, level: 1, type: Data
cpu244/index1: 64K, level: 1, type: Instruction
cpu244/index2: 1024K, level: 2, type: Unified
cpu245/index0: 64K, level: 1, type: Data
cpu245/index1: 64K, level: 1, type: Instruction
cpu245/index2: 1024K, level: 2, type: Unified
cpu246/index0: 64K, level: 1, type: Data
cpu246/index1: 64K, level: 1, type: Instruction
cpu246/index2: 1024K, level: 2, type: Unified
cpu247/index0: 64K, level: 1, type: Data
cpu247/index1: 64K, level: 1, type: Instruction
cpu247/index2: 1024K, level: 2, type: Unified
cpu248/index0: 64K, level: 1, type: Data
cpu248/index1: 64K, level: 1, type: Instruction
cpu248/index2: 1024K, level: 2, type: Unified
cpu249/index0: 64K, level: 1, type: Data
cpu249/index1: 64K, level: 1, type: Instruction
cpu249/index2: 1024K, level: 2, type: Unified
cpu250/index0: 64K, level: 1, type: Data
cpu250/index1: 64K, level: 1, type: Instruction
cpu250/index2: 1024K, level: 2, type: Unified
cpu251/index0: 64K, level: 1, type: Data
cpu251/index1: 64K, level: 1, type: Instruction
cpu251/index2: 1024K, level: 2, type: Unified
cpu252/index0: 64K, level: 1, type: Data
cpu252/index1: 64K, level: 1, type: Instruction
cpu252/index2: 1024K, level: 2, type: Unified
cpu253/index0: 64K, level: 1, type: Data
cpu253/index1: 64K, level: 1, type: Instruction
cpu253/index2: 1024K, level: 2, type: Unified
cpu254/index0: 64K, level: 1, type: Data
cpu254/index1: 64K, level: 1, type: Instruction
cpu254/index2: 1024K, level: 2, type: Unified
cpu255/index0: 64K, level: 1, type: Data
cpu255/index1: 64K, level: 1, type: Instruction
cpu255/index2: 1024K, level: 2, type: Unified

| HUAQIN P6410 HQ3110BR49000 | 3000 MHz | 5.4 | Ubuntu 20.04.5 LTS arm64 | 430860 | 4211 | 1710010 | 13310 | 47970 | - |