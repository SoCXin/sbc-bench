sbc-bench v0.9.39 Hardkernel ODROID-N2Plus (Mon, 27 Mar 2023 08:49:31 +0200)

Distributor ID:	Debian
Description:	Debian GNU/Linux 11 (bullseye)
Release:	11
Codename:	bullseye
Build system:   https://github.com/armbian/build, 23.02.2, Odroid N2, meson-g12b, meson64

/usr/bin/gcc (Debian 10.2.1-6) 10.2.1 20210110

Uptime: 08:49:31 up  1:00,  1 user,  load average: 0.31, 0.36, 0.19,  30.8Â°C,  96143579

Linux 6.1.11-meson64 (odroidn2) 	03/27/23 	_aarch64_	(6 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.76    0.00    0.29    0.03    0.00   98.92

Device             tps    kB_read/s    kB_wrtn/s    kB_dscd/s    kB_read    kB_wrtn    kB_dscd
mmcblk1           5.95       140.91       148.26         0.00     512880     539641          0
zram0             0.16         0.65         0.00         0.00       2384          4          0
zram1             0.13         0.13         0.64         0.00        484       2324          0

               total        used        free      shared  buff/cache   available
Mem:           3.7Gi       183Mi       3.4Gi       5.0Mi       126Mi       3.4Gi
Swap:          1.8Gi          0B       1.8Gi

Filename				Type		Size	Used	Priority
/dev/zram0                             	partition	1938044	0	5

##########################################################################

Checking cpufreq OPP for cpu0-cpu1 (Cortex-A53):

Cpufreq OPP: 2016    Measured: 2013 (2013.409/2013.360/2013.164)
Cpufreq OPP: 1908    Measured: 1905 (1905.828/1905.388/1905.344)
Cpufreq OPP: 1800    Measured: 1797 (1797.629/1797.629/1797.629)
Cpufreq OPP: 1704    Measured: 1701 (1701.686/1701.651/1701.546)
Cpufreq OPP: 1608    Measured: 1605 (1605.800/1605.761/1605.644)
Cpufreq OPP: 1512    Measured: 1497 (1497.859/1497.486/1497.452)
Cpufreq OPP: 1398    Measured: 1395 (1395.900/1395.782/1395.723)
Cpufreq OPP: 1200    Measured: 1197 (1197.826/1197.772/1197.772)
Cpufreq OPP: 1000    Measured:  997    (997.878/997.760/997.689)

Checking cpufreq OPP for cpu2-cpu5 (Cortex-A73):

Cpufreq OPP: 2400    Measured: 2398 (2398.769/2398.714/2398.714)
Cpufreq OPP: 2304    Measured: 2302 (2302.735/2302.684/2302.684)
Cpufreq OPP: 2208    Measured: 2206 (2206.858/2206.858/2206.763)
Cpufreq OPP: 2108    Measured: 2098 (2098.844/2098.801/2098.759)
Cpufreq OPP: 2016    Measured: 2014 (2014.735/2014.735/2014.735)
Cpufreq OPP: 1908    Measured: 1906 (1906.883/1906.839/1906.839)
Cpufreq OPP: 1800    Measured: 1798 (1798.920/1798.803/1798.764)
Cpufreq OPP: 1704    Measured: 1702 (1702.879/1702.843/1702.808)
Cpufreq OPP: 1608    Measured: 1606 (1606.854/1606.815/1606.776)
Cpufreq OPP: 1512    Measured: 1498 (1498.912/1498.810/1498.810)
Cpufreq OPP: 1398    Measured: 1396 (1396.962/1396.962/1396.873)
Cpufreq OPP: 1200    Measured: 1198 (1198.966/1198.939/1198.939)
Cpufreq OPP: 1000    Measured:  998    (998.938/998.914/998.890)

##########################################################################

Hardware sensors:

gpio_fan-isa-0000
fan1:           0 RPM  (min =    0 RPM, max = 1600 RPM)

cpu_thermal-virtual-0
temp1:        +28.4 C  (crit = +110.0 C)

ddr_thermal-virtual-0
temp1:        +30.7 C  (crit = +110.0 C)

##########################################################################

Executing benchmark on cpu0 (Cortex-A53):

tinymembench v0.4.9-nuumio (simple benchmark for memory throughput and latency)

CFLAGS: 
bandwidth test min repeats (-b): 2
bandwidth test max repeats (-B): 3
bandwidth test mem realloc (-M): no      (-m for realloc)
      latency test repeats (-l): 3
        latency test count (-c): 1000000

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Test result is the best of repeated runs. Number of repeats  ==
==         is shown in brackets                                         ==
== Note 3: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 4: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 5: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                 :   2051.9 MB/s (3, 2.7%)
 C copy backwards (32 byte blocks)                :   2109.6 MB/s (3, 1.6%)
 C copy backwards (64 byte blocks)                :   2061.9 MB/s (3, 1.4%)
 C copy                                           :   2189.3 MB/s (3, 0.5%)
 C copy prefetched (32 bytes step)                :   1623.7 MB/s (3, 0.2%)
 C copy prefetched (64 bytes step)                :   1919.7 MB/s (3, 0.2%)
 C 2-pass copy                                    :   1888.4 MB/s (2)
 C 2-pass copy prefetched (32 bytes step)         :   1355.7 MB/s (2)
 C 2-pass copy prefetched (64 bytes step)         :   1201.6 MB/s (2)
 C scan 8                                         :    390.8 MB/s (3)
 C scan 16                                        :    773.1 MB/s (3, 0.3%)
 C scan 32                                        :   1426.5 MB/s (2)
 C scan 64                                        :   2539.6 MB/s (3, 0.7%)
 C fill                                           :   7714.9 MB/s (2)
 C fill (shuffle within 16 byte blocks)           :   7715.6 MB/s (2)
 C fill (shuffle within 32 byte blocks)           :   7716.4 MB/s (2)
 C fill (shuffle within 64 byte blocks)           :   7716.2 MB/s (2)
 ---
 libc memcpy copy                                 :   2213.1 MB/s (3, 0.5%)
 libc memchr scan                                 :   2567.7 MB/s (3, 1.8%)
 libc memset fill                                 :   7721.9 MB/s (2)
 ---
 NEON LDP/STP copy                                :   2246.1 MB/s (2)
 NEON LDP/STP copy pldl2strm (32 bytes step)      :   1410.6 MB/s (3, 0.8%)
 NEON LDP/STP copy pldl2strm (64 bytes step)      :   1860.5 MB/s (3, 0.1%)
 NEON LDP/STP copy pldl1keep (32 bytes step)      :   2468.2 MB/s (2)
 NEON LDP/STP copy pldl1keep (64 bytes step)      :   2464.5 MB/s (2)
 NEON LD1/ST1 copy                                :   2205.0 MB/s (3, 0.7%)
 NEON LDP load                                    :   3504.0 MB/s (2)
 NEON LDNP load                                   :   2553.0 MB/s (3, 0.8%)
 NEON STP fill                                    :   7722.6 MB/s (2)
 NEON STNP fill                                   :   6327.2 MB/s (2)
 ARM LDP/STP copy                                 :   2241.7 MB/s (2)
 ARM LDP load                                     :   3509.6 MB/s (2)
 ARM LDNP load                                    :   2538.2 MB/s (3, 0.7%)
 ARM STP fill                                     :   7720.1 MB/s (2)
 ARM STNP fill                                    :   6327.2 MB/s (2)

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)             :    233.9 MB/s (2)
 NEON LDP/STP 2-pass copy (from framebuffer)      :    233.0 MB/s (2)
 NEON LD1/ST1 copy (from framebuffer)             :     64.6 MB/s (2)
 NEON LD1/ST1 2-pass copy (from framebuffer)      :     64.6 MB/s (2)
 ARM LDP/STP copy (from framebuffer)              :    123.7 MB/s (2)
 ARM LDP/STP 2-pass copy (from framebuffer)       :    124.5 MB/s (2)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.1 ns          /     0.0 ns 
     65536 :    3.5 ns          /     5.8 ns 
    131072 :    5.5 ns          /     8.0 ns 
    262144 :    6.9 ns          /     9.4 ns 
    524288 :   60.0 ns          /    93.8 ns 
   1048576 :   92.2 ns          /   125.2 ns 
   2097152 :  108.8 ns          /   136.3 ns 
   4194304 :  121.5 ns          /   144.5 ns 
   8388608 :  128.2 ns          /   148.7 ns 
  16777216 :  132.2 ns          /   151.6 ns 
  33554432 :  135.0 ns          /   153.8 ns 
  67108864 :  148.5 ns          /   179.4 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.1 ns          /     0.0 ns 
     65536 :    3.5 ns          /     5.8 ns 
    131072 :    5.5 ns          /     8.0 ns 
    262144 :    7.0 ns          /     9.4 ns 
    524288 :   60.8 ns          /    96.2 ns 
   1048576 :   93.5 ns          /   130.0 ns 
   2097152 :  108.2 ns          /   135.5 ns 
   4194304 :  116.4 ns          /   139.3 ns 
   8388608 :  120.5 ns          /   140.9 ns 
  16777216 :  122.5 ns          /   141.4 ns 
  33554432 :  123.5 ns          /   141.8 ns 
  67108864 :  124.0 ns          /   141.9 ns 

Executing benchmark on cpu2 (Cortex-A73):

tinymembench v0.4.9-nuumio (simple benchmark for memory throughput and latency)

CFLAGS: 
bandwidth test min repeats (-b): 2
bandwidth test max repeats (-B): 3
bandwidth test mem realloc (-M): no      (-m for realloc)
      latency test repeats (-l): 3
        latency test count (-c): 1000000

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Test result is the best of repeated runs. Number of repeats  ==
==         is shown in brackets                                         ==
== Note 3: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 4: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 5: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                 :   4118.1 MB/s (2)
 C copy backwards (32 byte blocks)                :   4117.5 MB/s (2)
 C copy backwards (64 byte blocks)                :   4118.2 MB/s (2)
 C copy                                           :   4202.0 MB/s (2)
 C copy prefetched (32 bytes step)                :   4175.3 MB/s (2)
 C copy prefetched (64 bytes step)                :   4202.7 MB/s (2)
 C 2-pass copy                                    :   3000.3 MB/s (3, 0.1%)
 C 2-pass copy prefetched (32 bytes step)         :   2917.3 MB/s (2)
 C 2-pass copy prefetched (64 bytes step)         :   2931.7 MB/s (3, 0.1%)
 C scan 8                                         :    798.0 MB/s (2)
 C scan 16                                        :   1596.0 MB/s (2)
 C scan 32                                        :   3158.0 MB/s (2)
 C scan 64                                        :   6057.9 MB/s (2)
 C fill                                           :   7510.0 MB/s (2)
 C fill (shuffle within 16 byte blocks)           :   7512.5 MB/s (2)
 C fill (shuffle within 32 byte blocks)           :   7502.8 MB/s (2)
 C fill (shuffle within 64 byte blocks)           :   7512.7 MB/s (2)
 ---
 libc memcpy copy                                 :   4216.4 MB/s (3)
 libc memchr scan                                 :   9367.6 MB/s (2)
 libc memset fill                                 :   7509.3 MB/s (2)
 ---
 NEON LDP/STP copy                                :   4220.5 MB/s (2)
 NEON LDP/STP copy pldl2strm (32 bytes step)      :   4221.6 MB/s (3, 0.1%)
 NEON LDP/STP copy pldl2strm (64 bytes step)      :   4220.5 MB/s (2)
 NEON LDP/STP copy pldl1keep (32 bytes step)      :   4009.6 MB/s (2)
 NEON LDP/STP copy pldl1keep (64 bytes step)      :   4164.4 MB/s (3, 0.1%)
 NEON LD1/ST1 copy                                :   4220.2 MB/s (3, 1.8%)
 NEON LDP load                                    :   9397.4 MB/s (2)
 NEON LDNP load                                   :   9398.4 MB/s (3, 0.2%)
 NEON STP fill                                    :   7510.1 MB/s (2)
 NEON STNP fill                                   :   7508.9 MB/s (2)
 ARM LDP/STP copy                                 :   4219.6 MB/s (2)
 ARM LDP load                                     :   9284.7 MB/s (3, 0.5%)
 ARM LDNP load                                    :   9305.6 MB/s (2)
 ARM STP fill                                     :   7484.5 MB/s (2)
 ARM STNP fill                                    :   7505.1 MB/s (3, 0.5%)

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)             :    401.4 MB/s (3, 0.1%)
 NEON LDP/STP 2-pass copy (from framebuffer)      :    388.6 MB/s (3, 0.3%)
 NEON LD1/ST1 copy (from framebuffer)             :    400.6 MB/s (2)
 NEON LD1/ST1 2-pass copy (from framebuffer)      :    388.4 MB/s (2)
 ARM LDP/STP copy (from framebuffer)              :    404.8 MB/s (2)
 ARM LDP/STP 2-pass copy (from framebuffer)       :    380.5 MB/s (2)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    4.7 ns          /     7.7 ns 
    131072 :    7.0 ns          /    10.8 ns 
    262144 :    8.8 ns          /    12.1 ns 
    524288 :   10.5 ns          /    12.6 ns 
   1048576 :   19.7 ns          /    27.8 ns 
   2097152 :   71.9 ns          /   106.9 ns 
   4194304 :  101.5 ns          /   135.1 ns 
   8388608 :  119.4 ns          /   150.3 ns 
  16777216 :  128.8 ns          /   156.4 ns 
  33554432 :  134.6 ns          /   159.1 ns 
  67108864 :  137.6 ns          /   161.5 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    4.7 ns          /     7.7 ns 
    131072 :    7.0 ns          /    10.8 ns 
    262144 :    8.3 ns          /    12.1 ns 
    524288 :    9.2 ns          /    12.7 ns 
   1048576 :   10.3 ns          /    13.6 ns 
   2097152 :   70.1 ns          /   104.9 ns 
   4194304 :   99.5 ns          /   132.3 ns 
   8388608 :  114.0 ns          /   141.6 ns 
  16777216 :  120.8 ns          /   145.4 ns 
  33554432 :  124.3 ns          /   146.9 ns 
  67108864 :  126.2 ns          /   148.2 ns 

##########################################################################

Executing ramlat on cpu0 (Cortex-A53), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 2.006 1.988 1.490 1.490 1.490 1.490 2.049 4.161 
         8k: 1.987 1.987 1.490 1.490 1.490 1.490 2.049 4.160 
        16k: 1.988 1.987 1.490 1.490 1.490 1.490 2.049 4.160 
        32k: 1.992 1.991 1.493 1.492 1.493 1.493 2.055 4.166 
        64k: 16.77 15.52 13.49 15.07 13.49 15.19 18.50 33.43 
       128k: 17.34 17.10 15.97 16.80 15.97 16.88 19.85 38.14 
       256k: 17.99 17.93 17.48 17.84 17.46 17.92 20.56 39.26 
       512k: 115.8 131.8 123.9 128.8 123.7 129.7 164.7 304.4 
      1024k: 137.8 138.6 137.9 137.7 138.0 138.4 168.8 323.0 
      2048k: 122.7 123.0 122.7 122.4 122.7 122.8 161.7 312.3 
      4096k: 131.2 131.9 131.3 131.0 131.2 131.4 162.0 317.9 
      8192k: 134.5 134.7 134.5 134.2 134.5 134.7 165.7 319.1 
     16384k: 136.9 137.0 137.0 137.7 137.0 136.9 166.6 320.9 
     32768k: 137.2 137.6 137.2 136.8 137.2 137.2 168.3 319.5 
     65536k: 137.8 137.9 137.7 137.1 137.8 137.8 168.5 312.2 
    131072k: 138.3 138.3 138.2 137.8 138.2 138.2 165.9 311.3 

Executing ramlat on cpu2 (Cortex-A73), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 1.669 1.668 1.668 1.668 1.251 1.251 2.374 3.306 
         8k: 1.668 1.668 1.668 1.669 1.251 1.251 2.316 3.296 
        16k: 1.668 1.668 1.668 1.668 1.251 1.251 1.736 3.297 
        32k: 1.670 1.669 1.670 1.669 1.252 1.252 1.748 3.298 
        64k: 8.870 9.172 8.870 9.173 8.941 9.085 10.72 19.48 
       128k: 10.39 10.39 10.37 10.39 10.38 10.38 11.86 23.28 
       256k: 10.42 10.43 10.42 10.42 10.42 10.42 11.68 23.49 
       512k: 10.45 10.42 10.42 10.42 10.42 10.42 11.68 23.50 
      1024k: 11.32 10.96 11.26 10.90 11.27 13.66 12.31 24.57 
      2048k: 123.7 123.8 124.0 123.8 122.7 121.8 127.5 152.7 
      4096k: 132.4 132.4 131.9 132.3 131.4 130.8 134.4 163.7 
      8192k: 136.5 136.4 136.4 137.2 135.6 134.4 135.1 173.8 
     16384k: 138.7 138.6 138.7 138.7 138.6 136.7 136.6 178.6 
     32768k: 139.3 139.3 139.2 139.3 139.2 137.1 137.7 183.2 
     65536k: 141.3 141.4 141.3 141.4 141.4 140.2 140.0 183.7 
    131072k: 141.6 141.8 141.7 141.8 141.7 140.5 139.6 185.8 

##########################################################################

Executing benchmark on each cluster individually

OpenSSL 1.1.1n, built on 15 Mar 2022
type             16 bytes     64 bytes    256 bytes   1024 bytes   8192 bytes  16384 bytes
aes-128-cbc     130948.77k   416337.28k   890559.32k  1299143.68k  1499187.88k  1514897.41k (Cortex-A53)
aes-128-cbc     365161.19k   949810.65k  1515313.15k  1777747.97k  1902089.56k  1910489.09k (Cortex-A73)
aes-192-cbc     125298.14k   377637.70k   743570.26k  1011984.73k  1130378.58k  1127246.51k (Cortex-A53)
aes-192-cbc     340667.90k   850499.50k  1273256.45k  1503838.55k  1586883.24k  1593043.63k (Cortex-A73)
aes-256-cbc     122455.06k   352694.29k   653012.22k   851485.70k   933934.42k   938633.90k (Cortex-A53)
aes-256-cbc     326408.62k   777245.55k  1146589.18k  1295141.89k  1361668.78k  1366185.30k (Cortex-A73)

##########################################################################

Executing benchmark single-threaded on cpu0 (Cortex-A53)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,6 CPUs LE)

LE
CPU Freq: 64000000 - - - 128000000 256000000 - - -

RAM size:    3785 MB,  # CPU hardware threads:   6
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       1170    99   1146   1139  |      21539   100   1847   1839
23:       1150    99   1180   1172  |      21052   100   1831   1822
24:       1135    99   1230   1221  |      20626   100   1820   1811
25:       1133    99   1303   1295  |      20257   100   1812   1803
----------------------------------  | ------------------------------
Avr:              99   1215   1207  |              100   1827   1819
Tot:              99   1521   1513

Executing benchmark single-threaded on cpu2 (Cortex-A73)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,6 CPUs LE)

LE
CPU Freq: 64000000 - 64000000 - - - - - -

RAM size:    3785 MB,  # CPU hardware threads:   6
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       2049   100   1999   1993  |      31428   100   2686   2683
23:       2004   100   2048   2043  |      30713   100   2662   2659
24:       1984   100   2139   2134  |      29963   100   2634   2630
25:       1956   100   2239   2234  |      29289   100   2616   2607
----------------------------------  | ------------------------------
Avr:             100   2106   2101  |              100   2649   2645
Tot:             100   2378   2373

##########################################################################

Executing benchmark 3 times multi-threaded on CPUs 0-5

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,6 CPUs LE)

LE
CPU Freq: 64000000 64000000 - - - - - - -

RAM size:    3785 MB,  # CPU hardware threads:   6
RAM usage:   1323 MB,  # Benchmark threads:      6

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       7419   552   1308   7218  |     140344   508   2357  11969
23:       7311   562   1325   7450  |     136778   507   2335  11835
24:       7175   563   1371   7715  |     128413   471   2395  11271
25:       7078   565   1431   8082  |     131084   507   2303  11666
----------------------------------  | ------------------------------
Avr:             560   1359   7616  |              498   2347  11685
Tot:             529   1853   9651

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,6 CPUs LE)

LE
CPU Freq: - - - - - - 512000000 - -

RAM size:    3785 MB,  # CPU hardware threads:   6
RAM usage:   1323 MB,  # Benchmark threads:      6

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       7534   561   1308   7330  |     140002   507   2355  11940
23:       7241   556   1326   7378  |     136968   507   2335  11852
24:       7221   565   1373   7764  |     133806   508   2313  11744
25:       7141   568   1437   8154  |     131281   509   2297  11683
----------------------------------  | ------------------------------
Avr:             562   1361   7657  |              508   2325  11805
Tot:             535   1843   9731

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,6 CPUs LE)

LE
CPU Freq: 64000000 - - - - 256000000 - - -

RAM size:    3785 MB,  # CPU hardware threads:   6
RAM usage:   1323 MB,  # Benchmark threads:      6

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       7584   562   1313   7378  |     140042   507   2357  11943
23:       7323   562   1328   7462  |     136997   507   2337  11854
24:       7269   569   1374   7816  |     133600   507   2314  11726
25:       7130   567   1435   8141  |     131397   509   2299  11694
----------------------------------  | ------------------------------
Avr:             565   1363   7699  |              507   2327  11804
Tot:             536   1845   9752

Compression: 7616,7657,7699
Decompression: 11685,11805,11804
Total: 9651,9731,9752

##########################################################################

Testing maximum cpufreq again, still under full load. System health now:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
09:02:10: 2400/2016MHz  5.70  98%   1%  96%   0%   0%   0%  46.9Â°C

Checking cpufreq OPP for cpu0-cpu1 (Cortex-A53):

Cpufreq OPP: 2016    Measured: 2013 (2013.606/2013.557/2013.507)

Checking cpufreq OPP for cpu2-cpu5 (Cortex-A73):

Cpufreq OPP: 2400    Measured: 2398 (2398.769/2398.769/2398.658)

##########################################################################

Hardware sensors:

gpio_fan-isa-0000
fan1:        1600 RPM  (min =    0 RPM, max = 1600 RPM)

cpu_thermal-virtual-0
temp1:        +34.2 C  (crit = +110.0 C)

ddr_thermal-virtual-0
temp1:        +36.3 C  (crit = +110.0 C)

##########################################################################

Thermal source: /sys/class/hwmon/hwmon0/ (cpu_thermal)

System health while running tinymembench:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
08:50:48: 2400/2016MHz  1.07   1%   0%   0%   0%   0%   0%  31.0Â°C
08:51:08: 2400/2016MHz  1.05  16%   0%  16%   0%   0%   0%  29.1Â°C
08:51:28: 2400/2016MHz  1.04  16%   0%  16%   0%   0%   0%  29.6Â°C
08:51:48: 2400/2016MHz  1.02  16%   0%  16%   0%   0%   0%  29.4Â°C
08:52:08: 2400/2016MHz  1.02  16%   0%  16%   0%   0%   0%  29.1Â°C
08:52:28: 2400/2016MHz  1.01  16%   0%  16%   0%   0%   0%  31.4Â°C
08:52:48: 2400/2016MHz  1.01  16%   0%  16%   0%   0%   0%  32.4Â°C
08:53:08: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  30.8Â°C

System health while running ramlat:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
08:53:28: 2400/2016MHz  1.00   2%   0%   1%   0%   0%   0%  32.1Â°C
08:53:34: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  29.6Â°C
08:53:40: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  29.7Â°C
08:53:46: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  29.7Â°C
08:53:52: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  29.6Â°C
08:53:58: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  29.6Â°C
08:54:04: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  30.5Â°C
08:54:10: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  30.8Â°C
08:54:16: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  30.6Â°C
08:54:22: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  30.3Â°C
08:54:28: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  30.6Â°C

System health while running OpenSSL benchmark:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
08:54:34: 2400/2016MHz  1.08   2%   0%   1%   0%   0%   0%  32.2Â°C
08:54:51: 2400/2016MHz  1.06  16%   0%  16%   0%   0%   0%  29.7Â°C
08:55:07: 2400/2016MHz  1.12  16%   0%  16%   0%   0%   0%  31.7Â°C
08:55:23: 2400/2016MHz  1.09  16%   0%  16%   0%   0%   0%  29.8Â°C
08:55:39: 2400/2016MHz  1.07  16%   0%  16%   0%   0%   0%  32.1Â°C
08:55:55: 2400/2016MHz  1.05  16%   0%  16%   0%   0%   0%  29.9Â°C
08:56:11: 2400/2016MHz  1.04  16%   0%  16%   0%   0%   0%  32.3Â°C

System health while running 7-zip single core benchmark:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
08:56:23: 2400/2016MHz  1.03   2%   0%   2%   0%   0%   0%  33.5Â°C
08:56:30: 2400/2016MHz  1.03  16%   0%  16%   0%   0%   0%  30.0Â°C
08:56:37: 2400/2016MHz  1.02  16%   0%  16%   0%   0%   0%  29.9Â°C
08:56:44: 2400/2016MHz  1.02  16%   0%  16%   0%   0%   0%  30.0Â°C
08:56:51: 2400/2016MHz  1.02  16%   0%  16%   0%   0%   0%  29.9Â°C
08:56:58: 2400/2016MHz  1.02  16%   0%  16%   0%   0%   0%  29.8Â°C
08:57:05: 2400/2016MHz  1.01  16%   0%  16%   0%   0%   0%  30.0Â°C
08:57:12: 2400/2016MHz  1.01  16%   0%  16%   0%   0%   0%  30.0Â°C
08:57:19: 2400/2016MHz  1.01  16%   0%  16%   0%   0%   0%  29.7Â°C
08:57:26: 2400/2016MHz  1.01  16%   0%  16%   0%   0%   0%  29.8Â°C
08:57:33: 2400/2016MHz  1.01  16%   0%  16%   0%   0%   0%  30.0Â°C
08:57:40: 2400/2016MHz  1.01  16%   0%  16%   0%   0%   0%  29.9Â°C
08:57:47: 2400/2016MHz  1.01  16%   0%  16%   0%   0%   0%  29.9Â°C
08:57:54: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  29.9Â°C
08:58:01: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  31.1Â°C
08:58:08: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  31.5Â°C
08:58:15: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  31.6Â°C
08:58:22: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  31.7Â°C
08:58:29: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  31.7Â°C
08:58:36: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  31.7Â°C
08:58:43: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  31.7Â°C
08:58:50: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  31.7Â°C
08:58:57: 2400/2016MHz  1.00  16%   0%  16%   0%   0%   0%  31.7Â°C

System health while running 7-zip multi core benchmark:

Time       big.LITTLE   load %cpu %sys %usr %nice %io %irq   Temp
08:59:02: 2400/2016MHz  1.00   3%   0%   2%   0%   0%   0%  38.4Â°C
08:59:13: 2400/2016MHz  1.84  97%   0%  96%   0%   0%   0%  44.7Â°C
08:59:25: 2400/2016MHz  2.11  85%   0%  84%   0%   0%   0%  44.3Â°C
08:59:35: 2400/2016MHz  2.97  80%   1%  79%   0%   0%   0%  44.1Â°C
08:59:45: 2400/2016MHz  3.36  74%   0%  73%   0%   0%   0%  40.6Â°C
08:59:56: 2400/2016MHz  4.07  96%   1%  94%   0%   0%   0%  40.9Â°C
09:00:06: 2400/2016MHz  4.13  85%   0%  83%   0%   0%   0%  40.6Â°C
09:00:17: 2400/2016MHz  4.49  97%   0%  96%   0%   0%   0%  46.1Â°C
09:00:29: 2400/2016MHz  4.65  85%   0%  84%   0%   0%   0%  46.0Â°C
09:00:42: 2400/2016MHz  4.72  83%   1%  82%   0%   0%   0%  45.7Â°C
09:00:53: 2400/2016MHz  4.70  81%   1%  80%   0%   0%   0%  41.7Â°C
09:01:07: 2400/2016MHz  5.05  98%   1%  96%   0%   0%   0%  46.1Â°C
09:01:20: 2400/2016MHz  5.55  88%   0%  87%   0%   0%   0%  46.7Â°C
09:01:33: 2400/2016MHz  5.79  87%   0%  86%   0%   0%   0%  46.8Â°C
09:01:46: 2400/2016MHz  5.61  84%   1%  82%   0%   0%   0%  46.6Â°C
09:01:57: 2400/2016MHz  5.47  81%   1%  80%   0%   0%   0%  43.2Â°C
09:02:10: 2400/2016MHz  5.70  98%   1%  96%   0%   0%   0%  46.9Â°C

##########################################################################

Linux 6.1.11-meson64 (odroidn2) 	03/27/23 	_aarch64_	(6 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           6.41    0.00    0.45    0.02    0.00   93.11

Device             tps    kB_read/s    kB_wrtn/s    kB_dscd/s    kB_read    kB_wrtn    kB_dscd
mmcblk1           5.04       117.72       134.88         0.00     518916     594573          0
zram0             0.14         0.54         0.00         0.00       2384          4          0
zram1             0.12         0.12         0.56         0.00        516       2452          0

               total        used        free      shared  buff/cache   available
Mem:           3.7Gi       181Mi       3.4Gi       5.0Mi       133Mi       3.4Gi
Swap:          1.8Gi          0B       1.8Gi

Filename				Type		Size	Used	Priority
/dev/zram0                             	partition	1938044	0	5

CPU sysfs topology (clusters, cpufreq members, clockspeeds)
                 cpufreq   min    max
 CPU    cluster  policy   speed  speed   core type
  0        0        0     1000    2016   Cortex-A53 / r0p4
  1        0        0     1000    2016   Cortex-A53 / r0p4
  2        0        2     1000    2400   Cortex-A73 / r0p2
  3        0        2     1000    2400   Cortex-A73 / r0p2
  4        0        2     1000    2400   Cortex-A73 / r0p2
  5        0        2     1000    2400   Cortex-A73 / r0p2

Architecture:                    aarch64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
CPU(s):                          6
On-line CPU(s) list:             0-5
Thread(s) per core:              1
Core(s) per socket:              6
Socket(s):                       1
NUMA node(s):                    1
Vendor ID:                       ARM
Model:                           4
Model name:                      Cortex-A53
Stepping:                        r0p4
CPU max MHz:                     2400.0000
CPU min MHz:                     1000.0000
BogoMIPS:                        48.00
NUMA node0 CPU(s):               0-5
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Not affected
Vulnerability Retbleed:          Not affected
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1:        Mitigation; __user pointer sanitization
Vulnerability Spectre v2:        Vulnerable
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid

SoC guess: Amlogic Meson G12B (S922X) Revision 29:c (40:2)
DT compat: hardkernel,odroid-n2-plus
           amlogic,s922x
           amlogic,g12b
 Compiler: /usr/bin/gcc (Debian 10.2.1-6) 10.2.1 20210110 / aarch64-linux-gnu
 Userland: arm64
   Kernel: 6.1.11-meson64/aarch64
           CONFIG_HZ=250
           CONFIG_HZ_250=y
           CONFIG_PREEMPTION=y
           CONFIG_PREEMPT=y
           CONFIG_PREEMPT_BUILD=y
           CONFIG_PREEMPT_COUNT=y
           CONFIG_PREEMPT_NOTIFIERS=y
           CONFIG_PREEMPT_RCU=y

##########################################################################

Kernel 6.1.11 is not latest 6.1.21 LTS that was released on 2023-03-22.

See https://endoflife.date/linux for details. Perhaps some kernel bugs have
been fixed in the meantime and maybe vulnerabilities as well.

##########################################################################

   opp-table-0:
      1000 MHz    761.0 mV
      1200 MHz    781.0 mV
      1398 MHz    811.0 mV
      1512 MHz    861.0 mV
      1608 MHz    901.0 mV
      1704 MHz    951.0 mV
      1800 MHz   1001.0 mV
      1908 MHz   1030.0 mV
      2016 MHz   1040.0 mV

   opp-table-1:
      1000 MHz    731.0 mV
      1200 MHz    751.0 mV
      1398 MHz    771.0 mV
      1512 MHz    771.0 mV
      1608 MHz    781.0 mV
      1704 MHz    791.0 mV
      1800 MHz    831.0 mV
      1908 MHz    861.0 mV
      2016 MHz    911.0 mV
      2108 MHz    951.0 mV
      2208 MHz   1011.0 mV
      2304 MHz   1030.0 mV
      2400 MHz   1040.0 mV

   opp-table-gpu:
       125 MHz    800.0 mV
       250 MHz    800.0 mV
       286 MHz    800.0 mV
       400 MHz    800.0 mV
       500 MHz    800.0 mV
       667 MHz    800.0 mV
       800 MHz    800.0 mV

##########################################################################

Results validation:

  * No mismatch between advertised and measured max CPU clockspeed
  * No swapping
  * Background activity (%system) OK
  * No throttling

Status of performance related governors found below /sys (w/o cpufreq):

  * ffe40000.gpu: simple_ondemand / 125 MHz (powersave performance simple_ondemand / 125 250 286 400 500 667 800)

| Hardkernel ODROID-N2Plus | 2400/2016 MHz | 6.1 | Debian GNU/Linux 11 (bullseye) arm64 | 9710 | 2373 | 1366180 | 4220 | 7720 | - |