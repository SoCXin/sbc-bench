sbc-bench v0.9.70 Radxa Orion O6 (Thu, 30 Jan 2025 17:45:08 +0800)

Distributor ID:	Debian
Description:	Debian GNU/Linux 12 (bookworm)
Release:	12
Codename:	bookworm

Device Info:
	Manufacturer: Radxa Computer (Shenzhen) Co., Ltd.
	Product Name: Radxa Orion O6
	Version: 1.0
	Family: Orion O6

BIOS/UEFI:
	Vendor: Radxa Computer (Shenzhen) Co., Ltd.
	Version: 1.0
	Release Date: Jan  1 1980
	BIOS Revision: 1.0

/usr/bin/gcc (Debian 12.2.0-14) 12.2.0

Uptime: 17:45:08 up 47 min,  5 users,  load average: 4.13, 4.40, 5.36,  28.0°C,  75002660

Linux 6.1.44-cix-build-generic (orion-o6) 	01/30/25 	_aarch64_	(12 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          17.48    0.00    0.20    0.06    0.00   82.26

Device             tps    kB_read/s    kB_wrtn/s    kB_dscd/s    kB_read    kB_wrtn    kB_dscd
nvme0n1           0.07         1.49         0.00         0.00       4264          0          0
sda              16.11       199.99        74.06         0.00     573709     212436          0

               total        used        free      shared  buff/cache   available
Mem:            14Gi       1.1Gi        13Gi        11Mi       283Mi        13Gi
Swap:             0B          0B          0B

##########################################################################

Checking cpufreq OPP for cpu0 (Cortex-A720):

Cpufreq OPP: 2600    Measured: 2598 (2598.787/2598.755/2598.723)
Cpufreq OPP: 2299    Measured: 2298 (2298.885/2298.799/2298.799)
Cpufreq OPP: 2099    Measured: 2098 (2098.841/2098.841/2098.815)
Cpufreq OPP: 1799    Measured: 1798 (1798.920/1798.898/1798.898)
Cpufreq OPP: 1500    Measured: 1498 (1498.981/1498.925/1498.906)
Cpufreq OPP: 1199    Measured: 1198 (1198.995/1198.980/1198.935)
Cpufreq OPP:  799    Measured:  798    (799.028/798.958/798.948)

Checking cpufreq OPP for cpu1-cpu4 (Cortex-A520):

Cpufreq OPP: 1799    Measured: 1796 (1796.724/1796.590/1796.590)
Cpufreq OPP:  799    Measured:  797    (797.624/797.574/797.564)

Checking cpufreq OPP for cpu5-cpu6 (Cortex-A720):

Cpufreq OPP: 2299    Measured: 2299 (2299.050/2299.021/2298.992)
Cpufreq OPP: 2099    Measured: 2099 (2099.064/2099.038/2099.038)
Cpufreq OPP: 1999    Measured: 1999 (1999.047/1999.047/1998.922)
Cpufreq OPP: 1799    Measured: 1799 (1799.122/1799.100/1799.078)
Cpufreq OPP: 1500    Measured: 1499 (1499.095/1499.058/1499.001)
Cpufreq OPP: 1199    Measured: 1199 (1199.170/1199.155/1198.960)
Cpufreq OPP:  799    Measured:  799    (799.167/799.167/799.157)

Checking cpufreq OPP for cpu7-cpu8 (Cortex-A720):

Cpufreq OPP: 2199    Measured: 2199 (2199.055/2199.055/2198.945)
Cpufreq OPP: 1999    Measured: 1999 (1999.072/1999.072/1999.072)
Cpufreq OPP: 1899    Measured: 1899 (1899.095/1899.095/1899.071)
Cpufreq OPP: 1799    Measured: 1799 (1799.100/1799.100/1799.078)
Cpufreq OPP: 1500    Measured: 1499 (1499.141/1499.122/1499.104)
Cpufreq OPP: 1199    Measured: 1199 (1199.183/1199.168/1199.168)
Cpufreq OPP:  799    Measured:  799    (799.212/799.212/799.162)

Checking cpufreq OPP for cpu9-cpu10 (Cortex-A720):

Cpufreq OPP: 2499    Measured: 2499 (2499.043/2499.011/2499.011)
Cpufreq OPP: 2199    Measured: 2199 (2199.060/2199.060/2199.060)
Cpufreq OPP: 1999    Measured: 1999 (1999.097/1999.072/1999.047)
Cpufreq OPP: 1799    Measured: 1799 (1799.122/1799.100/1799.100)
Cpufreq OPP: 1500    Measured: 1499 (1499.123/1499.123/1499.123)
Cpufreq OPP: 1199    Measured: 1199 (1199.155/1199.155/1199.140)
Cpufreq OPP:  799    Measured:  799    (799.212/799.212/799.112)

Checking cpufreq OPP for cpu11 (Cortex-A720):

No cpufreq support. Measured: 2598 MHz (2599.030/2598.997/2598.965)

##########################################################################

Hardware sensors:

scmi_sensors-virtual-0
VPU:          +27.0 C  
GPU_btm:      +26.0 C  
GPU_top:      +27.0 C  
SOC_BRC:      +28.0 C  
DDR_btm:      +28.0 C  
DDR_top:      +28.0 C  
CI:           +27.0 C  
NPU:          +27.0 C  
CPU_M1:       +27.0 C  
CPU_B1:       +30.0 C  
CPU_M0:       +28.0 C  
CPU_B0:       +28.0 C  
SOC_TRC:      +29.0 C  
PCB_HOT:      +28.0 C  
PCB_AMB:      +28.0 C  

/dev/nvme0:	31°C
/dev/sda:	24°C

##########################################################################

Executing benchmark on cpu0 (Cortex-A720):

tinymembench v0.4.9-nuumio (simple benchmark for memory throughput and latency)

CFLAGS: 
bandwidth test min repeats (-b): 2
bandwidth test max repeats (-B): 3
bandwidth test mem realloc (-M): no      (-m for realloc)
      latency test repeats (-l): 3
        latency test count (-c): 1000000

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Test result is the best of repeated runs. Number of repeats  ==
==         is shown in brackets                                         ==
== Note 3: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 4: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 5: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                 :  14118.0 MB/s (3, 5.2%)
 C copy backwards (32 byte blocks)                :  14618.7 MB/s (2)
 C copy backwards (64 byte blocks)                :  14715.1 MB/s (3, 0.2%)
 C copy                                           :  15579.2 MB/s (3, 0.5%)
 C copy prefetched (32 bytes step)                :  14462.7 MB/s (3, 0.8%)
 C copy prefetched (64 bytes step)                :  14262.3 MB/s (3, 0.2%)
 C 2-pass copy                                    :  14049.0 MB/s (2)
 C 2-pass copy prefetched (32 bytes step)         :  13568.8 MB/s (3, 1.5%)
 C 2-pass copy prefetched (64 bytes step)         :  16018.2 MB/s (3, 0.2%)
 C scan 8                                         :   2516.9 MB/s (3, 11.4%)
 C scan 16                                        :   5188.5 MB/s (3, 11.4%)
 C scan 32                                        :   8058.6 MB/s (2)
 C scan 64                                        :  16555.1 MB/s (3, 0.2%)
 C fill                                           :  41563.0 MB/s (2)
 C fill (shuffle within 16 byte blocks)           :  41568.3 MB/s (2)
 C fill (shuffle within 32 byte blocks)           :  41566.1 MB/s (2)
 C fill (shuffle within 64 byte blocks)           :  41565.5 MB/s (2)
 ---
 libc memcpy copy                                 :  13338.2 MB/s (3, 1.8%)
 libc memchr scan                                 :  26833.0 MB/s (2)
 libc memset fill                                 :  31438.5 MB/s (3, 7.3%)
 ---
 NEON LDP/STP copy                                :  17132.2 MB/s (3, 2.5%)
 NEON LDP/STP copy pldl2strm (32 bytes step)      :  14697.8 MB/s (2)
 NEON LDP/STP copy pldl2strm (64 bytes step)      :  15397.7 MB/s (2)
 NEON LDP/STP copy pldl1keep (32 bytes step)      :  16276.5 MB/s (3, 0.3%)
 NEON LDP/STP copy pldl1keep (64 bytes step)      :  16519.9 MB/s (3, 0.5%)
 NEON LD1/ST1 copy                                :  16624.7 MB/s (3, 0.2%)
 NEON LDP load                                    :  22046.9 MB/s (3, 0.8%)
 NEON LDNP load                                   :  22500.1 MB/s (3, 0.2%)
 NEON STP fill                                    :  48365.9 MB/s (3, 1.3%)
 NEON STNP fill                                   :  47582.1 MB/s (3, 2.0%)
 ARM LDP/STP copy                                 :  17096.9 MB/s (3, 1.5%)
 ARM LDP load                                     :  22648.4 MB/s (3, 0.7%)
 ARM LDNP load                                    :  22568.3 MB/s (3, 0.3%)
 ARM STP fill                                     :  48195.2 MB/s (3, 1.3%)
 ARM STNP fill                                    :  47314.6 MB/s (3, 1.7%)

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)             :   2198.4 MB/s (2)
 NEON LDP/STP 2-pass copy (from framebuffer)      :   1492.0 MB/s (2)
 NEON LD1/ST1 copy (from framebuffer)             :   2196.8 MB/s (2)
 NEON LD1/ST1 2-pass copy (from framebuffer)      :   1485.3 MB/s (3, 0.3%)
 ARM LDP/STP copy (from framebuffer)              :   1875.0 MB/s (2)
 ARM LDP/STP 2-pass copy (from framebuffer)       :   1158.9 MB/s (2)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.0 ns          /     1.5 ns 
    262144 :    2.2 ns          /     2.8 ns 
    524288 :    7.4 ns          /    12.0 ns 
   1048576 :   22.9 ns          /    30.8 ns 
   2097152 :   37.2 ns          /    40.0 ns 
   4194304 :   38.5 ns          /    42.2 ns 
   8388608 :   60.6 ns          /    41.5 ns 
  16777216 :   97.5 ns          /    68.9 ns 
  33554432 :  155.6 ns          /   173.9 ns 
  67108864 :  185.6 ns          /   229.9 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.0 ns          /     1.5 ns 
    262144 :    1.7 ns          /     2.1 ns 
    524288 :    2.1 ns          /     2.4 ns 
   1048576 :   18.6 ns          /    28.3 ns 
   2097152 :   33.4 ns          /    35.7 ns 
   4194304 :   37.7 ns          /    37.7 ns 
   8388608 :   53.6 ns          /    38.6 ns 
  16777216 :   43.9 ns          /    51.8 ns 
  33554432 :  112.7 ns          /   156.2 ns 
  67108864 :  158.2 ns          /   203.1 ns 

Executing benchmark on cpu1 (Cortex-A520):

tinymembench v0.4.9-nuumio (simple benchmark for memory throughput and latency)

CFLAGS: 
bandwidth test min repeats (-b): 2
bandwidth test max repeats (-B): 3
bandwidth test mem realloc (-M): no      (-m for realloc)
      latency test repeats (-l): 3
        latency test count (-c): 1000000

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Test result is the best of repeated runs. Number of repeats  ==
==         is shown in brackets                                         ==
== Note 3: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 4: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 5: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                 :   8568.7 MB/s (2)
 C copy backwards (32 byte blocks)                :   8857.9 MB/s (2)
 C copy backwards (64 byte blocks)                :   8777.0 MB/s (2)
 C copy                                           :   8206.2 MB/s (2)
 C copy prefetched (32 bytes step)                :   8175.2 MB/s (2)
 C copy prefetched (64 bytes step)                :   8270.9 MB/s (2)
 C 2-pass copy                                    :   4892.1 MB/s (2)
 C 2-pass copy prefetched (32 bytes step)         :   4676.8 MB/s (2)
 C 2-pass copy prefetched (64 bytes step)         :   4897.0 MB/s (2)
 C scan 8                                         :    843.8 MB/s (2)
 C scan 16                                        :   1593.5 MB/s (2)
 C scan 32                                        :   3185.0 MB/s (2)
 C scan 64                                        :   5218.5 MB/s (3, 1.0%)
 C fill                                           :  14362.9 MB/s (2)
 C fill (shuffle within 16 byte blocks)           :  14367.0 MB/s (2)
 C fill (shuffle within 32 byte blocks)           :   2250.7 MB/s (3, 0.6%)
 C fill (shuffle within 64 byte blocks)           :   2841.3 MB/s (3, 0.2%)
 ---
 libc memcpy copy                                 :   8870.4 MB/s (2)
 libc memchr scan                                 :   1792.8 MB/s (2)
 libc memset fill                                 :  28407.7 MB/s (2)
 ---
 NEON LDP/STP copy                                :   9043.6 MB/s (2)
 NEON LDP/STP copy pldl2strm (32 bytes step)      :   8549.7 MB/s (2)
 NEON LDP/STP copy pldl2strm (64 bytes step)      :   8742.9 MB/s (2)
 NEON LDP/STP copy pldl1keep (32 bytes step)      :   8947.5 MB/s (2)
 NEON LDP/STP copy pldl1keep (64 bytes step)      :   9076.8 MB/s (2)
 NEON LD1/ST1 copy                                :   8812.3 MB/s (2)
 NEON LDP load                                    :  10883.8 MB/s (2)
 NEON LDNP load                                   :  10885.2 MB/s (2)
 NEON STP fill                                    :  22948.6 MB/s (2)
 NEON STNP fill                                   :  22953.7 MB/s (2)
 ARM LDP/STP copy                                 :   9055.4 MB/s (2)
 ARM LDP load                                     :  10925.1 MB/s (2)
 ARM LDNP load                                    :  10821.4 MB/s (2)
 ARM STP fill                                     :  28406.2 MB/s (2)
 ARM STNP fill                                    :  28412.8 MB/s (2)

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)             :     72.7 MB/s (2)
 NEON LDP/STP 2-pass copy (from framebuffer)      :     72.5 MB/s (2)
 NEON LD1/ST1 copy (from framebuffer)             :     72.5 MB/s (2)
 NEON LD1/ST1 2-pass copy (from framebuffer)      :     71.9 MB/s (2)
 ARM LDP/STP copy (from framebuffer)              :     72.7 MB/s (2)
 ARM LDP/STP 2-pass copy (from framebuffer)       :     72.4 MB/s (2)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.4 ns          /     0.5 ns 
      2048 :    0.5 ns          /     0.5 ns 
      4096 :    0.6 ns          /     0.6 ns 
      8192 :    0.6 ns          /     0.6 ns 
     16384 :    0.7 ns          /     0.7 ns 
     32768 :    2.5 ns          /     4.2 ns 
     65536 :   19.0 ns          /    28.6 ns 
    131072 :   28.1 ns          /    36.4 ns 
    262144 :   33.3 ns          /    38.8 ns 
    524288 :   36.3 ns          /    40.9 ns 
   1048576 :   38.8 ns          /    41.7 ns 
   2097152 :   44.7 ns          /    42.1 ns 
   4194304 :   41.6 ns          /    42.2 ns 
   8388608 :   60.9 ns          /    43.5 ns 
  16777216 :  100.0 ns          /    71.7 ns 
  33554432 :  170.4 ns          /   190.0 ns 
  67108864 :  200.7 ns          /   243.3 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.4 ns          /     0.5 ns 
      2048 :    0.5 ns          /     0.6 ns 
      4096 :    0.5 ns          /     0.6 ns 
      8192 :    0.6 ns          /     0.6 ns 
     16384 :    0.7 ns          /     0.7 ns 
     32768 :    2.5 ns          /     4.2 ns 
     65536 :   19.0 ns          /    28.6 ns 
    131072 :   28.3 ns          /    36.4 ns 
    262144 :   33.2 ns          /    38.8 ns 
    524288 :   36.0 ns          /    39.5 ns 
   1048576 :   38.1 ns          /    39.8 ns 
   2097152 :   41.7 ns          /    40.0 ns 
   4194304 :   43.2 ns          /    40.1 ns 
   8388608 :   56.5 ns          /    40.2 ns 
  16777216 :   46.1 ns          /    53.7 ns 
  33554432 :  112.8 ns          /   153.4 ns 
  67108864 :  162.5 ns          /   205.1 ns 

Executing benchmark on cpu5 (Cortex-A720):

tinymembench v0.4.9-nuumio (simple benchmark for memory throughput and latency)

CFLAGS: 
bandwidth test min repeats (-b): 2
bandwidth test max repeats (-B): 3
bandwidth test mem realloc (-M): no      (-m for realloc)
      latency test repeats (-l): 3
        latency test count (-c): 1000000

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Test result is the best of repeated runs. Number of repeats  ==
==         is shown in brackets                                         ==
== Note 3: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 4: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 5: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                 :  14105.1 MB/s (3, 1.9%)
 C copy backwards (32 byte blocks)                :  14620.6 MB/s (3, 1.1%)
 C copy backwards (64 byte blocks)                :  14737.2 MB/s (2)
 C copy                                           :  15790.0 MB/s (2)
 C copy prefetched (32 bytes step)                :  14306.4 MB/s (3, 0.4%)
 C copy prefetched (64 bytes step)                :  14035.8 MB/s (2)
 C 2-pass copy                                    :  12046.1 MB/s (2)
 C 2-pass copy prefetched (32 bytes step)         :  12094.1 MB/s (3, 0.5%)
 C 2-pass copy prefetched (64 bytes step)         :  13240.3 MB/s (2)
 C scan 8                                         :   2297.4 MB/s (2)
 C scan 16                                        :   3906.4 MB/s (3, 5.0%)
 C scan 32                                        :   8252.6 MB/s (3, 5.6%)
 C scan 64                                        :  14586.8 MB/s (3, 1.4%)
 C fill                                           :  36704.0 MB/s (3, 1.0%)
 C fill (shuffle within 16 byte blocks)           :  36757.6 MB/s (2)
 C fill (shuffle within 32 byte blocks)           :  36757.4 MB/s (2)
 C fill (shuffle within 64 byte blocks)           :  36757.3 MB/s (2)
 ---
 libc memcpy copy                                 :  15861.2 MB/s (3, 1.0%)
 libc memchr scan                                 :  23934.9 MB/s (3, 0.1%)
 libc memset fill                                 :  44449.0 MB/s (3, 2.1%)
 ---
 NEON LDP/STP copy                                :  16842.9 MB/s (3, 2.0%)
 NEON LDP/STP copy pldl2strm (32 bytes step)      :  14771.5 MB/s (2)
 NEON LDP/STP copy pldl2strm (64 bytes step)      :  15286.2 MB/s (3, 0.1%)
 NEON LDP/STP copy pldl1keep (32 bytes step)      :  16014.8 MB/s (2)
 NEON LDP/STP copy pldl1keep (64 bytes step)      :  16562.1 MB/s (3, 0.5%)
 NEON LD1/ST1 copy                                :  16340.9 MB/s (2)
 NEON LDP load                                    :  22661.8 MB/s (3, 0.5%)
 NEON LDNP load                                   :  23392.4 MB/s (2)
 NEON STP fill                                    :  43281.4 MB/s (3, 2.2%)
 NEON STNP fill                                   :  42261.2 MB/s (3, 3.9%)
 ARM LDP/STP copy                                 :  16583.2 MB/s (3, 2.1%)
 ARM LDP load                                     :  24087.9 MB/s (2)
 ARM LDNP load                                    :  23949.8 MB/s (2)
 ARM STP fill                                     :  42322.4 MB/s (3, 2.1%)
 ARM STNP fill                                    :  41618.2 MB/s (3, 3.1%)

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)             :   2209.2 MB/s (2)
 NEON LDP/STP 2-pass copy (from framebuffer)      :   1505.6 MB/s (2)
 NEON LD1/ST1 copy (from framebuffer)             :   2210.8 MB/s (2)
 NEON LD1/ST1 2-pass copy (from framebuffer)      :   1496.7 MB/s (2)
 ARM LDP/STP copy (from framebuffer)              :   1883.4 MB/s (2)
 ARM LDP/STP 2-pass copy (from framebuffer)       :   1170.6 MB/s (2)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.9 ns 
    262144 :    1.9 ns          /     2.8 ns 
    524288 :    4.4 ns          /     7.6 ns 
   1048576 :   20.5 ns          /    28.5 ns 
   2097152 :   39.8 ns          /    41.8 ns 
   4194304 :   40.1 ns          /    43.6 ns 
   8388608 :   59.7 ns          /    38.7 ns 
  16777216 :   94.9 ns          /    65.0 ns 
  33554432 :  151.8 ns          /   172.0 ns 
  67108864 :  181.6 ns          /   225.5 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.1 ns          /     1.9 ns 
    262144 :    1.9 ns          /     2.8 ns 
    524288 :    2.2 ns          /     3.3 ns 
   1048576 :   16.9 ns          /    26.8 ns 
   2097152 :   31.5 ns          /    33.6 ns 
   4194304 :   35.9 ns          /    35.3 ns 
   8388608 :   51.1 ns          /    36.1 ns 
  16777216 :   41.0 ns          /    48.5 ns 
  33554432 :  109.5 ns          /   153.5 ns 
  67108864 :  154.6 ns          /   198.9 ns 

Executing benchmark on cpu7 (Cortex-A720):

tinymembench v0.4.9-nuumio (simple benchmark for memory throughput and latency)

CFLAGS: 
bandwidth test min repeats (-b): 2
bandwidth test max repeats (-B): 3
bandwidth test mem realloc (-M): no      (-m for realloc)
      latency test repeats (-l): 3
        latency test count (-c): 1000000

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Test result is the best of repeated runs. Number of repeats  ==
==         is shown in brackets                                         ==
== Note 3: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 4: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 5: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                 :  13630.7 MB/s (3, 2.8%)
 C copy backwards (32 byte blocks)                :  13838.5 MB/s (2)
 C copy backwards (64 byte blocks)                :  13779.1 MB/s (3, 0.4%)
 C copy                                           :  17068.9 MB/s (2)
 C copy prefetched (32 bytes step)                :  16194.5 MB/s (2)
 C copy prefetched (64 bytes step)                :  15861.5 MB/s (3, 0.1%)
 C 2-pass copy                                    :  11578.0 MB/s (2)
 C 2-pass copy prefetched (32 bytes step)         :  11601.3 MB/s (3, 0.3%)
 C 2-pass copy prefetched (64 bytes step)         :  12687.1 MB/s (2)
 C scan 8                                         :   2197.2 MB/s (3, 12.3%)
 C scan 16                                        :   4387.6 MB/s (3, 11.7%)
 C scan 32                                        :   7949.9 MB/s (3, 8.1%)
 C scan 64                                        :  14018.2 MB/s (3, 2.4%)
 C fill                                           :  35145.6 MB/s (3, 6.5%)
 C fill (shuffle within 16 byte blocks)           :  35153.8 MB/s (2)
 C fill (shuffle within 32 byte blocks)           :  35173.1 MB/s (2)
 C fill (shuffle within 64 byte blocks)           :  35174.6 MB/s (2)
 ---
 libc memcpy copy                                 :  16877.3 MB/s (2)
 libc memchr scan                                 :  22912.1 MB/s (2)
 libc memset fill                                 :  47083.7 MB/s (3, 16.6%)
 ---
 NEON LDP/STP copy                                :  17224.7 MB/s (3, 1.6%)
 NEON LDP/STP copy pldl2strm (32 bytes step)      :  14979.1 MB/s (2)
 NEON LDP/STP copy pldl2strm (64 bytes step)      :  15685.0 MB/s (3, 0.2%)
 NEON LDP/STP copy pldl1keep (32 bytes step)      :  16732.3 MB/s (3, 0.2%)
 NEON LDP/STP copy pldl1keep (64 bytes step)      :  16985.4 MB/s (3, 0.5%)
 NEON LD1/ST1 copy                                :  16975.9 MB/s (2)
 NEON LDP load                                    :  22617.7 MB/s (3, 0.4%)
 NEON LDNP load                                   :  23456.4 MB/s (3, 0.1%)
 NEON STP fill                                    :  46681.5 MB/s (3, 0.2%)
 NEON STNP fill                                   :  46482.6 MB/s (3, 0.4%)
 ARM LDP/STP copy                                 :  18035.1 MB/s (3, 0.5%)
 ARM LDP load                                     :  23156.2 MB/s (2)
 ARM LDNP load                                    :  22971.3 MB/s (2)
 ARM STP fill                                     :  46416.9 MB/s (2)
 ARM STNP fill                                    :  46262.3 MB/s (3, 0.1%)

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)             :   2219.4 MB/s (2)
 NEON LDP/STP 2-pass copy (from framebuffer)      :   1505.9 MB/s (2)
 NEON LD1/ST1 copy (from framebuffer)             :   2218.5 MB/s (2)
 NEON LD1/ST1 2-pass copy (from framebuffer)      :   1500.7 MB/s (2)
 ARM LDP/STP copy (from framebuffer)              :   1885.2 MB/s (2)
 ARM LDP/STP 2-pass copy (from framebuffer)       :   1168.9 MB/s (2)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     2.0 ns 
    262144 :    1.9 ns          /     2.9 ns 
    524288 :    5.7 ns          /     9.9 ns 
   1048576 :   20.7 ns          /    28.7 ns 
   2097152 :   39.1 ns          /    42.4 ns 
   4194304 :   40.0 ns          /    44.4 ns 
   8388608 :   60.0 ns          /    40.0 ns 
  16777216 :   93.5 ns          /    64.4 ns 
  33554432 :  150.2 ns          /   170.3 ns 
  67108864 :  180.6 ns          /   223.7 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.2 ns          /     1.9 ns 
    262144 :    1.9 ns          /     2.9 ns 
    524288 :    2.3 ns          /     3.4 ns 
   1048576 :   18.0 ns          /    27.2 ns 
   2097152 :   32.2 ns          /    33.9 ns 
   4194304 :   36.1 ns          /    35.7 ns 
   8388608 :   51.2 ns          /    36.4 ns 
  16777216 :   40.9 ns          /    47.5 ns 
  33554432 :  107.9 ns          /   150.2 ns 
  67108864 :  153.3 ns          /   196.8 ns 

Executing benchmark on cpu9 (Cortex-A720):

tinymembench v0.4.9-nuumio (simple benchmark for memory throughput and latency)

CFLAGS: 
bandwidth test min repeats (-b): 2
bandwidth test max repeats (-B): 3
bandwidth test mem realloc (-M): no      (-m for realloc)
      latency test repeats (-l): 3
        latency test count (-c): 1000000

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Test result is the best of repeated runs. Number of repeats  ==
==         is shown in brackets                                         ==
== Note 3: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 4: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 5: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                 :  14573.3 MB/s (3, 1.0%)
 C copy backwards (32 byte blocks)                :  15135.0 MB/s (3, 0.3%)
 C copy backwards (64 byte blocks)                :  15138.9 MB/s (2)
 C copy                                           :  15928.1 MB/s (3, 0.5%)
 C copy prefetched (32 bytes step)                :  14927.6 MB/s (3, 0.7%)
 C copy prefetched (64 bytes step)                :  14644.2 MB/s (3, 0.3%)
 C 2-pass copy                                    :  13549.4 MB/s (2)
 C 2-pass copy prefetched (32 bytes step)         :  13086.8 MB/s (3, 0.8%)
 C 2-pass copy prefetched (64 bytes step)         :  15463.7 MB/s (2)
 C scan 8                                         :   2496.4 MB/s (3, 11.1%)
 C scan 16                                        :   4288.4 MB/s (3, 5.5%)
 C scan 32                                        :   8330.8 MB/s (3, 4.0%)
 C scan 64                                        :  15915.0 MB/s (2)
 C fill                                           :  39972.6 MB/s (2)
 C fill (shuffle within 16 byte blocks)           :  39972.6 MB/s (2)
 C fill (shuffle within 32 byte blocks)           :  39972.0 MB/s (2)
 C fill (shuffle within 64 byte blocks)           :  39973.6 MB/s (2)
 ---
 libc memcpy copy                                 :  15264.6 MB/s (3, 0.7%)
 libc memchr scan                                 :  25926.8 MB/s (2)
 libc memset fill                                 :  48033.5 MB/s (3, 1.1%)
 ---
 NEON LDP/STP copy                                :  17747.0 MB/s (3, 2.5%)
 NEON LDP/STP copy pldl2strm (32 bytes step)      :  15243.6 MB/s (2)
 NEON LDP/STP copy pldl2strm (64 bytes step)      :  15937.0 MB/s (3, 0.4%)
 NEON LDP/STP copy pldl1keep (32 bytes step)      :  16873.4 MB/s (3, 0.3%)
 NEON LDP/STP copy pldl1keep (64 bytes step)      :  17107.1 MB/s (3, 0.4%)
 NEON LD1/ST1 copy                                :  17070.7 MB/s (2)
 NEON LDP load                                    :  23389.2 MB/s (3, 0.9%)
 NEON LDNP load                                   :  23648.6 MB/s (2)
 NEON STP fill                                    :  47479.6 MB/s (3, 1.2%)
 NEON STNP fill                                   :  46839.2 MB/s (3, 1.9%)
 ARM LDP/STP copy                                 :  17079.8 MB/s (3, 1.1%)
 ARM LDP load                                     :  23800.3 MB/s (3, 0.5%)
 ARM LDNP load                                    :  23702.8 MB/s (3, 0.1%)
 ARM STP fill                                     :  47336.0 MB/s (3, 1.2%)
 ARM STNP fill                                    :  46763.3 MB/s (3, 1.7%)

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)             :   2265.8 MB/s (2)
 NEON LDP/STP 2-pass copy (from framebuffer)      :   1537.6 MB/s (2)
 NEON LD1/ST1 copy (from framebuffer)             :   2264.9 MB/s (2)
 NEON LD1/ST1 2-pass copy (from framebuffer)      :   1530.2 MB/s (2)
 ARM LDP/STP copy (from framebuffer)              :   1934.8 MB/s (2)
 ARM LDP/STP 2-pass copy (from framebuffer)       :   1199.3 MB/s (2)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.1 ns          /     1.6 ns 
    262144 :    1.8 ns          /     2.2 ns 
    524288 :    5.6 ns          /    10.0 ns 
   1048576 :   19.6 ns          /    27.5 ns 
   2097152 :   35.5 ns          /    37.5 ns 
   4194304 :   36.4 ns          /    39.3 ns 
   8388608 :   57.1 ns          /    38.4 ns 
  16777216 :   92.1 ns          /    62.5 ns 
  33554432 :  145.1 ns          /   162.8 ns 
  67108864 :  172.1 ns          /   216.2 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.0 ns          /     1.6 ns 
    262144 :    1.7 ns          /     2.2 ns 
    524288 :    2.2 ns          /     2.5 ns 
   1048576 :   16.8 ns          /    26.2 ns 
   2097152 :   31.4 ns          /    33.0 ns 
   4194304 :   35.5 ns          /    34.6 ns 
   8388608 :   50.2 ns          /    35.4 ns 
  16777216 :   59.9 ns          /    50.9 ns 
  33554432 :  106.3 ns          /   149.1 ns 
  67108864 :  150.3 ns          /   194.6 ns 

Executing benchmark on cpu11 (Cortex-A720):

tinymembench v0.4.9-nuumio (simple benchmark for memory throughput and latency)

CFLAGS: 
bandwidth test min repeats (-b): 2
bandwidth test max repeats (-B): 3
bandwidth test mem realloc (-M): no      (-m for realloc)
      latency test repeats (-l): 3
        latency test count (-c): 1000000

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Test result is the best of repeated runs. Number of repeats  ==
==         is shown in brackets                                         ==
== Note 3: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 4: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 5: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                 :  14167.3 MB/s (3, 0.9%)
 C copy backwards (32 byte blocks)                :  14663.4 MB/s (3, 0.4%)
 C copy backwards (64 byte blocks)                :  14630.2 MB/s (2)
 C copy                                           :  14636.7 MB/s (3, 0.5%)
 C copy prefetched (32 bytes step)                :  13774.8 MB/s (2)
 C copy prefetched (64 bytes step)                :  13923.2 MB/s (3, 0.6%)
 C 2-pass copy                                    :  14074.4 MB/s (2)
 C 2-pass copy prefetched (32 bytes step)         :  13576.1 MB/s (2)
 C 2-pass copy prefetched (64 bytes step)         :  14754.7 MB/s (3, 0.3%)
 C scan 8                                         :   2596.2 MB/s (3, 11.8%)
 C scan 16                                        :   4183.4 MB/s (3, 2.0%)
 C scan 32                                        :   8210.8 MB/s (3, 1.0%)
 C scan 64                                        :  16569.4 MB/s (2)
 C fill                                           :  38592.8 MB/s (3, 3.1%)
 C fill (shuffle within 16 byte blocks)           :  38547.3 MB/s (2)
 C fill (shuffle within 32 byte blocks)           :  38596.0 MB/s (3)
 C fill (shuffle within 64 byte blocks)           :  38482.1 MB/s (2)
 ---
 libc memcpy copy                                 :  13525.7 MB/s (3, 1.3%)
 libc memchr scan                                 :  26331.8 MB/s (3, 0.4%)
 libc memset fill                                 :  39928.0 MB/s (3, 13.1%)
 ---
 NEON LDP/STP copy                                :  16173.9 MB/s (3, 1.2%)
 NEON LDP/STP copy pldl2strm (32 bytes step)      :  14516.7 MB/s (2)
 NEON LDP/STP copy pldl2strm (64 bytes step)      :  14815.8 MB/s (2)
 NEON LDP/STP copy pldl1keep (32 bytes step)      :  15367.2 MB/s (2)
 NEON LDP/STP copy pldl1keep (64 bytes step)      :  15619.5 MB/s (2)
 NEON LD1/ST1 copy                                :  15616.8 MB/s (3, 0.1%)
 NEON LDP load                                    :  23085.3 MB/s (3, 0.6%)
 NEON LDNP load                                   :  23425.1 MB/s (2)
 NEON STP fill                                    :  35864.8 MB/s (3, 3.3%)
 NEON STNP fill                                   :  34809.8 MB/s (3, 4.4%)
 ARM LDP/STP copy                                 :  15813.6 MB/s (3, 0.8%)
 ARM LDP load                                     :  23683.7 MB/s (3, 0.6%)
 ARM LDNP load                                    :  23476.6 MB/s (3)
 ARM STP fill                                     :  35911.8 MB/s (3, 3.5%)
 ARM STNP fill                                    :  34875.0 MB/s (3, 4.6%)

==========================================================================
== Framebuffer read tests.                                              ==
==                                                                      ==
== Many ARM devices use a part of the system memory as the framebuffer, ==
== typically mapped as uncached but with write-combining enabled.       ==
== Writes to such framebuffers are quite fast, but reads are much       ==
== slower and very sensitive to the alignment and the selection of      ==
== CPU instructions which are used for accessing memory.                ==
==                                                                      ==
== Many x86 systems allocate the framebuffer in the GPU memory,         ==
== accessible for the CPU via a relatively slow PCI-E bus. Moreover,    ==
== PCI-E is asymmetric and handles reads a lot worse than writes.       ==
==                                                                      ==
== If uncached framebuffer reads are reasonably fast (at least 100 MB/s ==
== or preferably >300 MB/s), then using the shadow framebuffer layer    ==
== is not necessary in Xorg DDX drivers, resulting in a nice overall    ==
== performance improvement. For example, the xf86-video-fbturbo DDX     ==
== uses this trick.                                                     ==
==========================================================================

 NEON LDP/STP copy (from framebuffer)             :   2242.9 MB/s (3, 0.1%)
 NEON LDP/STP 2-pass copy (from framebuffer)      :   1523.9 MB/s (2)
 NEON LD1/ST1 copy (from framebuffer)             :   2242.5 MB/s (2)
 NEON LD1/ST1 2-pass copy (from framebuffer)      :   1516.2 MB/s (2)
 ARM LDP/STP copy (from framebuffer)              :   1918.5 MB/s (2)
 ARM LDP/STP 2-pass copy (from framebuffer)       :   1188.3 MB/s (2)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read, [MADV_NOHUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.0 ns          /     1.5 ns 
    262144 :    2.2 ns          /     2.8 ns 
    524288 :    5.3 ns          /     8.2 ns 
   1048576 :   20.6 ns          /    28.5 ns 
   2097152 :   33.6 ns          /    34.4 ns 
   4194304 :   34.7 ns          /    37.1 ns 
   8388608 :   56.7 ns          /    39.0 ns 
  16777216 :   97.0 ns          /    66.9 ns 
  33554432 :  149.9 ns          /   167.5 ns 
  67108864 :  180.2 ns          /   222.5 ns 

block size : single random read / dual random read, [MADV_HUGEPAGE]
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    1.0 ns          /     1.5 ns 
    262144 :    1.7 ns          /     2.1 ns 
    524288 :    2.1 ns          /     2.4 ns 
   1048576 :   16.4 ns          /    25.9 ns 
   2097152 :   30.7 ns          /    32.7 ns 
   4194304 :   35.0 ns          /    34.4 ns 
   8388608 :   55.0 ns          /    35.1 ns 
  16777216 :   40.2 ns          /    47.4 ns 
  33554432 :  108.4 ns          /   152.8 ns 
  67108864 :  152.5 ns          /   199.2 ns 

##########################################################################

Executing ramlat on cpu0 (Cortex-A720), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 1.544 1.540 1.540 1.540 1.539 1.539 1.539 2.845 
         8k: 1.539 1.539 1.539 1.539 1.539 1.539 1.539 2.903 
        16k: 1.539 1.539 1.539 1.539 1.539 1.539 1.539 2.907 
        32k: 1.539 1.539 1.539 1.539 1.539 1.539 1.539 2.909 
        64k: 1.541 1.540 1.541 1.540 1.541 1.540 1.541 2.910 
       128k: 3.726 3.739 3.727 3.737 3.727 4.542 6.400 16.93 
       256k: 3.524 3.539 3.524 3.542 3.524 4.562 8.659 17.29 
       512k: 3.483 3.477 3.478 3.477 3.481 4.563 8.674 17.32 
      1024k: 12.12 13.86 12.39 14.00 12.35 16.47 23.87 36.12 
      2048k: 13.07 14.76 12.99 14.77 12.78 19.87 21.88 38.38 
      4096k: 20.89 17.66 20.78 17.75 20.94 23.48 22.50 35.73 
      8192k: 27.95 22.94 28.15 22.99 28.34 26.29 26.67 37.36 
     16384k: 36.88 26.52 35.63 26.50 35.14 29.51 31.97 44.90 
     32768k: 42.28 44.42 41.89 44.60 41.99 51.14 72.44 115.2 
     65536k: 51.02 62.06 51.83 60.81 50.93 59.57 89.34 135.0 
    131072k: 50.57 73.38 50.24 73.80 50.34 67.51 99.35 139.5 

Executing ramlat on cpu1 (Cortex-A520), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 2.786 2.784 2.784 2.784 2.227 2.227 2.229 2.338 
         8k: 2.784 2.784 2.784 2.784 2.227 2.227 2.488 2.629 
        16k: 2.786 2.786 2.786 2.786 2.230 2.230 2.883 4.131 
        32k: 2.804 3.580 2.804 3.581 2.246 2.245 3.625 17.54 
        64k: 33.35 39.13 33.33 39.13 31.51 37.22 44.62 63.65 
       128k: 38.42 46.34 38.30 46.64 35.25 41.73 47.45 69.81 
       256k: 38.91 46.55 38.97 46.57 37.21 44.06 49.19 72.08 
       512k: 39.01 39.32 38.96 39.45 38.25 39.20 47.33 72.77 
      1024k: 39.17 39.13 38.88 39.09 38.57 39.31 48.28 73.09 
      2048k: 39.48 39.13 38.93 39.44 38.92 38.87 46.84 73.77 
      4096k: 41.37 40.67 40.24 40.68 39.82 39.87 47.38 71.92 
      8192k: 42.70 40.74 40.30 40.73 40.11 40.02 46.97 71.25 
     16384k: 59.50 56.44 54.05 56.21 53.60 55.75 66.05 94.07 
     32768k: 182.9 189.5 183.3 196.3 183.4 197.8 211.8 288.1 
     65536k: 209.1 221.3 218.3 221.7 218.4 221.6 232.3 318.2 
    131072k: 218.7 222.3 220.7 222.7 220.0 222.1 234.8 325.1 

Executing ramlat on cpu5 (Cortex-A720), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 1.740 1.741 1.740 1.740 1.740 1.740 1.740 3.310 
         8k: 1.740 1.740 1.740 1.740 1.740 1.740 1.740 3.380 
        16k: 1.740 1.740 1.740 1.740 1.740 1.740 1.740 3.383 
        32k: 1.740 1.740 1.740 1.740 1.740 1.740 1.741 3.384 
        64k: 1.742 1.741 1.742 1.741 1.742 1.742 1.742 3.386 
       128k: 4.212 4.999 4.213 4.999 4.213 5.231 7.434 19.33 
       256k: 3.984 5.089 3.985 5.092 3.983 5.274 9.895 19.59 
       512k: 3.934 5.061 3.930 5.061 3.935 5.282 9.910 19.61 
      1024k: 12.46 14.03 12.37 13.95 12.61 16.12 23.53 34.00 
      2048k: 12.67 14.86 12.95 14.81 12.81 19.02 22.12 37.84 
      4096k: 20.31 17.81 20.23 17.78 20.13 22.13 21.72 35.32 
      8192k: 27.26 22.52 28.32 22.57 27.52 25.04 26.71 36.40 
     16384k: 36.14 25.94 34.29 25.74 33.61 28.69 32.27 43.48 
     32768k: 42.12 42.72 42.55 42.63 42.36 51.90 70.18 114.6 
     65536k: 50.00 60.28 49.45 58.98 49.80 57.71 89.42 133.2 
    131072k: 49.71 73.37 48.91 73.30 50.02 67.09 99.19 138.0 

Executing ramlat on cpu7 (Cortex-A720), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 1.820 1.819 1.819 1.819 1.819 1.819 1.819 3.461 
         8k: 1.819 1.819 1.819 1.819 1.819 1.819 1.819 3.534 
        16k: 1.819 1.819 1.819 1.819 1.819 1.819 1.819 3.537 
        32k: 1.819 1.819 1.819 1.819 1.819 1.819 1.820 3.538 
        64k: 1.821 1.820 1.821 1.820 1.821 1.821 1.821 3.541 
       128k: 4.403 5.227 4.405 5.226 4.404 5.467 7.772 20.21 
       256k: 4.166 5.322 4.165 5.318 4.165 5.514 10.34 20.48 
       512k: 4.109 5.290 4.112 5.291 4.109 5.522 10.36 20.50 
      1024k: 12.61 14.29 12.78 14.13 12.81 16.27 24.29 34.56 
      2048k: 13.06 14.91 13.02 15.04 12.94 18.51 22.71 38.34 
      4096k: 20.39 17.96 20.55 18.03 20.30 21.94 22.31 35.83 
      8192k: 27.05 22.60 26.86 22.60 26.93 25.07 27.07 36.88 
     16384k: 36.82 25.39 32.57 25.32 33.23 29.23 30.68 43.06 
     32768k: 42.91 42.44 41.29 42.23 41.17 49.88 69.63 116.5 
     65536k: 49.31 60.26 48.66 58.12 49.27 57.33 89.48 133.6 
    131072k: 49.53 73.46 49.75 72.16 48.80 64.93 98.31 136.8 

Executing ramlat on cpu9 (Cortex-A720), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 1.601 1.601 1.601 1.601 1.601 1.601 1.601 2.956 
         8k: 1.601 1.601 1.601 1.601 1.601 1.601 1.601 3.019 
        16k: 1.601 1.601 1.601 1.601 1.601 1.601 1.601 3.023 
        32k: 1.601 1.601 1.601 1.601 1.601 1.601 1.601 3.025 
        64k: 1.602 1.601 1.602 1.601 1.602 1.602 1.602 3.026 
       128k: 3.875 3.887 3.876 3.887 3.876 4.723 6.654 17.61 
       256k: 3.666 3.671 3.664 3.671 3.664 4.744 9.004 17.98 
       512k: 3.617 3.615 3.616 3.614 3.615 4.743 9.019 18.01 
      1024k: 12.05 13.50 12.18 13.39 11.93 15.74 22.84 33.57 
      2048k: 12.40 14.09 12.36 13.99 12.46 18.44 21.30 36.57 
      4096k: 19.79 16.91 19.66 16.91 19.49 21.56 21.44 33.96 
      8192k: 26.66 21.86 26.62 21.75 26.32 24.52 25.31 35.38 
     16384k: 34.91 25.00 33.97 24.80 34.59 27.66 30.54 41.51 
     32768k: 41.30 42.29 41.02 42.47 41.44 50.29 69.23 112.1 
     65536k: 50.53 59.89 50.83 60.51 50.69 60.49 86.21 131.7 
    131072k: 48.61 70.06 49.51 67.60 49.83 62.42 98.66 137.3 

Executing ramlat on cpu11 (Cortex-A720), results in ns:

       size:  1x32  2x32  1x64  2x64 1xPTR 2xPTR 4xPTR 8xPTR
         4k: 1.539 1.540 1.539 1.539 1.539 1.539 1.539 2.843 
         8k: 1.540 1.539 1.539 1.540 1.539 1.539 1.539 2.903 
        16k: 1.539 1.539 1.540 1.539 1.540 1.539 1.539 2.906 
        32k: 1.539 1.539 1.543 1.540 1.540 1.539 1.540 2.908 
        64k: 1.540 1.540 1.540 1.540 1.540 1.541 1.540 2.910 
       128k: 3.724 3.759 3.725 3.751 3.727 4.542 6.398 16.94 
       256k: 3.524 3.544 3.524 3.540 3.524 4.563 8.657 17.29 
       512k: 3.479 3.478 3.477 3.478 3.476 4.562 8.672 17.31 
      1024k: 11.55 13.33 11.68 13.34 11.80 15.60 22.79 33.30 
      2048k: 11.99 13.90 12.16 13.91 12.16 18.34 20.53 36.12 
      4096k: 19.63 16.78 19.22 16.85 19.52 21.69 21.17 33.59 
      8192k: 26.60 21.68 26.59 21.78 26.46 24.44 25.74 35.03 
     16384k: 33.12 25.20 32.83 25.18 33.04 27.55 30.31 41.83 
     32768k: 41.80 43.55 41.85 43.14 41.89 50.44 70.08 113.9 
     65536k: 50.24 61.97 50.23 60.87 50.30 59.56 87.23 133.0 
    131072k: 49.58 71.82 49.01 70.11 48.89 63.16 94.08 136.9 

##########################################################################

Executing benchmark on each cluster individually

OpenSSL 3.0.15, built on 3 Sep 2024 (Library: OpenSSL 3.0.15 3 Sep 2024)
type             16 bytes     64 bytes    256 bytes   1024 bytes   8192 bytes  16384 bytes
aes-256-cbc     865957.21k  1308442.84k  1430646.19k  1451713.54k  1457979.39k  1458416.30k (Cortex-A720)
aes-256-cbc     171534.68k   344544.45k   463446.53k   506069.67k   520238.42k   520989.35k (Cortex-A520)
aes-256-cbc     766195.43k  1157482.97k  1265519.45k  1284163.24k  1289710.25k  1290114.39k (Cortex-A720)
aes-256-cbc     732927.11k  1107198.83k  1210558.12k  1228357.63k  1233704.28k  1234097.49k (Cortex-A720)
aes-256-cbc     832910.49k  1258453.10k  1375737.17k  1395967.32k  1402008.92k  1402475.86k (Cortex-A720)
aes-256-cbc     866090.98k  1308479.81k  1430496.43k  1451707.73k  1457785.51k  1458410.84k (Cortex-A720)

##########################################################################

Executing benchmark single-threaded on cpu0 (Cortex-A720)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,12 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:   15222 MB,  # CPU hardware threads:  12
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       4279   100   4171   4164  |      48770   100   4168   4164
23:       3547   100   3620   3614  |      48122   100   4170   4166
24:       3079   100   3317   3311  |      47384   100   4165   4160
25:       2842   100   3250   3245  |      46581   100   4151   4146
----------------------------------  | ------------------------------
Avr:             100   3590   3583  |              100   4164   4159
Tot:             100   3877   3871

Executing benchmark single-threaded on cpu1 (Cortex-A520)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,12 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:   15222 MB,  # CPU hardware threads:  12
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       1371   100   1339   1334  |      24898   100   2133   2126
23:       1243   100   1272   1267  |      24667   100   2142   2135
24:       1161   100   1254   1249  |      24417   100   2151   2144
25:       1105   100   1268   1263  |      24118   100   2154   2147
----------------------------------  | ------------------------------
Avr:             100   1283   1278  |              100   2145   2138
Tot:             100   1714   1708

Executing benchmark single-threaded on cpu5 (Cortex-A720)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,12 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:   15222 MB,  # CPU hardware threads:  12
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       4208   100   4100   4094  |      42944   100   3670   3667
23:       3375   100   3445   3439  |      42456   100   3679   3675
24:       2951   100   3179   3174  |      41899   100   3683   3678
25:       2744   100   3138   3133  |      41207   100   3672   3668
----------------------------------  | ------------------------------
Avr:             100   3465   3460  |              100   3676   3672
Tot:             100   3571   3566

Executing benchmark single-threaded on cpu7 (Cortex-A720)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,12 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:   15222 MB,  # CPU hardware threads:  12
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       4108   100   4002   3997  |      41143   100   3516   3513
23:       3338   100   3406   3401  |      40640   100   3521   3518
24:       2930   100   3155   3151  |      40105   100   3525   3521
25:       2716   100   3106   3101  |      39520   100   3521   3518
----------------------------------  | ------------------------------
Avr:             100   3417   3412  |              100   3521   3517
Tot:             100   3469   3465

Executing benchmark single-threaded on cpu9 (Cortex-A720)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,12 CPUs LE)

LE
CPU Freq: - 64000000 - - - - - - -

RAM size:   15222 MB,  # CPU hardware threads:  12
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       4526   100   4409   4404  |      47104   100   4025   4022
23:       3591   100   3664   3659  |      46576   100   4035   4032
24:       3120   100   3360   3355  |      45940   100   4037   4033
25:       2890   100   3305   3300  |      45145   100   4022   4018
----------------------------------  | ------------------------------
Avr:             100   3684   3680  |              100   4030   4026
Tot:             100   3857   3853

Executing benchmark single-threaded on cpu11 (Cortex-A720)

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,12 CPUs LE)

LE
CPU Freq: - - - - - - - - 2048000000

RAM size:   15222 MB,  # CPU hardware threads:  12
RAM usage:    435 MB,  # Benchmark threads:      1

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:       4589   100   4471   4464  |      48901   100   4179   4175
23:       3623   100   3698   3692  |      48354   100   4190   4186
24:       3147   100   3389   3384  |      47664   100   4190   4184
25:       2900   100   3316   3311  |      46788   100   4170   4164
----------------------------------  | ------------------------------
Avr:             100   3719   3713  |              100   4182   4177
Tot:             100   3950   3945

##########################################################################

Executing benchmark 3 times multi-threaded on CPUs 0-11

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,12 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:   15222 MB,  # CPU hardware threads:  12
RAM usage:   2647 MB,  # Benchmark threads:     12

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:      34491  1131   2967  33554  |     375658   958   3346  32040
23:      32087  1102   2966  32693  |     369706   969   3300  31987
24:      31776  1142   2991  34166  |     358881   972   3240  31499
25:      31277  1171   3050  35711  |     345117   965   3181  30714
----------------------------------  | ------------------------------
Avr:            1137   2994  34031  |              966   3267  31560
Tot:            1051   3130  32796

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,12 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:   15222 MB,  # CPU hardware threads:  12
RAM usage:   2647 MB,  # Benchmark threads:     12

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:      34648  1118   3015  33706  |     375021   956   3346  31986
23:      32355  1115   2956  32966  |     365488   959   3299  31622
24:      31259  1120   3001  33610  |     355177   962   3241  31174
25:      30029  1113   3082  34286  |     348004   973   3182  30971
----------------------------------  | ------------------------------
Avr:            1116   3014  33642  |              963   3267  31438
Tot:            1039   3140  32540

7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,12 CPUs LE)

LE
CPU Freq: - - - - - - - - -

RAM size:   15222 MB,  # CPU hardware threads:  12
RAM usage:   2647 MB,  # Benchmark threads:     12

                       Compressing  |                  Decompressing
Dict     Speed Usage    R/U Rating  |      Speed Usage    R/U Rating
         KiB/s     %   MIPS   MIPS  |      KiB/s     %   MIPS   MIPS

22:      35012  1138   2994  34060  |     376279   959   3347  32093
23:      32279  1107   2971  32889  |     366367   960   3301  31698
24:      31841  1150   2978  34236  |     358965   972   3240  31506
25:      30349  1128   3072  34651  |     345657   967   3182  30762
----------------------------------  | ------------------------------
Avr:            1131   3004  33959  |              965   3267  31515
Tot:            1048   3135  32737

Compression: 34031,33642,33959
Decompression: 31560,31438,31515
Total: 32796,32540,32737

##########################################################################

** cpuminer-multi 1.3.7 by tpruvot@github **
BTC donation address: 1FhDPLPpw18X4srecguG3MxJYe4a1JsZnd (tpruvot)

[2025-01-30 18:07:01] 12 miner threads started, using 'scrypt' algorithm.
[2025-01-30 18:07:01] CPU #0: 4.79 kH/s
[2025-01-30 18:07:01] CPU #11: 4.83 kH/s
[2025-01-30 18:07:01] CPU #9: 4.65 kH/s
[2025-01-30 18:07:01] CPU #10: 4.64 kH/s
[2025-01-30 18:07:01] CPU #6: 4.26 kH/s
[2025-01-30 18:07:01] CPU #5: 4.26 kH/s
[2025-01-30 18:07:01] CPU #7: 4.08 kH/s
[2025-01-30 18:07:01] CPU #8: 4.07 kH/s
[2025-01-30 18:07:01] CPU #1: 1.83 kH/s
[2025-01-30 18:07:01] CPU #4: 1.83 kH/s
[2025-01-30 18:07:01] CPU #2: 1.74 kH/s
[2025-01-30 18:07:01] CPU #3: 1.56 kH/s
[2025-01-30 18:07:06] Total: 42.70 kH/s
[2025-01-30 18:07:11] CPU #2: 1.84 kH/s
[2025-01-30 18:07:11] CPU #0: 4.79 kH/s
[2025-01-30 18:07:11] CPU #8: 4.08 kH/s
[2025-01-30 18:07:11] CPU #11: 4.81 kH/s
[2025-01-30 18:07:11] Total: 42.79 kH/s
[2025-01-30 18:07:11] CPU #5: 4.26 kH/s
[2025-01-30 18:07:11] CPU #10: 4.63 kH/s
[2025-01-30 18:07:11] CPU #6: 4.26 kH/s
[2025-01-30 18:07:11] CPU #7: 4.08 kH/s
[2025-01-30 18:07:11] CPU #9: 4.63 kH/s
[2025-01-30 18:07:11] CPU #4: 1.84 kH/s
[2025-01-30 18:07:11] CPU #1: 1.83 kH/s
[2025-01-30 18:07:11] CPU #3: 1.82 kH/s
[2025-01-30 18:07:16] Total: 42.91 kH/s
[2025-01-30 18:07:21] CPU #2: 1.84 kH/s
[2025-01-30 18:07:21] CPU #0: 4.83 kH/s
[2025-01-30 18:07:21] CPU #11: 4.83 kH/s
[2025-01-30 18:07:21] Total: 43.00 kH/s
[2025-01-30 18:07:21] CPU #9: 4.65 kH/s
[2025-01-30 18:07:21] CPU #10: 4.65 kH/s
[2025-01-30 18:07:21] CPU #6: 4.27 kH/s
[2025-01-30 18:07:21] CPU #5: 4.27 kH/s
[2025-01-30 18:07:21] CPU #8: 4.08 kH/s
[2025-01-30 18:07:21] CPU #7: 4.08 kH/s
[2025-01-30 18:07:21] CPU #1: 1.84 kH/s
[2025-01-30 18:07:21] CPU #4: 1.84 kH/s
[2025-01-30 18:07:21] CPU #3: 1.82 kH/s
[2025-01-30 18:07:26] Total: 43.00 kH/s
[2025-01-30 18:07:31] CPU #2: 1.84 kH/s
[2025-01-30 18:07:31] CPU #0: 4.83 kH/s
[2025-01-30 18:07:31] CPU #11: 4.83 kH/s
[2025-01-30 18:07:31] Total: 43.00 kH/s
[2025-01-30 18:07:31] CPU #10: 4.65 kH/s
[2025-01-30 18:07:31] CPU #9: 4.65 kH/s
[2025-01-30 18:07:31] CPU #5: 4.27 kH/s
[2025-01-30 18:07:31] CPU #6: 4.27 kH/s
[2025-01-30 18:07:31] CPU #8: 4.08 kH/s
[2025-01-30 18:07:31] CPU #7: 4.08 kH/s
[2025-01-30 18:07:31] CPU #1: 1.84 kH/s
[2025-01-30 18:07:31] CPU #4: 1.84 kH/s
[2025-01-30 18:07:31] CPU #3: 1.82 kH/s
[2025-01-30 18:07:36] Total: 43.00 kH/s
[2025-01-30 18:07:41] CPU #2: 1.84 kH/s
[2025-01-30 18:07:41] CPU #0: 4.83 kH/s
[2025-01-30 18:07:41] CPU #11: 4.83 kH/s
[2025-01-30 18:07:41] Total: 43.00 kH/s
[2025-01-30 18:07:41] CPU #9: 4.65 kH/s
[2025-01-30 18:07:41] CPU #10: 4.65 kH/s
[2025-01-30 18:07:41] CPU #5: 4.27 kH/s
[2025-01-30 18:07:41] CPU #6: 4.27 kH/s
[2025-01-30 18:07:41] CPU #8: 4.08 kH/s
[2025-01-30 18:07:41] CPU #7: 4.08 kH/s
[2025-01-30 18:07:41] CPU #1: 1.84 kH/s
[2025-01-30 18:07:41] CPU #4: 1.84 kH/s
[2025-01-30 18:07:41] CPU #3: 1.82 kH/s
[2025-01-30 18:07:46] Total: 43.00 kH/s
[2025-01-30 18:07:51] CPU #2: 1.84 kH/s
[2025-01-30 18:07:51] CPU #0: 4.83 kH/s
[2025-01-30 18:07:51] CPU #9: 4.65 kH/s
[2025-01-30 18:07:51] CPU #10: 4.65 kH/s
[2025-01-30 18:07:51] CPU #11: 4.83 kH/s
[2025-01-30 18:07:51] Total: 43.00 kH/s
[2025-01-30 18:07:51] CPU #5: 4.27 kH/s
[2025-01-30 18:07:51] CPU #6: 4.27 kH/s
[2025-01-30 18:07:51] CPU #8: 4.08 kH/s
[2025-01-30 18:07:51] CPU #7: 4.08 kH/s
[2025-01-30 18:07:51] CPU #1: 1.84 kH/s
[2025-01-30 18:07:51] CPU #4: 1.84 kH/s
[2025-01-30 18:07:51] CPU #3: 1.82 kH/s
[2025-01-30 18:07:56] Total: 43.00 kH/s
[2025-01-30 18:08:01] CPU #2: 1.84 kH/s
[2025-01-30 18:08:01] CPU #0: 4.83 kH/s
[2025-01-30 18:08:01] CPU #5: 4.27 kH/s
[2025-01-30 18:08:01] CPU #6: 4.27 kH/s
[2025-01-30 18:08:01] CPU #8: 4.08 kH/s
[2025-01-30 18:08:01] CPU #10: 4.65 kH/s
[2025-01-30 18:08:01] CPU #11: 4.83 kH/s
[2025-01-30 18:08:01] Total: 42.98 kH/s
[2025-01-30 18:08:01] CPU #9: 4.65 kH/s
[2025-01-30 18:08:01] CPU #7: 4.08 kH/s
[2025-01-30 18:08:01] CPU #1: 1.83 kH/s
[2025-01-30 18:08:01] CPU #4: 1.84 kH/s
[2025-01-30 18:08:01] CPU #3: 1.82 kH/s
[2025-01-30 18:08:06] Total: 42.99 kH/s
[2025-01-30 18:08:11] CPU #2: 1.84 kH/s
[2025-01-30 18:08:11] CPU #0: 4.83 kH/s
[2025-01-30 18:08:11] CPU #5: 4.27 kH/s
[2025-01-30 18:08:11] CPU #10: 4.65 kH/s
[2025-01-30 18:08:11] CPU #6: 4.27 kH/s
[2025-01-30 18:08:11] CPU #9: 4.65 kH/s
[2025-01-30 18:08:11] CPU #8: 4.08 kH/s
[2025-01-30 18:08:11] CPU #11: 4.83 kH/s
[2025-01-30 18:08:11] Total: 42.99 kH/s
[2025-01-30 18:08:11] CPU #7: 4.08 kH/s
[2025-01-30 18:08:11] CPU #1: 1.84 kH/s
[2025-01-30 18:08:11] CPU #4: 1.84 kH/s
[2025-01-30 18:08:11] CPU #3: 1.82 kH/s
[2025-01-30 18:08:16] Total: 43.00 kH/s
[2025-01-30 18:08:21] CPU #2: 1.84 kH/s
[2025-01-30 18:08:21] CPU #0: 4.83 kH/s
[2025-01-30 18:08:21] CPU #5: 4.27 kH/s
[2025-01-30 18:08:21] CPU #8: 4.08 kH/s
[2025-01-30 18:08:21] CPU #10: 4.65 kH/s
[2025-01-30 18:08:21] CPU #6: 4.27 kH/s
[2025-01-30 18:08:21] CPU #9: 4.65 kH/s
[2025-01-30 18:08:21] CPU #11: 4.83 kH/s
[2025-01-30 18:08:21] Total: 43.00 kH/s
[2025-01-30 18:08:21] CPU #7: 4.08 kH/s
[2025-01-30 18:08:21] CPU #1: 1.84 kH/s
[2025-01-30 18:08:21] CPU #4: 1.84 kH/s
[2025-01-30 18:08:21] CPU #3: 1.82 kH/s
[2025-01-30 18:08:26] Total: 43.00 kH/s
[2025-01-30 18:08:31] CPU #2: 1.84 kH/s
[2025-01-30 18:08:31] CPU #0: 4.83 kH/s
[2025-01-30 18:08:31] CPU #5: 4.27 kH/s
[2025-01-30 18:08:31] CPU #8: 4.08 kH/s
[2025-01-30 18:08:31] CPU #6: 4.27 kH/s
[2025-01-30 18:08:31] CPU #10: 4.65 kH/s
[2025-01-30 18:08:31] CPU #9: 4.65 kH/s
[2025-01-30 18:08:31] CPU #11: 4.83 kH/s
[2025-01-30 18:08:31] Total: 43.00 kH/s
[2025-01-30 18:08:31] CPU #7: 4.08 kH/s
[2025-01-30 18:08:31] CPU #1: 1.84 kH/s
[2025-01-30 18:08:31] CPU #4: 1.84 kH/s
[2025-01-30 18:08:31] CPU #3: 1.82 kH/s
[2025-01-30 18:08:36] Total: 43.00 kH/s
[2025-01-30 18:08:41] CPU #2: 1.84 kH/s
[2025-01-30 18:08:41] CPU #0: 4.83 kH/s
[2025-01-30 18:08:41] CPU #5: 4.27 kH/s
[2025-01-30 18:08:41] CPU #8: 4.08 kH/s
[2025-01-30 18:08:41] CPU #6: 4.27 kH/s
[2025-01-30 18:08:41] CPU #9: 4.65 kH/s
[2025-01-30 18:08:41] CPU #10: 4.65 kH/s
[2025-01-30 18:08:41] CPU #7: 4.08 kH/s
[2025-01-30 18:08:41] CPU #11: 4.83 kH/s
[2025-01-30 18:08:41] Total: 42.99 kH/s
[2025-01-30 18:08:41] CPU #1: 1.84 kH/s
[2025-01-30 18:08:41] CPU #4: 1.84 kH/s
[2025-01-30 18:08:41] CPU #3: 1.82 kH/s
[2025-01-30 18:08:46] Total: 42.98 kH/s
[2025-01-30 18:08:51] CPU #2: 1.84 kH/s
[2025-01-30 18:08:51] CPU #0: 4.83 kH/s
[2025-01-30 18:08:51] CPU #5: 4.27 kH/s
[2025-01-30 18:08:51] CPU #8: 4.08 kH/s
[2025-01-30 18:08:51] CPU #10: 4.65 kH/s
[2025-01-30 18:08:51] CPU #6: 4.27 kH/s
[2025-01-30 18:08:51] CPU #9: 4.65 kH/s
[2025-01-30 18:08:51] CPU #7: 4.08 kH/s
[2025-01-30 18:08:51] CPU #11: 4.83 kH/s
[2025-01-30 18:08:51] Total: 43.00 kH/s
[2025-01-30 18:08:51] CPU #1: 1.84 kH/s
[2025-01-30 18:08:51] CPU #4: 1.84 kH/s
[2025-01-30 18:08:51] CPU #3: 1.82 kH/s
[2025-01-30 18:08:56] Total: 43.00 kH/s
[2025-01-30 18:09:01] CPU #2: 1.84 kH/s
[2025-01-30 18:09:01] CPU #0: 4.83 kH/s
[2025-01-30 18:09:01] CPU #5: 4.27 kH/s
[2025-01-30 18:09:01] CPU #8: 4.08 kH/s
[2025-01-30 18:09:01] CPU #6: 4.27 kH/s
[2025-01-30 18:09:01] CPU #10: 4.65 kH/s
[2025-01-30 18:09:01] CPU #9: 4.65 kH/s
[2025-01-30 18:09:01] CPU #7: 4.08 kH/s
[2025-01-30 18:09:01] CPU #11: 4.83 kH/s
[2025-01-30 18:09:01] Total: 43.00 kH/s
[2025-01-30 18:09:01] CPU #4: 1.84 kH/s
[2025-01-30 18:09:01] CPU #1: 1.84 kH/s
[2025-01-30 18:09:01] CPU #3: 1.82 kH/s
[2025-01-30 18:09:06] Total: 43.00 kH/s
[2025-01-30 18:09:11] CPU #2: 1.84 kH/s
[2025-01-30 18:09:11] CPU #0: 4.83 kH/s
[2025-01-30 18:09:11] CPU #5: 4.27 kH/s
[2025-01-30 18:09:11] CPU #8: 4.08 kH/s
[2025-01-30 18:09:11] CPU #6: 4.27 kH/s
[2025-01-30 18:09:11] CPU #10: 4.65 kH/s
[2025-01-30 18:09:11] CPU #9: 4.65 kH/s
[2025-01-30 18:09:11] CPU #7: 4.08 kH/s
[2025-01-30 18:09:11] CPU #11: 4.83 kH/s
[2025-01-30 18:09:11] Total: 42.99 kH/s
[2025-01-30 18:09:11] CPU #4: 1.84 kH/s
[2025-01-30 18:09:11] CPU #1: 1.84 kH/s
[2025-01-30 18:09:11] CPU #3: 1.82 kH/s
[2025-01-30 18:09:16] Total: 43.00 kH/s
[2025-01-30 18:09:21] CPU #2: 1.84 kH/s
[2025-01-30 18:09:21] CPU #0: 4.83 kH/s
[2025-01-30 18:09:21] CPU #5: 4.27 kH/s
[2025-01-30 18:09:21] CPU #8: 4.08 kH/s
[2025-01-30 18:09:21] CPU #6: 4.27 kH/s
[2025-01-30 18:09:21] CPU #10: 4.65 kH/s
[2025-01-30 18:09:21] CPU #9: 4.65 kH/s
[2025-01-30 18:09:21] CPU #7: 4.08 kH/s
[2025-01-30 18:09:21] CPU #11: 4.83 kH/s
[2025-01-30 18:09:21] Total: 43.00 kH/s
[2025-01-30 18:09:21] CPU #4: 1.84 kH/s
[2025-01-30 18:09:21] CPU #1: 1.84 kH/s
[2025-01-30 18:09:21] CPU #3: 1.82 kH/s
[2025-01-30 18:09:26] Total: 43.00 kH/s
[2025-01-30 18:09:31] CPU #2: 1.84 kH/s
[2025-01-30 18:09:31] CPU #0: 4.82 kH/s
[2025-01-30 18:09:31] CPU #5: 4.27 kH/s
[2025-01-30 18:09:31] CPU #8: 4.08 kH/s
[2025-01-30 18:09:31] CPU #6: 4.27 kH/s
[2025-01-30 18:09:31] CPU #9: 4.65 kH/s
[2025-01-30 18:09:31] CPU #10: 4.64 kH/s
[2025-01-30 18:09:31] CPU #7: 4.08 kH/s
[2025-01-30 18:09:31] CPU #11: 4.83 kH/s
[2025-01-30 18:09:31] Total: 42.98 kH/s
[2025-01-30 18:09:31] CPU #4: 1.84 kH/s
[2025-01-30 18:09:31] CPU #1: 1.84 kH/s
[2025-01-30 18:09:31] CPU #3: 1.82 kH/s
[2025-01-30 18:09:36] Total: 43.00 kH/s
[2025-01-30 18:09:41] CPU #2: 1.84 kH/s
[2025-01-30 18:09:41] CPU #0: 4.83 kH/s
[2025-01-30 18:09:41] CPU #5: 4.27 kH/s
[2025-01-30 18:09:41] CPU #8: 4.08 kH/s
[2025-01-30 18:09:41] CPU #6: 4.27 kH/s
[2025-01-30 18:09:41] CPU #10: 4.65 kH/s
[2025-01-30 18:09:41] CPU #9: 4.65 kH/s
[2025-01-30 18:09:41] CPU #7: 4.08 kH/s
[2025-01-30 18:09:41] CPU #11: 4.83 kH/s
[2025-01-30 18:09:41] Total: 43.00 kH/s
[2025-01-30 18:09:41] CPU #4: 1.84 kH/s
[2025-01-30 18:09:41] CPU #1: 1.84 kH/s
[2025-01-30 18:09:41] CPU #3: 1.82 kH/s
[2025-01-30 18:09:46] Total: 43.00 kH/s
[2025-01-30 18:09:51] CPU #2: 1.84 kH/s
[2025-01-30 18:09:51] CPU #0: 4.83 kH/s
[2025-01-30 18:09:51] CPU #5: 4.27 kH/s
[2025-01-30 18:09:51] CPU #8: 4.08 kH/s
[2025-01-30 18:09:51] CPU #6: 4.27 kH/s
[2025-01-30 18:09:51] CPU #10: 4.65 kH/s
[2025-01-30 18:09:51] CPU #9: 4.65 kH/s
[2025-01-30 18:09:51] CPU #7: 4.08 kH/s
[2025-01-30 18:09:51] CPU #11: 4.83 kH/s
[2025-01-30 18:09:51] Total: 43.00 kH/s
[2025-01-30 18:09:51] CPU #4: 1.84 kH/s
[2025-01-30 18:09:51] CPU #1: 1.84 kH/s
[2025-01-30 18:09:51] CPU #3: 1.82 kH/s
[2025-01-30 18:09:56] Total: 43.00 kH/s
[2025-01-30 18:10:01] CPU #2: 1.84 kH/s
[2025-01-30 18:10:01] CPU #0: 4.83 kH/s
[2025-01-30 18:10:01] CPU #5: 4.27 kH/s
[2025-01-30 18:10:01] CPU #8: 4.08 kH/s
[2025-01-30 18:10:01] CPU #6: 4.27 kH/s
[2025-01-30 18:10:01] CPU #10: 4.65 kH/s
[2025-01-30 18:10:01] CPU #9: 4.65 kH/s
[2025-01-30 18:10:01] CPU #7: 4.08 kH/s
[2025-01-30 18:10:01] CPU #11: 4.83 kH/s
[2025-01-30 18:10:01] Total: 43.00 kH/s
[2025-01-30 18:10:01] CPU #4: 1.84 kH/s
[2025-01-30 18:10:01] CPU #1: 1.84 kH/s
[2025-01-30 18:10:01] CPU #3: 1.82 kH/s
[2025-01-30 18:10:06] Total: 43.00 kH/s
[2025-01-30 18:10:11] CPU #2: 1.84 kH/s
[2025-01-30 18:10:11] CPU #0: 4.83 kH/s
[2025-01-30 18:10:11] CPU #5: 4.27 kH/s
[2025-01-30 18:10:11] CPU #8: 4.08 kH/s
[2025-01-30 18:10:11] CPU #6: 4.27 kH/s
[2025-01-30 18:10:11] CPU #10: 4.65 kH/s
[2025-01-30 18:10:11] CPU #9: 4.65 kH/s
[2025-01-30 18:10:11] CPU #7: 4.08 kH/s
[2025-01-30 18:10:11] CPU #11: 4.83 kH/s
[2025-01-30 18:10:11] Total: 42.99 kH/s
[2025-01-30 18:10:11] CPU #4: 1.84 kH/s
[2025-01-30 18:10:11] CPU #1: 1.84 kH/s
[2025-01-30 18:10:11] CPU #3: 1.82 kH/s
[2025-01-30 18:10:16] Total: 42.98 kH/s
[2025-01-30 18:10:21] CPU #2: 1.84 kH/s
[2025-01-30 18:10:21] CPU #0: 4.83 kH/s
[2025-01-30 18:10:21] CPU #5: 4.27 kH/s
[2025-01-30 18:10:21] CPU #8: 4.08 kH/s
[2025-01-30 18:10:21] CPU #6: 4.27 kH/s
[2025-01-30 18:10:21] CPU #10: 4.65 kH/s
[2025-01-30 18:10:21] CPU #9: 4.65 kH/s
[2025-01-30 18:10:21] CPU #7: 4.08 kH/s
[2025-01-30 18:10:21] CPU #11: 4.83 kH/s
[2025-01-30 18:10:21] Total: 43.00 kH/s
[2025-01-30 18:10:21] CPU #4: 1.84 kH/s
[2025-01-30 18:10:21] CPU #1: 1.84 kH/s
[2025-01-30 18:10:21] CPU #3: 1.82 kH/s
[2025-01-30 18:10:26] Total: 43.00 kH/s
[2025-01-30 18:10:31] CPU #2: 1.84 kH/s
[2025-01-30 18:10:31] CPU #0: 4.83 kH/s
[2025-01-30 18:10:31] CPU #5: 4.27 kH/s
[2025-01-30 18:10:31] CPU #8: 4.08 kH/s
[2025-01-30 18:10:31] CPU #6: 4.27 kH/s
[2025-01-30 18:10:31] CPU #10: 4.65 kH/s
[2025-01-30 18:10:31] CPU #9: 4.65 kH/s
[2025-01-30 18:10:31] CPU #7: 4.08 kH/s
[2025-01-30 18:10:31] CPU #11: 4.83 kH/s
[2025-01-30 18:10:31] Total: 43.00 kH/s
[2025-01-30 18:10:31] CPU #4: 1.84 kH/s
[2025-01-30 18:10:31] CPU #1: 1.84 kH/s
[2025-01-30 18:10:31] CPU #3: 1.82 kH/s
[2025-01-30 18:10:36] Total: 43.00 kH/s
[2025-01-30 18:10:41] CPU #2: 1.84 kH/s
[2025-01-30 18:10:41] CPU #0: 4.83 kH/s
[2025-01-30 18:10:41] CPU #5: 4.27 kH/s
[2025-01-30 18:10:41] CPU #8: 4.08 kH/s
[2025-01-30 18:10:41] CPU #6: 4.27 kH/s
[2025-01-30 18:10:41] CPU #10: 4.65 kH/s
[2025-01-30 18:10:41] CPU #9: 4.65 kH/s
[2025-01-30 18:10:41] CPU #7: 4.08 kH/s
[2025-01-30 18:10:41] CPU #11: 4.83 kH/s
[2025-01-30 18:10:41] Total: 43.00 kH/s
[2025-01-30 18:10:41] CPU #4: 1.84 kH/s
[2025-01-30 18:10:41] CPU #1: 1.84 kH/s
[2025-01-30 18:10:41] CPU #3: 1.82 kH/s
[2025-01-30 18:10:46] Total: 43.00 kH/s
[2025-01-30 18:10:51] CPU #2: 1.84 kH/s
[2025-01-30 18:10:51] CPU #0: 4.83 kH/s
[2025-01-30 18:10:51] CPU #5: 4.27 kH/s
[2025-01-30 18:10:51] CPU #8: 4.08 kH/s
[2025-01-30 18:10:51] CPU #6: 4.27 kH/s
[2025-01-30 18:10:51] CPU #10: 4.65 kH/s
[2025-01-30 18:10:51] CPU #9: 4.65 kH/s
[2025-01-30 18:10:51] CPU #7: 4.08 kH/s
[2025-01-30 18:10:51] CPU #11: 4.83 kH/s
[2025-01-30 18:10:51] Total: 43.01 kH/s
[2025-01-30 18:10:51] CPU #4: 1.84 kH/s
[2025-01-30 18:10:51] CPU #1: 1.84 kH/s
[2025-01-30 18:10:51] CPU #3: 1.84 kH/s
[2025-01-30 18:10:56] Total: 43.02 kH/s
[2025-01-30 18:11:01] CPU #2: 1.84 kH/s
[2025-01-30 18:11:01] CPU #0: 4.82 kH/s
[2025-01-30 18:11:01] CPU #5: 4.27 kH/s
[2025-01-30 18:11:01] CPU #8: 4.08 kH/s
[2025-01-30 18:11:01] CPU #6: 4.27 kH/s
[2025-01-30 18:11:01] CPU #10: 4.65 kH/s
[2025-01-30 18:11:01] CPU #7: 4.08 kH/s
[2025-01-30 18:11:01] CPU #9: 4.65 kH/s
[2025-01-30 18:11:01] CPU #11: 4.83 kH/s
[2025-01-30 18:11:01] Total: 43.00 kH/s
[2025-01-30 18:11:01] CPU #4: 1.84 kH/s
[2025-01-30 18:11:01] CPU #1: 1.84 kH/s
[2025-01-30 18:11:01] CPU #3: 1.84 kH/s
[2025-01-30 18:11:06] Total: 43.01 kH/s
[2025-01-30 18:11:11] CPU #2: 1.84 kH/s
[2025-01-30 18:11:11] CPU #0: 4.83 kH/s
[2025-01-30 18:11:11] CPU #5: 4.27 kH/s
[2025-01-30 18:11:11] CPU #8: 4.08 kH/s
[2025-01-30 18:11:11] CPU #6: 4.27 kH/s
[2025-01-30 18:11:11] CPU #10: 4.65 kH/s
[2025-01-30 18:11:11] CPU #7: 4.08 kH/s
[2025-01-30 18:11:11] CPU #9: 4.65 kH/s
[2025-01-30 18:11:11] CPU #11: 4.83 kH/s
[2025-01-30 18:11:11] Total: 43.01 kH/s
[2025-01-30 18:11:11] CPU #4: 1.84 kH/s
[2025-01-30 18:11:11] CPU #1: 1.84 kH/s
[2025-01-30 18:11:11] CPU #3: 1.84 kH/s
[2025-01-30 18:11:16] Total: 43.02 kH/s
[2025-01-30 18:11:21] CPU #2: 1.84 kH/s
[2025-01-30 18:11:21] CPU #0: 4.83 kH/s
[2025-01-30 18:11:21] CPU #5: 4.27 kH/s
[2025-01-30 18:11:21] CPU #8: 4.08 kH/s
[2025-01-30 18:11:21] CPU #6: 4.27 kH/s
[2025-01-30 18:11:21] CPU #10: 4.65 kH/s
[2025-01-30 18:11:21] CPU #7: 4.08 kH/s
[2025-01-30 18:11:21] CPU #9: 4.65 kH/s
[2025-01-30 18:11:21] CPU #11: 4.83 kH/s
[2025-01-30 18:11:21] Total: 43.02 kH/s
[2025-01-30 18:11:21] CPU #4: 1.84 kH/s
[2025-01-30 18:11:21] CPU #1: 1.84 kH/s
[2025-01-30 18:11:21] CPU #3: 1.84 kH/s
[2025-01-30 18:11:26] Total: 43.02 kH/s
[2025-01-30 18:11:31] CPU #2: 1.84 kH/s
[2025-01-30 18:11:31] CPU #0: 4.83 kH/s
[2025-01-30 18:11:31] CPU #5: 4.27 kH/s
[2025-01-30 18:11:31] CPU #8: 4.08 kH/s
[2025-01-30 18:11:31] CPU #6: 4.27 kH/s
[2025-01-30 18:11:31] CPU #10: 4.65 kH/s
[2025-01-30 18:11:31] CPU #7: 4.08 kH/s
[2025-01-30 18:11:31] CPU #9: 4.65 kH/s
[2025-01-30 18:11:31] CPU #11: 4.83 kH/s
[2025-01-30 18:11:31] Total: 43.02 kH/s
[2025-01-30 18:11:31] CPU #4: 1.84 kH/s
[2025-01-30 18:11:31] CPU #1: 1.84 kH/s
[2025-01-30 18:11:31] CPU #3: 1.84 kH/s
[2025-01-30 18:11:36] Total: 43.02 kH/s
[2025-01-30 18:11:41] CPU #2: 1.84 kH/s
[2025-01-30 18:11:41] CPU #0: 4.83 kH/s
[2025-01-30 18:11:41] CPU #5: 4.27 kH/s
[2025-01-30 18:11:41] CPU #8: 4.08 kH/s
[2025-01-30 18:11:41] CPU #6: 4.27 kH/s
[2025-01-30 18:11:41] CPU #10: 4.65 kH/s
[2025-01-30 18:11:41] CPU #7: 4.08 kH/s
[2025-01-30 18:11:41] CPU #9: 4.65 kH/s
[2025-01-30 18:11:41] CPU #11: 4.83 kH/s
[2025-01-30 18:11:41] Total: 43.02 kH/s
[2025-01-30 18:11:41] CPU #4: 1.84 kH/s
[2025-01-30 18:11:41] CPU #1: 1.84 kH/s
[2025-01-30 18:11:41] CPU #3: 1.84 kH/s
[2025-01-30 18:11:46] Total: 43.00 kH/s
[2025-01-30 18:11:51] CPU #2: 1.84 kH/s
[2025-01-30 18:11:51] CPU #0: 4.83 kH/s
[2025-01-30 18:11:51] CPU #5: 4.27 kH/s
[2025-01-30 18:11:51] CPU #8: 4.08 kH/s
[2025-01-30 18:11:51] CPU #6: 4.27 kH/s
[2025-01-30 18:11:51] CPU #10: 4.65 kH/s
[2025-01-30 18:11:51] CPU #7: 4.08 kH/s
[2025-01-30 18:11:51] CPU #9: 4.65 kH/s
[2025-01-30 18:11:51] CPU #11: 4.83 kH/s
[2025-01-30 18:11:51] Total: 43.01 kH/s
[2025-01-30 18:11:51] CPU #4: 1.84 kH/s
[2025-01-30 18:11:51] CPU #1: 1.84 kH/s
[2025-01-30 18:11:51] CPU #3: 1.84 kH/s
[2025-01-30 18:11:56] Total: 43.02 kH/s
[2025-01-30 18:12:01] CPU #2: 1.84 kH/s
[2025-01-30 18:12:01] CPU #0: 4.83 kH/s
[2025-01-30 18:12:01] CPU #5: 4.27 kH/s
[2025-01-30 18:12:01] CPU #8: 4.08 kH/s

Total Scores: 43.02,43.01,43.00,42.99,42.98,42.91

##########################################################################

Testing maximum cpufreq again, still under full load. System health now:

Time       cpu0/cpu1/cpu5/cpu7/cpu9    load %cpu %sys %usr %nice %io %irq   Temp
18:11:47: 2600/1800/2300/2200/2500MHz 16.05 100%   0%  99%   0%   0%   0%  46.0°C  

Checking cpufreq OPP for cpu0 (Cortex-A720):

Cpufreq OPP: 2600    Measured: 2598 (2598.723/2598.690/2598.690)

Checking cpufreq OPP for cpu1-cpu4 (Cortex-A520):

Cpufreq OPP: 1799    Measured: 1796 (1796.618/1796.595/1796.595)

Checking cpufreq OPP for cpu5-cpu6 (Cortex-A720):

Cpufreq OPP: 2299    Measured: 2298 (2299.011/2299.011/2298.954)

Checking cpufreq OPP for cpu7-cpu8 (Cortex-A720):

Cpufreq OPP: 2199    Measured: 2199 (2199.033/2199.033/2199.033)

Checking cpufreq OPP for cpu9-cpu10 (Cortex-A720):

Cpufreq OPP: 2499    Measured: 2499 (2499.035/2499.004/2499.004)

Checking cpufreq OPP for cpu11 (Cortex-A720):

No cpufreq support. Measured: 2598 MHz (2598.997/2598.933/2598.933)

##########################################################################

Hardware sensors:

scmi_sensors-virtual-0
VPU:          +32.0 C  
GPU_btm:      +32.0 C  
GPU_top:      +32.0 C  
SOC_BRC:      +34.0 C  
DDR_btm:      +33.0 C  
DDR_top:      +34.0 C  
CI:           +32.0 C  
NPU:          +32.0 C  
CPU_M1:       +32.0 C  
CPU_B1:       +35.0 C  
CPU_M0:       +33.0 C  
CPU_B0:       +33.0 C  
SOC_TRC:      +35.0 C  
PCB_HOT:      +34.0 C  
PCB_AMB:      +31.0 C  

/dev/nvme0:	34°C
/dev/sda:	24°C

##########################################################################

Thermal source: /sys/devices/virtual/thermal/thermal_zone9/ (thermal-zone4)

System health while running tinymembench:

Time       cpu0/cpu1/cpu5/cpu7/cpu9    load %cpu %sys %usr %nice %io %irq   Temp
17:47:30: 2600/1800/2300/2200/2500MHz  4.95  17%   0%  17%   0%   0%   0%  32.0°C  
17:48:30: 2600/1800/2300/2200/2500MHz  4.98   8%   0%   8%   0%   0%   0%  34.0°C  
17:49:30: 2600/1800/2300/2200/2500MHz  5.00   8%   0%   8%   0%   0%   0%  30.0°C  
17:50:30: 2600/1800/2300/2200/2500MHz  5.00   8%   0%   8%   0%   0%   0%  31.0°C  
17:51:30: 2600/1800/2300/2200/2500MHz  5.00   8%   0%   8%   0%   0%   0%  30.0°C  
17:52:30: 2600/1800/2300/2200/2500MHz  5.00   8%   0%   8%   0%   0%   0%  32.0°C  
17:53:30: 2600/1800/2300/2200/2500MHz  5.00   8%   0%   8%   0%   0%   0%  32.0°C  
17:54:31: 2600/1800/2300/2200/2500MHz  5.04   8%   0%   8%   0%   0%   0%  36.0°C  
17:55:31: 2600/1800/2300/2200/2500MHz  5.04   8%   0%   8%   0%   0%   0%  32.0°C  

System health while running ramlat:

Time       cpu0/cpu1/cpu5/cpu7/cpu9    load %cpu %sys %usr %nice %io %irq   Temp
17:55:34: 2600/1800/2300/2200/2500MHz  5.04  16%   0%  15%   0%   0%   0%  32.0°C  
17:55:52: 2600/1800/2300/2200/2500MHz  5.03   8%   0%   8%   0%   0%   0%  32.0°C  
17:56:10: 2600/1800/2300/2200/2500MHz  5.02   8%   0%   8%   0%   0%   0%  30.0°C  
17:56:29: 2600/1800/2300/2200/2500MHz  5.01   8%   0%   8%   0%   0%   0%  29.0°C  
17:56:47: 2600/1800/2300/2200/2500MHz  5.01   8%   0%   8%   0%   0%   0%  30.0°C  
17:57:05: 2600/1800/2300/2200/2500MHz  5.01   8%   0%   8%   0%   0%   0%  29.0°C  
17:57:23: 2600/1800/2300/2200/2500MHz  5.00   8%   0%   8%   0%   0%   0%  29.0°C  
17:57:41: 2600/1800/2300/2200/2500MHz  5.00   8%   0%   8%   0%   0%   0%  29.0°C  
17:57:59: 2600/1800/2300/2200/2500MHz  5.00   8%   0%   8%   0%   0%   0%  31.0°C  
17:58:17: 2600/1800/2300/2200/2500MHz  5.07   8%   0%   8%   0%   0%   0%  32.0°C  

System health while running OpenSSL benchmark:

Time       cpu0/cpu1/cpu5/cpu7/cpu9    load %cpu %sys %usr %nice %io %irq   Temp
17:58:26: 2600/1800/2300/2200/2500MHz  5.07  15%   0%  15%   0%   0%   0%  35.0°C  
17:58:42: 2600/1800/2300/2200/2500MHz  5.05   8%   0%   8%   0%   0%   0%  32.0°C  
17:58:58: 2600/1800/2300/2200/2500MHz  5.04   8%   0%   8%   0%   0%   0%  29.0°C  
17:59:14: 2600/1800/2300/2200/2500MHz  5.10   8%   0%   8%   0%   0%   0%  29.0°C  
17:59:31: 2600/1800/2300/2200/2500MHz  5.15   8%   0%   8%   0%   0%   0%  29.0°C  
17:59:47: 2600/1800/2300/2200/2500MHz  5.12   8%   0%   8%   0%   0%   0%  29.0°C  
18:00:03: 2600/1800/2300/2200/2500MHz  5.08   8%   0%   8%   0%   0%   0%  32.0°C  

System health while running 7-zip single core benchmark:

Time       cpu0/cpu1/cpu5/cpu7/cpu9    load %cpu %sys %usr %nice %io %irq   Temp
18:00:14: 2600/1800/2300/2200/2500MHz  5.07  15%   0%  15%   0%   0%   0%  32.0°C  
18:00:27: 2600/1800/2300/2200/2500MHz  5.05   8%   0%   8%   0%   0%   0%  32.0°C  
18:00:41: 2600/1800/2300/2200/2500MHz  5.05   8%   0%   8%   0%   0%   0%  31.0°C  
18:00:54: 2600/1800/2300/2200/2500MHz  5.04   8%   0%   8%   0%   0%   0%  29.0°C  
18:01:07: 2600/1800/2300/2200/2500MHz  5.03   8%   0%   8%   0%   0%   0%  29.0°C  
18:01:20: 2600/1800/2300/2200/2500MHz  5.02   8%   0%   8%   0%   0%   0%  29.0°C  
18:01:33: 2600/1800/2300/2200/2500MHz  5.02   8%   0%   8%   0%   0%   0%  29.0°C  
18:01:46: 2600/1800/2300/2200/2500MHz  5.01   8%   0%   8%   0%   0%   0%  29.0°C  
18:01:59: 2600/1800/2300/2200/2500MHz  5.08   8%   0%   8%   0%   0%   0%  29.0°C  
18:02:12: 2600/1800/2300/2200/2500MHz  5.07   8%   0%   8%   0%   0%   0%  28.0°C  
18:02:25: 2600/1800/2300/2200/2500MHz  5.05   8%   0%   8%   0%   0%   0%  29.0°C  
18:02:38: 2600/1800/2300/2200/2500MHz  5.12   8%   0%   8%   0%   0%   0%  29.0°C  
18:02:51: 2600/1800/2300/2200/2500MHz  5.10   8%   0%   8%   0%   0%   0%  29.0°C  
18:03:05: 2600/1800/2300/2200/2500MHz  5.08   8%   0%   8%   0%   0%   0%  29.0°C  
18:03:18: 2600/1800/2300/2200/2500MHz  5.06   8%   0%   8%   0%   0%   0%  29.0°C  
18:03:31: 2600/1800/2300/2200/2500MHz  5.13   8%   0%   8%   0%   0%   0%  29.0°C  
18:03:44: 2600/1800/2300/2200/2500MHz  5.10   8%   0%   8%   0%   0%   0%  29.0°C  
18:03:57: 2600/1800/2300/2200/2500MHz  5.08   8%   0%   8%   0%   0%   0%  29.0°C  
18:04:10: 2600/1800/2300/2200/2500MHz  5.06   8%   0%   8%   0%   0%   0%  29.0°C  
18:04:23: 2600/1800/2300/2200/2500MHz  5.05   8%   0%   8%   0%   0%   0%  30.0°C  
18:04:36: 2600/1800/2300/2200/2500MHz  5.04   8%   0%   8%   0%   0%   0%  36.0°C  
18:04:49: 2600/1800/2300/2200/2500MHz  5.03   8%   0%   8%   0%   0%   0%  33.0°C  

System health while running 7-zip multi core benchmark:

Time       cpu0/cpu1/cpu5/cpu7/cpu9    load %cpu %sys %usr %nice %io %irq   Temp
18:05:03: 2600/1800/2300/2200/2500MHz  6.06  15%   0%  14%   0%   0%   0%  38.0°C  
18:05:16: 2600/1800/2300/2200/2500MHz  7.59  85%   0%  85%   0%   0%   0%  45.0°C  
18:05:26: 2600/1800/2300/2200/2500MHz  8.30  83%   0%  82%   0%   0%   0%  46.0°C  
18:05:39: 2600/1800/2300/2200/2500MHz  9.29  81%   0%  81%   0%   0%   0%  46.0°C  
18:05:49: 2600/1800/2300/2200/2500MHz 10.92  79%   0%  79%   0%   0%   0%  46.0°C  
18:05:59: 2600/1800/2300/2200/2500MHz 11.06  75%   0%  74%   0%   0%   0%  45.0°C  
18:06:10: 2600/1800/2300/2200/2500MHz 11.18  77%   0%  76%   0%   0%   0%  46.0°C  
18:06:20: 2600/1800/2300/2200/2500MHz 12.14  93%   0%  93%   0%   0%   0%  47.0°C  
18:06:30: 2600/1800/2300/2200/2500MHz 12.80  75%   0%  74%   0%   0%   0%  36.0°C  
18:06:40: 2600/1800/2300/2200/2500MHz 12.65  80%   0%  80%   0%   0%   0%  44.0°C  
18:06:50: 2600/1800/2300/2200/2500MHz 12.37  78%   0%  77%   0%   0%   0%  46.0°C  
18:07:00: 2600/1800/2300/2200/2500MHz 13.15  89%   0%  88%   0%   0%   0%  48.0°C  

System health while running cpuminer:

Time       cpu0/cpu1/cpu5/cpu7/cpu9    load %cpu %sys %usr %nice %io %irq   Temp
18:07:14: 2600/1800/2300/2200/2500MHz 13.82  17%   0%  16%   0%   0%   0%  46.0°C  
18:07:59: 2600/1800/2300/2200/2500MHz 15.01  99%   0%  99%   0%   0%   0%  46.0°C  
18:08:45: 2600/1800/2300/2200/2500MHz 15.58 100%   0%  99%   0%   0%   0%  46.0°C  
18:09:30: 2600/1800/2300/2200/2500MHz 15.84 100%   0%  99%   0%   0%   0%  46.0°C  
18:10:16: 2600/1800/2300/2200/2500MHz 15.97 100%   0%  99%   0%   0%   0%  46.0°C  
18:11:01: 2600/1800/2300/2200/2500MHz 16.03 100%   0%  99%   0%   0%   0%  46.0°C  
18:11:47: 2600/1800/2300/2200/2500MHz 16.05 100%   0%  99%   0%   0%   0%  46.0°C  

##########################################################################

Linux 6.1.44-cix-build-generic (orion-o6) 	01/30/25 	_aarch64_	(12 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          22.17    0.00    0.18    0.04    0.00   77.61

Device             tps    kB_read/s    kB_wrtn/s    kB_dscd/s    kB_read    kB_wrtn    kB_dscd
nvme0n1           0.04         0.95         0.00         0.00       4264          0          0
sda              10.57       129.18        49.87         0.00     582589     224924          0

               total        used        free      shared  buff/cache   available
Mem:            14Gi       1.2Gi        13Gi        10Mi       298Mi        13Gi
Swap:             0B          0B          0B

CPU sysfs topology (clusters, cpufreq members, clockspeeds)
                 cpufreq   min    max
 CPU    cluster  policy   speed  speed   core type
  0        0        0      800    2600   Cortex-A720 / r0p1
  1        0        1      800    1800   Cortex-A520 / r0p1
  2        0        1      800    1800   Cortex-A520 / r0p1
  3        0        1      800    1800   Cortex-A520 / r0p1
  4        0        1      800    1800   Cortex-A520 / r0p1
  5        0        5      800    2300   Cortex-A720 / r0p1
  6        0        5      800    2300   Cortex-A720 / r0p1
  7        0        7      800    2200   Cortex-A720 / r0p1
  8        0        7      800    2200   Cortex-A720 / r0p1
  9        0        9      800    2500   Cortex-A720 / r0p1
 10        0        9      800    2500   Cortex-A720 / r0p1
 11        0        9      800    2500   Cortex-A720 / r0p1

Architecture:                       aarch64
CPU op-mode(s):                     64-bit
Byte Order:                         Little Endian
CPU(s):                             12
On-line CPU(s) list:                0-11
Vendor ID:                          ARM
BIOS Vendor ID:                     Cix Technology Group Co., Ltd.
Model name:                         CIX P1 CD8180
BIOS Model name:                    CIX P1 CD8180   CPU @ 1.8GHz
BIOS CPU family:                    258
Model:                              1
Thread(s) per core:                 1
Core(s) per socket:                 1
Socket(s):                          1
Stepping:                           r0p1
CPU(s) scaling MHz:                 100%
CPU max MHz:                        2600.1731
CPU min MHz:                        799.8580
BogoMIPS:                           2000.00
Flags:                              fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma lrcpc dcpop sha3 sm3 sm4 asimddp sha512 sve asimdfhm dit uscat ilrcpc flagm ssbs sb paca pacg dcpodp sve2 sveaes svepmull svebitperm svesha3 svesm4 flagm2 frint svei8mm svebf16 i8mm bf16 dgh bti ecv afp wfxt
Model name:                         CIX P1 CD8180
BIOS Model name:                    CIX P1 CD8180   CPU @ 1.8GHz
BIOS CPU family:                    258
Model:                              1
Thread(s) per core:                 1
Core(s) per socket:                 4
Socket(s):                          1
Stepping:                           r0p1
CPU(s) scaling MHz:                 100%
CPU max MHz:                        1799.9980
CPU min MHz:                        799.9990
BogoMIPS:                           2000.00
Flags:                              fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma lrcpc dcpop sha3 sm3 sm4 asimddp sha512 sve asimdfhm dit uscat ilrcpc flagm ssbs sb paca pacg dcpodp sve2 sveaes svepmull svebitperm svesha3 svesm4 flagm2 frint svei8mm svebf16 i8mm bf16 dgh bti ecv afp wfxt
Model name:                         CIX P1 CD8180
BIOS Model name:                    CIX P1 CD8180   CPU @ 1.8GHz
BIOS CPU family:                    258
Model:                              1
Thread(s) per core:                 1
Core(s) per socket:                 7
Socket(s):                          1
Stepping:                           r0p1
CPU(s) scaling MHz:                 100%
CPU max MHz:                        2600.1731
CPU min MHz:                        799.8580
BogoMIPS:                           2000.00
Flags:                              fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma lrcpc dcpop sha3 sm3 sm4 asimddp sha512 sve asimdfhm dit uscat ilrcpc flagm ssbs sb paca pacg dcpodp sve2 sveaes svepmull svebitperm svesha3 svesm4 flagm2 frint svei8mm svebf16 i8mm bf16 dgh bti ecv afp wfxt
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-11
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Not affected
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Not affected
Vulnerability Retbleed:             Not affected
Vulnerability Spec rstack overflow: Not affected
Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:           Mitigation; __user pointer sanitization
Vulnerability Spectre v2:           Not affected
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Not affected

  cpuinfo: http://0x0.st/88AR.txt

Processor Information
	Socket Designation: CPU01
	Type: Central Processor
	Family: ARMv9
	Manufacturer: Cix Technology Group Co., Ltd.
	ID: 11 D8 0F 41 00 00 00 00
	Version: CIX P1 CD8180
	External Clock: 1000 MHz
	Max Speed: 2600 MHz
	Current Speed: 1800 MHz
	Status: Populated, Enabled
	L1 Cache Handle: 0x0001
	L2 Cache Handle: 0x0002
	L3 Cache Handle: 0x0003
	Asset Tag:  
	Part Number:  
	Core Count: 12
	Core Enabled: 12
	Thread Count: 12
	Characteristics:
		64-bit capable
		Multi-Core
		Execute Protection
		Power/Performance Control

SoC guess: Cix P1/CD8180
DT compat: radxa,orion-o6
 Compiler: /usr/bin/gcc (Debian 12.2.0-14) 12.2.0 / aarch64-linux-gnu
 Userland: arm64
   Kernel: 6.1.44-cix-build-generic/aarch64
           CONFIG_HZ=250
           CONFIG_HZ_250=y
           CONFIG_PREEMPTION=y
           CONFIG_PREEMPT=y
           CONFIG_PREEMPT_BUILD=y
           CONFIG_PREEMPT_COUNT=y
           CONFIG_PREEMPT_NOTIFIERS=y
           CONFIG_PREEMPT_RCU=y

##########################################################################

Kernel 6.1.44 is not latest 6.1.127 LTS that was released on 2025-01-23.

See https://endoflife.date/linux for details. It is somewhat likely that
a lot of exploitable vulnerabilities exist for this kernel as well as many
unfixed bugs.

##########################################################################

RAM configuration:
     *-bank:0
          description: Row of chips 5500 MHz (0.2 ns)
          physical id: 0
          slot: Top - on board
          size: 4GiB
          width: 32 bits
          clock: 1205MHz (0.8ns)
     *-bank:1
          description: Row of chips 5500 MHz (0.2 ns)
          physical id: 1
          slot: Top - on board
          size: 4GiB
          width: 32 bits
          clock: 1205MHz (0.8ns)
     *-bank:2
          description: Row of chips 5500 MHz (0.2 ns)
          physical id: 2
          slot: Top - on board
          size: 4GiB
          width: 32 bits
          clock: 1205MHz (0.8ns)
     *-bank:3
          description: Row of chips 5500 MHz (0.2 ns)
          physical id: 3
          slot: Top - on board
          size: 4GiB
          width: 32 bits
          clock: 1205MHz (0.8ns)

##########################################################################

Results validation:

  * Measured clockspeed not lower than advertised max CPU clockspeed
  * Background activity (%system) OK
  * No throttling

Status of performance related governors found below /sys (w/o cpufreq):

  * 14230000.vpu: performance / 1200 MHz (userspace performance simple_ondemand / 150 300 480 600 800 1200)
  * 14260000.aipu: performance / 1200 MHz (userspace performance simple_ondemand / 400 600 800 1200)
  * 15000000.gpu: performance / 900 MHz (userspace performance simple_ondemand / 72 216 350 600 800 900)

Status of performance related policies found below /sys:

  * /sys/devices/platform/soc@0/14260000.aipu/gm_policy: [1] AIPU GM is shared by tasks of all QoS level.
  * /sys/devices/platform/soc@0/15000000.gpu/power_policy: [coarse_demand] always_on
  * /sys/module/pcie_aspm/parameters/policy: default [performance] powersave powersupersave

##########################################################################

# Radxa Orion O6

Tested with sbc-bench v0.9.70 on Thu, 30 Jan 2025 18:12:30 +0800.

### General information:

The CPU features 6 clusters consisting of 2 different core types:

    Cix P1/CD8180, Kernel: aarch64, Userland: arm64
    
    CPU sysfs topology (clusters, cpufreq members, clockspeeds)
                     cpufreq   min    max
     CPU    cluster  policy   speed  speed   core type
      0        0        0      800    2600   Cortex-A720 / r0p1
      1        0        1      800    1800   Cortex-A520 / r0p1
      2        0        1      800    1800   Cortex-A520 / r0p1
      3        0        1      800    1800   Cortex-A520 / r0p1
      4        0        1      800    1800   Cortex-A520 / r0p1
      5        0        5      800    2300   Cortex-A720 / r0p1
      6        0        5      800    2300   Cortex-A720 / r0p1
      7        0        7      800    2200   Cortex-A720 / r0p1
      8        0        7      800    2200   Cortex-A720 / r0p1
      9        0        9      800    2500   Cortex-A720 / r0p1
     10        0        9      800    2500   Cortex-A720 / r0p1
     11        0        9      800    2500   Cortex-A720 / r0p1

15222 KB available RAM

### Governors/policies (performance vs. idle consumption):

Original governor settings:

    cpufreq-policy0: performance / 2600 MHz (userspace performance schedutil / 800 1200 1500 1800 2100 2300 2600)
    cpufreq-policy1: performance / 1800 MHz (userspace performance schedutil / 800 1800)
    cpufreq-policy5: performance / 2300 MHz (userspace performance schedutil / 800 1200 1500 1800 2000 2100 2300)
    cpufreq-policy7: performance / 2200 MHz (userspace performance schedutil / 800 1200 1500 1800 1900 2000 2200)
    cpufreq-policy9: performance / 2500 MHz (userspace performance schedutil / 800 1200 1500 1800 2000 2200 2500)
    14230000.vpu: performance / 1200 MHz (userspace performance simple_ondemand / 150 300 480 600 800 1200)
    14260000.aipu: performance / 1200 MHz (userspace performance simple_ondemand / 400 600 800 1200)
    15000000.gpu: performance / 900 MHz (userspace performance simple_ondemand / 72 216 350 600 800 900)

Tuned governor settings:

    cpufreq-policy0: performance / 2600 MHz
    cpufreq-policy1: performance / 1800 MHz
    cpufreq-policy5: performance / 2300 MHz
    cpufreq-policy7: performance / 2200 MHz
    cpufreq-policy9: performance / 2500 MHz
    14230000.vpu: performance / 1200 MHz
    14260000.aipu: performance / 1200 MHz
    15000000.gpu: performance / 900 MHz

Status of performance related policies found below /sys:

    /sys/devices/platform/soc@0/14260000.aipu/gm_policy: [1] AIPU GM is shared by tasks of all QoS level.
    /sys/devices/platform/soc@0/15000000.gpu/power_policy: [coarse_demand] always_on
    /sys/module/pcie_aspm/parameters/policy: default [performance] powersave powersupersave

### Clockspeeds (idle vs. heated up):

Before at 28.0°C:

    cpu0 (Cortex-A720): OPP: 2600, Measured: 2598 
    cpu1-cpu4 (Cortex-A520): OPP: 1799, Measured: 1796 
    cpu5-cpu6 (Cortex-A720): OPP: 2299, Measured: 2299 
    cpu7-cpu8 (Cortex-A720): OPP: 2199, Measured: 2199 
    cpu9-cpu10 (Cortex-A720): OPP: 2499, Measured: 2499 
    cpu11 (Cortex-A720): Measured: 2598

After at 46.0°C:

    cpu0 (Cortex-A720): OPP: 2600, Measured: 2598 
    cpu1-cpu4 (Cortex-A520): OPP: 1799, Measured: 1796 
    cpu5-cpu6 (Cortex-A720): OPP: 2299, Measured: 2298 
    cpu7-cpu8 (Cortex-A720): OPP: 2199, Measured: 2199 
    cpu9-cpu10 (Cortex-A720): OPP: 2499, Measured: 2499 
    cpu11 (Cortex-A720): Measured: 2598

### Performance baseline

  * cpu0 (Cortex-A720): memcpy: 13338.2 MB/s, memchr: 26833.0 MB/s, memset: 31438.5 MB/s
  * cpu1 (Cortex-A520): memcpy: 8870.4 MB/s, memchr: 1792.8 MB/s, memset: 28407.7 MB/s
  * cpu5 (Cortex-A720): memcpy: 15861.2 MB/s, memchr: 23934.9 MB/s, memset: 44449.0 MB/s
  * cpu7 (Cortex-A720): memcpy: 16877.3 MB/s, memchr: 22912.1 MB/s, memset: 47083.7 MB/s
  * cpu9 (Cortex-A720): memcpy: 15264.6 MB/s, memchr: 25926.8 MB/s, memset: 48033.5 MB/s
  * cpu11 (Cortex-A720): memcpy: 13525.7 MB/s, memchr: 26331.8 MB/s, memset: 39928.0 MB/s
  * cpu0 (Cortex-A720) 16M latency: 36.88 26.52 35.63 26.50 35.14 29.51 31.97 44.90 
  * cpu1 (Cortex-A520) 16M latency: 59.50 56.44 54.05 56.21 53.60 55.75 66.05 94.07 
  * cpu5 (Cortex-A720) 16M latency: 36.14 25.94 34.29 25.74 33.61 28.69 32.27 43.48 
  * cpu7 (Cortex-A720) 16M latency: 36.82 25.39 32.57 25.32 33.23 29.23 30.68 43.06 
  * cpu9 (Cortex-A720) 16M latency: 34.91 25.00 33.97 24.80 34.59 27.66 30.54 41.51 
  * cpu11 (Cortex-A720) 16M latency: 33.12 25.20 32.83 25.18 33.04 27.55 30.31 41.83 
  * cpu0 (Cortex-A720) 128M latency: 50.57 73.38 50.24 73.80 50.34 67.51 99.35 139.5 
  * cpu1 (Cortex-A520) 128M latency: 218.7 222.3 220.7 222.7 220.0 222.1 234.8 325.1 
  * cpu5 (Cortex-A720) 128M latency: 49.71 73.37 48.91 73.30 50.02 67.09 99.19 138.0 
  * cpu7 (Cortex-A720) 128M latency: 49.53 73.46 49.75 72.16 48.80 64.93 98.31 136.8 
  * cpu9 (Cortex-A720) 128M latency: 48.61 70.06 49.51 67.60 49.83 62.42 98.66 137.3 
  * cpu11 (Cortex-A720) 128M latency: 49.58 71.82 49.01 70.11 48.89 63.16 94.08 136.9 
  * 7-zip MIPS (3 consecutive runs): 32796, 32540, 32737 (32690 avg), single-threaded: 3945
  * `aes-256-cbc     865957.21k  1308442.84k  1430646.19k  1451713.54k  1457979.39k  1458416.30k (Cortex-A720)`
  * `aes-256-cbc     171534.68k   344544.45k   463446.53k   506069.67k   520238.42k   520989.35k (Cortex-A520)`
  * `aes-256-cbc     766195.43k  1157482.97k  1265519.45k  1284163.24k  1289710.25k  1290114.39k (Cortex-A720)`
  * `aes-256-cbc     732927.11k  1107198.83k  1210558.12k  1228357.63k  1233704.28k  1234097.49k (Cortex-A720)`
  * `aes-256-cbc     832910.49k  1258453.10k  1375737.17k  1395967.32k  1402008.92k  1402475.86k (Cortex-A720)`
  * `aes-256-cbc     866090.98k  1308479.81k  1430496.43k  1451707.73k  1457785.51k  1458410.84k (Cortex-A720)`

### PCIe and storage devices:

  * Realtek Device 8126: Speed 8GT/s, Width x1, driver in use: r8126, ASPM Disabled
  * Realtek Device 8126: Speed 8GT/s, Width x1, driver in use: r8126, ASPM Disabled
  * 1.8TB "Samsung SSD 990 PRO with Heatsink 2TB" SSD as /dev/nvme0: Speed 16GT/s, Width x4, 0% worn out, drive temp: 34°C, ASPM Disabled
  * 119.2GB "SAMSUNG MZ7TE128HMGR-00004" SSD as /dev/sda [SATA 3.1, 6.0 Gb/s (current: 6.0 Gb/s)]: behind VIA Labs VL715/VL716 SATA 6Gb/s bridge (2109:0715), 4% worn out, Driver=uas, 10Gbps (capable of 12Mbps, 480Mbps, 5Gbps, 10Gb/s Symmetric RX SuperSpeedPlus, 10Gb/s Symmetric TX SuperSpeedPlus), drive temp: 24°C

### Software versions:

  * Debian GNU/Linux 12 (bookworm)
  * Compiler: /usr/bin/gcc (Debian 12.2.0-14) 12.2.0 / aarch64-linux-gnu
  * OpenSSL 3.0.15, built on 3 Sep 2024 (Library: OpenSSL 3.0.15 3 Sep 2024)    

### Kernel info:

  * `/proc/cmdline: BOOT_IMAGE=/Image loglevel=0 console=ttyAMA2,115200 efi=noruntime earlycon=pl011,0x040d0000 arm-smmu-v3.disable_bypass=0 acpi=off root=/dev/sda2 rootwait rw`
  * Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl
  * Vulnerability Spectre v1:           Mitigation; __user pointer sanitization
  * Kernel 6.1.44-cix-build-generic / CONFIG_HZ=250

Kernel 6.1.44 is not latest 6.1.127 LTS that was released on 2025-01-23.

See https://endoflife.date/linux for details. It is somewhat likely that
a lot of exploitable vulnerabilities exist for this kernel as well as many
unfixed bugs.
